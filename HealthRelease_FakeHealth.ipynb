{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wUAQOKWx0XXP",
        "_QVl-z5a0-Kh",
        "Yzz8mXEz1qT1",
        "_sCdbU6edF2c",
        "Fmi-I9vUZ7Cx",
        "ptgLlJJpDh36",
        "3_uUi2mHER1a",
        "jCcvm1WUEiJb",
        "CVmeC334gZN0",
        "uyj1ZIDulCA9",
        "lMCKVtfxlbIe",
        "iSspSHm0utwY",
        "dL8z_2ITuw9g"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yobangs/Ensemble-Indobert-BiLSTM-Fake-Health/blob/main/HealthRelease_FakeHealth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece torch pandas tqdm\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HSVxTpBEz_CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALISIS_DATASET Health RELEASE"
      ],
      "metadata": {
        "id": "wUAQOKWx0XXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rjcL4LdzqZb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthRelease\"\n",
        "output_csv = \"dataset_release.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Baca file pertama untuk ambil header\n",
        "with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "\n",
        "    # Fungsi flatten nested dict\n",
        "def flatten(d, parent=''):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        key = f\"{parent}_{k}\" if parent else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten(v, key).items())\n",
        "        else:\n",
        "            items.append((key, v))\n",
        "    return dict(items)\n",
        "\n",
        "    # Proses semua file\n",
        "data = []\n",
        "for file in json_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        data.append(flatten(obj))\n",
        "\n",
        "        # Tulis ke CSV\n",
        "if data:\n",
        "    keys = set()\n",
        "    for d in data:\n",
        "        keys.update(d.keys())\n",
        "\n",
        "    with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted(keys))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Berhasil! Dataset disimpan ke {output_csv}\")\n",
        "    print(f\"Total baris: {len(data)}, Total kolom: {len(keys)}\")\n",
        "else:\n",
        "    print(\"Tidak ada data untuk diproses\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('dataset_release.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ANALISIS DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal baris: {len(df)}\")\n",
        "print(f\"Total kolom: {len(df.columns)}\")\n",
        "\n",
        "# Tampilkan semua nama kolom\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DAFTAR SEMUA KOLOM:\")\n",
        "print(\"=\"*60)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    # Hitung nilai non-null\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = (non_null/len(df)*100)\n",
        "    print(f\"{i:3d}. {col:50s} - {non_null:3d} ({pct:5.1f}%)\")\n"
      ],
      "metadata": {
        "id": "SzWXBqGx0N0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan contoh 3 baris pertama\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONTOH DATA (3 baris pertama):\")\n",
        "print(\"=\"*60)\n",
        "print(df.head(3).to_string())\n",
        "\n",
        "# Analisis kolom dengan nilai terbanyak\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 KOLOM DENGAN DATA TERLENGKAP:\")\n",
        "print(\"=\"*60)\n",
        "completeness = df.notna().sum().sort_values(ascending=False).head(20)\n",
        "for col, count in completeness.items():\n",
        "    pct = (count/len(df)*100)\n",
        "    print(f\"{col:50s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Silakan tentukan kolom mana yang ingin dipertahankan!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "0RhY9Iam0qfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALISIS_DATASET Health STORY"
      ],
      "metadata": {
        "id": "_QVl-z5a0-Kh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G5NYxu60-Ki"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthStory\"\n",
        "output_csv = \"dataset_story.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Baca file pertama untuk ambil header\n",
        "with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "\n",
        "    # Fungsi flatten nested dict\n",
        "def flatten(d, parent=''):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        key = f\"{parent}_{k}\" if parent else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten(v, key).items())\n",
        "        else:\n",
        "            items.append((key, v))\n",
        "    return dict(items)\n",
        "\n",
        "    # Proses semua file\n",
        "data = []\n",
        "for file in json_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        data.append(flatten(obj))\n",
        "\n",
        "        # Tulis ke CSV\n",
        "if data:\n",
        "    keys = set()\n",
        "    for d in data:\n",
        "        keys.update(d.keys())\n",
        "\n",
        "    with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted(keys))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Berhasil! Dataset disimpan ke {output_csv}\")\n",
        "    print(f\"Total baris: {len(data)}, Total kolom: {len(keys)}\")\n",
        "else:\n",
        "    print(\"Tidak ada data untuk diproses\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('dataset_story.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ANALISIS DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal baris: {len(df)}\")\n",
        "print(f\"Total kolom: {len(df.columns)}\")\n",
        "\n",
        "# Tampilkan semua nama kolom\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DAFTAR SEMUA KOLOM:\")\n",
        "print(\"=\"*60)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    # Hitung nilai non-null\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = (non_null/len(df)*100)\n",
        "    print(f\"{i:3d}. {col:50s} - {non_null:3d} ({pct:5.1f}%)\")\n"
      ],
      "metadata": {
        "id": "N7Gsi2Bt0-Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan contoh 3 baris pertama\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONTOH DATA (3 baris pertama):\")\n",
        "print(\"=\"*60)\n",
        "print(df.head(3).to_string())\n",
        "\n",
        "# Analisis kolom dengan nilai terbanyak\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 KOLOM DENGAN DATA TERLENGKAP:\")\n",
        "print(\"=\"*60)\n",
        "completeness = df.notna().sum().sort_values(ascending=False).head(20)\n",
        "for col, count in completeness.items():\n",
        "    pct = (count/len(df)*100)\n",
        "    print(f\"{col:50s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Silakan tentukan kolom mana yang ingin dipertahankan!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "3g049An_0-Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REKAP MENJADI CSV"
      ],
      "metadata": {
        "id": "Yzz8mXEz1qT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthRelease\"\n",
        "output_csv = \"HealthRelease_Cleaned.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"üìÅ Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Proses semua file\n",
        "data = []\n",
        "errors = []\n",
        "\n",
        "for file in json_files:\n",
        "    try:\n",
        "        file_id = file.stem\n",
        "\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            obj = json.load(f)\n",
        "\n",
        "            data.append({\n",
        "                'file_id': file_id,\n",
        "                'title': obj.get('title', ''),\n",
        "                'text': obj.get('text', ''),\n",
        "                'url': obj.get('url', ''),\n",
        "                'source': obj.get('source', ''),\n",
        "                'publish_date': obj.get('publish_date', '')\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(f\"{file.name}: {e}\")\n",
        "\n",
        "# Buat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bersihkan data\n",
        "df['title'] = df['title'].str.strip()\n",
        "df['text'] = df['text'].str.strip()\n",
        "\n",
        "# Hapus baris kosong\n",
        "df_original_len = len(df)\n",
        "df = df[(df['title'] != '') & (df['text'] != '')]\n",
        "df = df[df['text'].str.len() >= 100]\n",
        "df = df.sort_values('file_id').reset_index(drop=True)\n",
        "\n",
        "# Simpan\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Output ringkas dan rapi\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úÖ DATASET BERHASIL DIBUAT: {output_csv}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total file JSON     : {len(json_files)}\")\n",
        "print(f\"Artikel valid       : {len(df)} ({len(df)/len(json_files)*100:.1f}%)\")\n",
        "print(f\"Artikel dibuang     : {df_original_len - len(df)} (text < 100 chars atau kosong)\")\n",
        "if errors:\n",
        "    print(f\"File error          : {len(errors)}\")\n",
        "\n",
        "# Statistik kolom (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"KELENGKAPAN DATA:\")\n",
        "for col in df.columns:\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = non_null/len(df)*100\n",
        "    bar = \"‚ñà\" * int(pct/5) + \"‚ñë\" * (20 - int(pct/5))\n",
        "    print(f\"  {col:15s} [{bar}] {pct:5.1f}% ({non_null}/{len(df)})\")\n",
        "\n",
        "# Statistik teks\n",
        "print(\"-\" * 80)\n",
        "print(\"STATISTIK TEKS:\")\n",
        "text_lengths = df['text'].str.len()\n",
        "print(f\"  Rata-rata panjang  : {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Terpendek          : {text_lengths.min()} karakter\")\n",
        "print(f\"  Terpanjang         : {text_lengths.max()} karakter\")\n",
        "\n",
        "# Preview data (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"PREVIEW 5 ARTIKEL PERTAMA:\")\n",
        "for idx, row in df.head(5).iterrows():\n",
        "    title_preview = row['title'][:65] + \"...\" if len(row['title']) > 65 else row['title']\n",
        "    print(f\"  [{row['file_id']}] {title_preview}\")\n",
        "    print(f\"    ‚îî‚îÄ {len(row['text'])} chars | {row['source'][:40]}\")\n",
        "\n",
        "# Error summary\n",
        "if errors:\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"‚ö†Ô∏è  FILE DENGAN ERROR ({len(errors)}):\")\n",
        "    for err in errors[:5]:  # Show first 5 errors\n",
        "        print(f\"  ‚Ä¢ {err}\")\n",
        "    if len(errors) > 5:\n",
        "        print(f\"  ... dan {len(errors)-5} error lainnya\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "JmfWCsYs1vDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthStory\"\n",
        "output_csv = \"HealthStory_Cleaned.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"üìÅ Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Proses semua file\n",
        "data = []\n",
        "errors = []\n",
        "\n",
        "for file in json_files:\n",
        "    try:\n",
        "        file_id = file.stem\n",
        "\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            obj = json.load(f)\n",
        "\n",
        "            data.append({\n",
        "                'file_id': file_id,\n",
        "                'title': obj.get('title', ''),\n",
        "                'text': obj.get('text', ''),\n",
        "                'url': obj.get('url', ''),\n",
        "                'source': obj.get('source', ''),\n",
        "                'publish_date': obj.get('publish_date', '')\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(f\"{file.name}: {e}\")\n",
        "\n",
        "# Buat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bersihkan data\n",
        "df['title'] = df['title'].str.strip()\n",
        "df['text'] = df['text'].str.strip()\n",
        "\n",
        "# Hapus baris kosong\n",
        "df_original_len = len(df)\n",
        "df = df[(df['title'] != '') & (df['text'] != '')]\n",
        "df = df[df['text'].str.len() >= 100]\n",
        "df = df.sort_values('file_id').reset_index(drop=True)\n",
        "\n",
        "# Simpan\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Output ringkas dan rapi\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úÖ DATASET BERHASIL DIBUAT: {output_csv}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total file JSON     : {len(json_files)}\")\n",
        "print(f\"Artikel valid       : {len(df)} ({len(df)/len(json_files)*100:.1f}%)\")\n",
        "print(f\"Artikel dibuang     : {df_original_len - len(df)} (text < 100 chars atau kosong)\")\n",
        "if errors:\n",
        "    print(f\"File error          : {len(errors)}\")\n",
        "\n",
        "# Statistik kolom (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"KELENGKAPAN DATA:\")\n",
        "for col in df.columns:\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = non_null/len(df)*100\n",
        "    bar = \"‚ñà\" * int(pct/5) + \"‚ñë\" * (20 - int(pct/5))\n",
        "    print(f\"  {col:15s} [{bar}] {pct:5.1f}% ({non_null}/{len(df)})\")\n",
        "\n",
        "# Statistik teks\n",
        "print(\"-\" * 80)\n",
        "print(\"STATISTIK TEKS:\")\n",
        "text_lengths = df['text'].str.len()\n",
        "print(f\"  Rata-rata panjang  : {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Terpendek          : {text_lengths.min()} karakter\")\n",
        "print(f\"  Terpanjang         : {text_lengths.max()} karakter\")\n",
        "\n",
        "# Preview data (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"PREVIEW 5 ARTIKEL PERTAMA:\")\n",
        "for idx, row in df.head(5).iterrows():\n",
        "    title_preview = row['title'][:65] + \"...\" if len(row['title']) > 65 else row['title']\n",
        "    print(f\"  [{row['file_id']}] {title_preview}\")\n",
        "    print(f\"    ‚îî‚îÄ {len(row['text'])} chars | {row['source'][:40]}\")\n",
        "\n",
        "# Error summary\n",
        "if errors:\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"‚ö†Ô∏è  FILE DENGAN ERROR ({len(errors)}):\")\n",
        "    for err in errors[:5]:  # Show first 5 errors\n",
        "        print(f\"  ‚Ä¢ {err}\")\n",
        "    if len(errors) > 5:\n",
        "        print(f\"  ... dan {len(errors)-5} error lainnya\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "G1II8Jm51uoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sCdbU6edF2c"
      },
      "source": [
        "# DATA LABELLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmIzLZJRDeg2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def extract_reviews(folder_path):\n",
        "    \"\"\"Ekstrak label dari folder reviews\"\"\"\n",
        "    data = []\n",
        "\n",
        "    for file in Path(folder_path).glob('*.json'):\n",
        "        try:\n",
        "            with open(file, 'r', encoding='utf-8') as f:\n",
        "                d = json.load(f)\n",
        "\n",
        "            file_id = file.stem\n",
        "            rating = d.get('rating', '')\n",
        "            satisfactory = d.get('satisfactory', '')\n",
        "            label_text = d.get('label', '')\n",
        "            label_raw = (satisfactory or rating or label_text or '').lower()\n",
        "\n",
        "            data.append({'file_id': file_id, 'label_raw': label_raw})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error {file.name}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def map_label(label_raw):\n",
        "    \"\"\"Convert label text ke binary\"\"\"\n",
        "    label_raw = str(label_raw).lower()\n",
        "\n",
        "    if 'satisfactory' in label_raw and 'not' not in label_raw:\n",
        "        return 0\n",
        "    elif 'not satisfactory' in label_raw or 'notsatisfactory' in label_raw:\n",
        "        return 1\n",
        "    elif 'notapplicable' in label_raw or 'not applicable' in label_raw:\n",
        "        return -1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def label_dataset(content_csv, reviews_folder, output_csv):\n",
        "    \"\"\"Labeling dataset dengan merge reviews\"\"\"\n",
        "\n",
        "    print(f\"\\nProcessing: {content_csv}\")\n",
        "\n",
        "    df_content = pd.read_csv(content_csv)\n",
        "    print(f\"Content: {len(df_content)} rows\")\n",
        "\n",
        "    df_reviews = extract_reviews(reviews_folder)\n",
        "    print(f\"Reviews: {len(df_reviews)} rows\")\n",
        "\n",
        "    df_reviews['label'] = df_reviews['label_raw'].apply(map_label)\n",
        "\n",
        "    print(f\"Label: Credible={(df_reviews['label']==0).sum()}, Fake={(df_reviews['label']==1).sum()}, N/A={(df_reviews['label']==-1).sum()}\")\n",
        "\n",
        "    df_labeled = df_content.merge(df_reviews[['file_id', 'label', 'label_raw']], on='file_id', how='left')\n",
        "\n",
        "    df_final = df_labeled[df_labeled['label'].isin([0, 1])].copy()\n",
        "    df_final['label'] = df_final['label'].astype(int)\n",
        "\n",
        "    print(f\"Final: {len(df_final)} rows (Credible={(df_final['label']==0).sum()}, Fake={(df_final['label']==1).sum()})\")\n",
        "\n",
        "    df_final.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úì Saved: {output_csv}\\n\")\n",
        "\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3M6Z3maHtLF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Cek HealthRelease.json\n",
        "print(\"=\"*50)\n",
        "print(\"STRUKTUR HEALTHRELEASE.JSON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "with open('/content/reviews/HealthRelease.json', 'r') as f:\n",
        "    data_release = json.load(f)\n",
        "\n",
        "print(f\"Type: {type(data_release)}\")\n",
        "print(f\"Length: {len(data_release) if isinstance(data_release, (list, dict)) else 'N/A'}\")\n",
        "\n",
        "# Jika dict, tampilkan keys\n",
        "if isinstance(data_release, dict):\n",
        "    print(f\"\\nKeys: {list(data_release.keys())[:10]}\")\n",
        "    # Ambil sample 1 item\n",
        "    first_key = list(data_release.keys())[0]\n",
        "    print(f\"\\nSample (key: {first_key}):\")\n",
        "    print(json.dumps(data_release[first_key], indent=2)[:500])\n",
        "\n",
        "# Jika list, tampilkan item pertama\n",
        "elif isinstance(data_release, list):\n",
        "    print(f\"\\nFirst item:\")\n",
        "    print(json.dumps(data_release[0], indent=2)[:500])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STRUKTUR HEALTHSTORY.JSON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "with open('/content/reviews/HealthStory.json', 'r') as f:\n",
        "    data_story = json.load(f)\n",
        "\n",
        "print(f\"Type: {type(data_story)}\")\n",
        "print(f\"Length: {len(data_story) if isinstance(data_story, (list, dict)) else 'N/A'}\")\n",
        "\n",
        "if isinstance(data_story, dict):\n",
        "    print(f\"\\nKeys: {list(data_story.keys())[:10]}\")\n",
        "    first_key = list(data_story.keys())[0]\n",
        "    print(f\"\\nSample (key: {first_key}):\")\n",
        "    print(json.dumps(data_story[first_key], indent=2)[:500])\n",
        "elif isinstance(data_story, list):\n",
        "    print(f\"\\nFirst item:\")\n",
        "    print(json.dumps(data_story[0], indent=2)[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuUDc0izH8Hp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Baca file\n",
        "with open('/content/reviews/HealthRelease.json', 'r') as f:\n",
        "    data_release = json.load(f)\n",
        "\n",
        "with open('/content/reviews/HealthStory.json', 'r') as f:\n",
        "    data_story = json.load(f)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"HEALTHRELEASE - Sample Item (FULL)\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(data_release[0], indent=2))\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"HEALTHSTORY - Sample Item (FULL)\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(data_story[0], indent=2))\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"SEMUA KEYS YANG ADA DI HEALTHRELEASE\")\n",
        "print(\"=\"*60)\n",
        "all_keys = set()\n",
        "for item in data_release[:50]:\n",
        "    all_keys.update(item.keys())\n",
        "print(sorted(all_keys))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SEMUA KEYS YANG ADA DI HEALTHSTORY\")\n",
        "print(\"=\"*60)\n",
        "all_keys = set()\n",
        "for item in data_story[:50]:\n",
        "    all_keys.update(item.keys())\n",
        "print(sorted(all_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO-jY2fHIUq2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extract_reviews_from_json(json_file):\n",
        "    \"\"\"Ekstrak label dari file JSON reviews\"\"\"\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for item in data:\n",
        "        news_id = item.get('news_id', '')\n",
        "        rating = item.get('rating', 0)\n",
        "\n",
        "        # Hitung Satisfactory vs Not Satisfactory dari criteria\n",
        "        satisfactory_count = 0\n",
        "        not_satisfactory_count = 0\n",
        "        not_applicable_count = 0\n",
        "\n",
        "        criteria = item.get('criteria', [])\n",
        "        for criterion in criteria:\n",
        "            answer = criterion.get('answer', '').lower()\n",
        "            if 'satisfactory' in answer and 'not' not in answer:\n",
        "                satisfactory_count += 1\n",
        "            elif 'not satisfactory' in answer:\n",
        "                not_satisfactory_count += 1\n",
        "            elif 'not applicable' in answer:\n",
        "                not_applicable_count += 1\n",
        "\n",
        "        # Hitung persentase satisfactory\n",
        "        total_applicable = satisfactory_count + not_satisfactory_count\n",
        "        if total_applicable > 0:\n",
        "            satisfactory_ratio = satisfactory_count / total_applicable\n",
        "        else:\n",
        "            satisfactory_ratio = 0\n",
        "\n",
        "        # Tentukan label berdasarkan ratio\n",
        "        # Jika >= 50% satisfactory = Credible (0)\n",
        "        # Jika < 50% satisfactory = Fake/Not Credible (1)\n",
        "        if satisfactory_ratio >= 0.5:\n",
        "            label = 0  # Credible\n",
        "        else:\n",
        "            label = 1  # Not Credible/Fake\n",
        "\n",
        "        results.append({\n",
        "            'news_id': news_id,\n",
        "            'rating': rating,\n",
        "            'satisfactory': satisfactory_count,\n",
        "            'not_satisfactory': not_satisfactory_count,\n",
        "            'not_applicable': not_applicable_count,\n",
        "            'satisfactory_ratio': round(satisfactory_ratio, 2),\n",
        "            'label': label\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def label_dataset(content_csv, reviews_json, output_csv, dataset_name):\n",
        "    \"\"\"Merge content dengan reviews untuk labeling\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LABELING {dataset_name.upper()}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Baca content\n",
        "    df_content = pd.read_csv(content_csv)\n",
        "    print(f\"Content: {len(df_content)} rows\")\n",
        "\n",
        "    # Ekstrak reviews\n",
        "    df_reviews = extract_reviews_from_json(reviews_json)\n",
        "    print(f\"Reviews: {len(df_reviews)} rows\")\n",
        "\n",
        "    # Rename 'news_id' ke 'file_id' untuk matching\n",
        "    df_reviews = df_reviews.rename(columns={'news_id': 'file_id'})\n",
        "\n",
        "    # Merge\n",
        "    df_labeled = df_content.merge(df_reviews, on='file_id', how='inner')\n",
        "\n",
        "    print(f\"\\nMerged: {len(df_labeled)} rows\")\n",
        "    print(f\"\\nLabel Distribution:\")\n",
        "    print(f\"  Credible (0): {(df_labeled['label']==0).sum()} ({(df_labeled['label']==0).sum()/len(df_labeled)*100:.1f}%)\")\n",
        "    print(f\"  Fake (1): {(df_labeled['label']==1).sum()} ({(df_labeled['label']==1).sum()/len(df_labeled)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nRating Statistics:\")\n",
        "    print(f\"  Mean rating: {df_labeled['rating'].mean():.2f}\")\n",
        "    print(f\"  Min rating: {df_labeled['rating'].min()}\")\n",
        "    print(f\"  Max rating: {df_labeled['rating'].max()}\")\n",
        "\n",
        "    # Simpan\n",
        "    df_labeled.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n‚úì Saved: {output_csv}\")\n",
        "\n",
        "    # Preview\n",
        "    print(f\"\\nPreview (3 rows):\")\n",
        "    print(df_labeled[['file_id', 'title', 'rating', 'satisfactory', 'not_satisfactory', 'label']].head(3))\n",
        "\n",
        "    return df_labeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ3iGEw1IYdI"
      },
      "outputs": [],
      "source": [
        "# MAIN EXECUTION\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET LABELING PROCESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# HealthRelease\n",
        "df_release = label_dataset(\n",
        "    content_csv='/content/HealthRelease_Cleaned.csv',\n",
        "    reviews_json='/content/reviews/HealthRelease.json',\n",
        "    output_csv='health_release_LABELED.csv',\n",
        "    dataset_name='HealthRelease'\n",
        ")\n",
        "\n",
        "# HealthStory\n",
        "df_story = label_dataset(\n",
        "    content_csv='/content/HealthStory_Cleaned.csv',\n",
        "    reviews_json='/content/reviews/HealthStory.json',\n",
        "    output_csv='health_story_LABELED.csv',\n",
        "    dataset_name='HealthStory'\n",
        ")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nHealthRelease:\")\n",
        "print(f\"  Total: {len(df_release)} rows\")\n",
        "print(f\"  Credible (0): {(df_release['label']==0).sum()}\")\n",
        "print(f\"  Fake (1): {(df_release['label']==1).sum()}\")\n",
        "\n",
        "print(f\"\\nHealthStory:\")\n",
        "print(f\"  Total: {len(df_story)} rows\")\n",
        "print(f\"  Credible (0): {(df_story['label']==0).sum()}\")\n",
        "print(f\"  Fake (1): {(df_story['label']==1).sum()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Output files (terpisah):\")\n",
        "print(\"  - health_release_LABELED.csv\")\n",
        "print(\"  - health_story_LABELED.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bjy4rVMA7WLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING"
      ],
      "metadata": {
        "id": "Oy2It-x07Xcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def light_preprocessing(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. Normalisasi Unicode (Mengubah bentuk karakter aneh ke bentuk standar)\n",
        "    # Contoh: Mengubah '√©' menjadi 'e' atau memastikan simbol tetap terbaca\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # 2. Menghapus HTML Tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "    # 3. Menghapus URL/Link (Penting agar model fokus pada konten berita)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "    # 4. Menghapus Email\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "\n",
        "    # 5. Membersihkan Newlines, Tabs, dan karakter aneh lainnya\n",
        "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
        "\n",
        "    # 6. Menghapus karakter non-printable (karakter kontrol yang tidak terlihat)\n",
        "    text = \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
        "\n",
        "    # 7. Mempertahankan tanda baca penting & Angka\n",
        "    # Kita hanya menghapus simbol-simbol dekoratif (seperti emoji atau simbol hiasan)\n",
        "    # Tetap pertahankan: . , ! ? : ; - ( ) %\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:\\-\\(\\)%]', ' ', text)\n",
        "\n",
        "    # 8. Lowercase (Gunakan jika menggunakan model IndoBERT-uncased)\n",
        "    text = text.lower()\n",
        "\n",
        "    # 9. Menghapus spasi ganda\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# ========== LOAD DATASETS ==========\n",
        "print(\"=\" * 60)\n",
        "print(\"1Ô∏è‚É£  LOADING LABELED DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    release_df = pd.read_csv('health_release_LABELED.csv')\n",
        "    story_df = pd.read_csv('health_story_LABELED.csv')\n",
        "\n",
        "    print(f\"üìÑ health_release_LABELED.csv: {len(release_df)} sampel\")\n",
        "    print(f\"üìÑ health_story_LABELED.csv: {len(story_df)} sampel\")\n",
        "    print(f\"‚úÖ Berhasil load kedua dataset.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå File tidak ditemukan: {e}\")\n",
        "    print(\"üí° Pastikan file CSV ada di direktori yang sama dengan script ini.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saat load file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ========== CEK KOLOM ==========\n",
        "print(f\"\\nüìã Kolom health_release: {list(release_df.columns)}\")\n",
        "print(f\"üìã Kolom health_story: {list(story_df.columns)}\")\n",
        "\n",
        "# Tentukan kolom teks (sesuaikan dengan nama kolom di dataset Anda)\n",
        "column_to_clean = 'text'  # ‚¨ÖÔ∏è GANTI JIKA NAMA KOLOM BERBEDA\n",
        "\n",
        "if column_to_clean not in release_df.columns:\n",
        "    print(f\"\\n‚ö†Ô∏è  Kolom '{column_to_clean}' tidak ditemukan di health_release!\")\n",
        "    print(f\"üí° Ganti variabel 'column_to_clean' dengan salah satu dari: {list(release_df.columns)}\")\n",
        "    exit()\n",
        "\n",
        "if column_to_clean not in story_df.columns:\n",
        "    print(f\"\\n‚ö†Ô∏è  Kolom '{column_to_clean}' tidak ditemukan di health_story!\")\n",
        "    print(f\"üí° Ganti variabel 'column_to_clean' dengan salah satu dari: {list(story_df.columns)}\")\n",
        "    exit()\n",
        "\n",
        "# ========== PROSES HEALTH RELEASE ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2Ô∏è‚É£  PREPROCESSING HEALTH RELEASE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "null_count_release = release_df[column_to_clean].isna().sum()\n",
        "empty_count_release = (release_df[column_to_clean] == '').sum()\n",
        "\n",
        "print(f\"üîç Jumlah nilai NULL: {null_count_release}\")\n",
        "print(f\"üîç Jumlah teks kosong: {empty_count_release}\")\n",
        "print(f\"üîç Total data valid: {len(release_df) - null_count_release - empty_count_release}\")\n",
        "\n",
        "release_df['text_cleaned'] = release_df[column_to_clean].apply(light_preprocessing)\n",
        "\n",
        "before_drop_release = len(release_df)\n",
        "release_df = release_df[release_df['text_cleaned'] != ''].reset_index(drop=True)\n",
        "after_drop_release = len(release_df)\n",
        "\n",
        "dropped_release = before_drop_release - after_drop_release\n",
        "print(f\"‚úÖ Preprocessing selesai!\")\n",
        "print(f\"üóëÔ∏è  Dihapus {dropped_release} baris kosong setelah cleaning\")\n",
        "print(f\"üìä Total data final: {len(release_df)} sampel\")\n",
        "\n",
        "# ========== PROSES HEALTH STORY ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3Ô∏è‚É£  PREPROCESSING HEALTH STORY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "null_count_story = story_df[column_to_clean].isna().sum()\n",
        "empty_count_story = (story_df[column_to_clean] == '').sum()\n",
        "\n",
        "print(f\"üîç Jumlah nilai NULL: {null_count_story}\")\n",
        "print(f\"üîç Jumlah teks kosong: {empty_count_story}\")\n",
        "print(f\"üîç Total data valid: {len(story_df) - null_count_story - empty_count_story}\")\n",
        "\n",
        "story_df['text_cleaned'] = story_df[column_to_clean].apply(light_preprocessing)\n",
        "\n",
        "before_drop_story = len(story_df)\n",
        "story_df = story_df[story_df['text_cleaned'] != ''].reset_index(drop=True)\n",
        "after_drop_story = len(story_df)\n",
        "\n",
        "dropped_story = before_drop_story - after_drop_story\n",
        "print(f\"‚úÖ Preprocessing selesai!\")\n",
        "print(f\"üóëÔ∏è  Dihapus {dropped_story} baris kosong setelah cleaning\")\n",
        "print(f\"üìä Total data final: {len(story_df)} sampel\")\n",
        "\n",
        "# ========== CONTOH HASIL HEALTH RELEASE ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4Ô∏è‚É£  CONTOH HASIL PREPROCESSING HEALTH RELEASE (3 sampel acak)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_release = release_df.sample(min(3, len(release_df)))\n",
        "for idx, row in sample_release.iterrows():\n",
        "    print(f\"\\n--- SAMPEL {idx + 1} ---\")\n",
        "    print(f\"SEBELUM: {row[column_to_clean][:200]}...\")\n",
        "    print(f\"SESUDAH: {row['text_cleaned'][:200]}...\")\n",
        "    print(f\"LABEL: {row['label']}\")\n",
        "\n",
        "# ========== CONTOH HASIL HEALTH STORY ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"5Ô∏è‚É£  CONTOH HASIL PREPROCESSING HEALTH STORY (3 sampel acak)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_story = story_df.sample(min(3, len(story_df)))\n",
        "for idx, row in sample_story.iterrows():\n",
        "    print(f\"\\n--- SAMPEL {idx + 1} ---\")\n",
        "    print(f\"SEBELUM: {row[column_to_clean][:200]}...\")\n",
        "    print(f\"SESUDAH: {row['text_cleaned'][:200]}...\")\n",
        "    print(f\"LABEL: {row['label']}\")\n",
        "\n",
        "# ========== SIMPAN HASIL (TERPISAH) ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"6Ô∏è‚É£  MENYIMPAN HASIL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "output_file_release = 'health_release_CLEANED.csv'\n",
        "output_file_story = 'health_story_CLEANED.csv'\n",
        "\n",
        "release_df.to_csv(output_file_release, index=False)\n",
        "story_df.to_csv(output_file_story, index=False)\n",
        "\n",
        "print(f\"üíæ Health Release disimpan: {output_file_release}\")\n",
        "print(f\"üíæ Health Story disimpan: {output_file_story}\")\n",
        "\n",
        "# ========== STATISTIK AKHIR HEALTH RELEASE ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"7Ô∏è‚É£  STATISTIK DISTRIBUSI LABEL - HEALTH RELEASE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'label' in release_df.columns:\n",
        "    print(release_df['label'].value_counts())\n",
        "    print(f\"\\nüìä Persentase:\")\n",
        "    print(release_df['label'].value_counts(normalize=True) * 100)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kolom 'label' tidak ditemukan di dataset!\")\n",
        "\n",
        "# ========== STATISTIK AKHIR HEALTH STORY ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"8Ô∏è‚É£  STATISTIK DISTRIBUSI LABEL - HEALTH STORY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'label' in story_df.columns:\n",
        "    print(story_df['label'].value_counts())\n",
        "    print(f\"\\nüìä Persentase:\")\n",
        "    print(story_df['label'].value_counts(normalize=True) * 100)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kolom 'label' tidak ditemukan di dataset!\")\n",
        "\n",
        "# ========== SIAPKAN FILE UNTUK TRANSLATE (TERPISAH) ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"9Ô∏è‚É£  MENYIAPKAN FILE UNTUK TRANSLATE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Health Release\n",
        "df_to_translate_release = release_df[['title', 'text_cleaned', 'label']].copy()\n",
        "df_to_translate_release.to_csv('ready_to_translate_release_en.csv', index=False)\n",
        "print(f\"‚úÖ Health Release siap translate: ready_to_translate_release_en.csv ({len(df_to_translate_release)} sampel)\")\n",
        "\n",
        "# Health Story\n",
        "df_to_translate_story = story_df[['title', 'text_cleaned', 'label']].copy()\n",
        "df_to_translate_story.to_csv('ready_to_translate_story_en.csv', index=False)\n",
        "print(f\"‚úÖ Health Story siap translate: ready_to_translate_story_en.csv ({len(df_to_translate_story)} sampel)\")\n",
        "\n",
        "print(\"\\n‚úÖ PREPROCESSING SELESAI!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìå Total Health Release: {len(release_df)} sampel\")\n",
        "print(f\"üìå Total Health Story: {len(story_df)} sampel\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "cBVZNgbS7aPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSLASI"
      ],
      "metadata": {
        "id": "YHTXZFc6AKCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ========== KONFIGURASI ==========\n",
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-id\"\n",
        "\n",
        "# File input (2 dataset terpisah dari preprocessing)\n",
        "INPUT_FILE_RELEASE = \"ready_to_translate_release_en.csv\"\n",
        "INPUT_FILE_STORY = \"ready_to_translate_story_en.csv\"\n",
        "\n",
        "# File output (tetap terpisah)\n",
        "OUTPUT_FILE_RELEASE = \"health_release_translated_id.csv\"\n",
        "OUTPUT_FILE_STORY = \"health_story_translated_id.csv\"\n",
        "\n",
        "# File checkpoint (tetap terpisah)\n",
        "CHECKPOINT_FILE_RELEASE = \"health_release_translated_id_checkpoint.csv\"\n",
        "CHECKPOINT_FILE_STORY = \"health_story_translated_id_checkpoint.csv\"\n",
        "\n",
        "# BATCH_SIZE 32-64 biasanya optimal untuk GPU dengan VRAM 8GB+\n",
        "BATCH_SIZE = 32\n",
        "# Berapa banyak baris berita yang diproses sekaligus sebelum disimpan\n",
        "CHUNK_ROWS = 50\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"=\" * 60)\n",
        "print(f\"üñ•Ô∏è  Menggunakan: {'GPU' if device == 0 else 'CPU'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ========== LOAD MODEL ==========\n",
        "print(\"\\nüì¶ Loading translation model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Pipeline diatur dengan batch_size internal\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "def split_sentences(text):\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        return []\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', str(text))\n",
        "    return [s.strip() for s in sentences if len(s.strip()) > 2]\n",
        "\n",
        "def translate_dataset(input_file, output_file, checkpoint_file, dataset_name):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menerjemahkan satu dataset secara terpisah\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üåê TRANSLATING {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ========== PROSES UTAMA ==========\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        df = pd.read_csv(checkpoint_file)\n",
        "        print(f\"‚ôªÔ∏è  Melanjutkan dari checkpoint: {checkpoint_file}\")\n",
        "    else:\n",
        "        df = pd.read_csv(input_file)\n",
        "        df['text_id'] = \"\"\n",
        "        print(f\"üìÑ Memulai translasi baru: {input_file}\")\n",
        "\n",
        "    indices_todo = df[df['text_id'].isna() | (df['text_id'] == \"\")].index.tolist()\n",
        "    print(f\"üöÄ Total data: {len(df)} baris\")\n",
        "    print(f\"üöÄ Sisa yang harus diterjemahkan: {len(indices_todo)} baris\")\n",
        "\n",
        "    if len(indices_todo) == 0:\n",
        "        print(f\"‚úÖ {dataset_name} sudah selesai diterjemahkan sebelumnya!\")\n",
        "        return\n",
        "\n",
        "    # Proses dalam potongan (Chunks) agar tidak memakan RAM berlebih\n",
        "    for i in range(0, len(indices_todo), CHUNK_ROWS):\n",
        "        current_indices = indices_todo[i:i + CHUNK_ROWS]\n",
        "        rows_to_process = df.loc[current_indices, 'text_cleaned'].tolist()\n",
        "\n",
        "        # 1. Flatten: Kumpulkan semua kalimat dan catat asalnya\n",
        "        all_sentences = []\n",
        "        row_map = [] # Mencatat kalimat ini milik baris ke berapa\n",
        "\n",
        "        for idx, row_text in enumerate(rows_to_process):\n",
        "            sents = split_sentences(row_text)\n",
        "            all_sentences.extend(sents)\n",
        "            row_map.extend([idx] * len(sents))\n",
        "\n",
        "        if not all_sentences:\n",
        "            continue\n",
        "\n",
        "        # 2. Batch Translate: Kirim SEMUA kalimat ke GPU sekaligus\n",
        "        # Ini bagian paling cepat karena memanfaatkan paralelisme GPU\n",
        "        translated_sentences = []\n",
        "        for out in translator(all_sentences, truncation=True, max_length=512):\n",
        "            translated_sentences.append(out['translation_text'])\n",
        "\n",
        "        # 3. Reassemble: Satukan kembali kalimat ke baris asalnya\n",
        "        reassembled_texts = [\"\"] * len(rows_to_process)\n",
        "        for sent_idx, row_idx in enumerate(row_map):\n",
        "            reassembled_texts[row_idx] += translated_sentences[sent_idx] + \" \"\n",
        "\n",
        "        # 4. Update DataFrame\n",
        "        for idx, final_text in enumerate(reassembled_texts):\n",
        "            df.at[current_indices[idx], 'text_id'] = final_text.strip()\n",
        "\n",
        "        # Save Checkpoint per Chunk\n",
        "        df.to_csv(checkpoint_file, index=False)\n",
        "        progress = ((i + len(current_indices)) / len(indices_todo)) * 100\n",
        "        print(f\"‚úÖ Progress {dataset_name}: {progress:.1f}% (Baris {current_indices[-1]} selesai...)\")\n",
        "\n",
        "    # Simpan file final\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"üéØ {dataset_name} selesai! Disimpan di: {output_file}\")\n",
        "\n",
        "# ========== TRANSLATE HEALTH RELEASE ==========\n",
        "translate_dataset(\n",
        "    input_file=INPUT_FILE_RELEASE,\n",
        "    output_file=OUTPUT_FILE_RELEASE,\n",
        "    checkpoint_file=CHECKPOINT_FILE_RELEASE,\n",
        "    dataset_name=\"HEALTH RELEASE\"\n",
        ")\n",
        "\n",
        "# ========== TRANSLATE HEALTH STORY ==========\n",
        "translate_dataset(\n",
        "    input_file=INPUT_FILE_STORY,\n",
        "    output_file=OUTPUT_FILE_STORY,\n",
        "    checkpoint_file=CHECKPOINT_FILE_STORY,\n",
        "    dataset_name=\"HEALTH STORY\"\n",
        ")\n",
        "\n",
        "# ========== SUMMARY ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ SEMUA TRANSLASI SELESAI!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Tampilkan statistik\n",
        "try:\n",
        "    df_release = pd.read_csv(OUTPUT_FILE_RELEASE)\n",
        "    df_story = pd.read_csv(OUTPUT_FILE_STORY)\n",
        "\n",
        "    print(f\"\\nüìä STATISTIK:\")\n",
        "    print(f\"   ‚Ä¢ Health Release: {len(df_release)} baris\")\n",
        "    print(f\"   ‚Ä¢ Health Story: {len(df_story)} baris\")\n",
        "    print(f\"\\nüìÅ FILE OUTPUT:\")\n",
        "    print(f\"   ‚Ä¢ {OUTPUT_FILE_RELEASE}\")\n",
        "    print(f\"   ‚Ä¢ {OUTPUT_FILE_STORY}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error saat membaca hasil: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ PROSES SELESAI!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "ZmePrqcrALnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ========== KONFIGURASI ==========\n",
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-id\"\n",
        "\n",
        "# File input (2 dataset terpisah dari translasi sebelumnya)\n",
        "INPUT_FILE_RELEASE = \"health_release_translated_id.csv\"\n",
        "INPUT_FILE_STORY = \"health_story_translated_id.csv\"\n",
        "\n",
        "# File output final (tetap terpisah)\n",
        "OUTPUT_FILE_RELEASE = \"health_release_translated_id_COMPLETE.csv\"\n",
        "OUTPUT_FILE_STORY = \"health_story_translated_id_COMPLETE.csv\"\n",
        "\n",
        "# Batch size bisa lebih besar untuk title (misal 64 atau 128) karena teksnya pendek\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# ========== SETUP DEVICE ==========\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"=\" * 60)\n",
        "print(f\"üñ•Ô∏è  Menggunakan: {'GPU' if device == 0 else 'CPU'}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ========== LOAD MODEL & TOKENIZER ==========\n",
        "print(f\"\\nüì¶ Loading model: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "def translate_titles(input_file, output_file, dataset_name):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menerjemahkan title dari satu dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üìù TRANSLATING TITLES - {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ========== LOAD DATA ==========\n",
        "    df = pd.read_csv(input_file)\n",
        "    print(f\"üìÑ Loaded: {input_file} ({len(df)} baris)\")\n",
        "\n",
        "    # Inisialisasi kolom title_id jika belum ada\n",
        "    if 'title_id' not in df.columns:\n",
        "        df['title_id'] = \"\"\n",
        "\n",
        "    # Cari indeks yang title_id nya masih kosong\n",
        "    indices_to_process = df[df['title_id'].isna() | (df['title_id'] == \"\")].index.tolist()\n",
        "\n",
        "    print(f\"üöÄ Judul yang perlu diterjemahkan: {len(indices_to_process)} dari {len(df)}\")\n",
        "\n",
        "    # ========== PROSES TRANSLASI (BATCH MODE) ==========\n",
        "    if len(indices_to_process) > 0:\n",
        "        # Ambil list judul yang akan diterjemahkan\n",
        "        titles_en = df.loc[indices_to_process, 'title'].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "        translated_titles = []\n",
        "\n",
        "        # Gunakan tqdm untuk memantau proses\n",
        "        print(f\"‚öôÔ∏è  Memproses translasi judul {dataset_name}...\")\n",
        "        for out in tqdm(translator(titles_en, truncation=True, max_length=512),\n",
        "                       total=len(titles_en),\n",
        "                       desc=f\"Translating {dataset_name} Titles\"):\n",
        "            translated_titles.append(out['translation_text'])\n",
        "\n",
        "        # Masukkan kembali hasil translasi ke DataFrame\n",
        "        for i, idx in enumerate(indices_to_process):\n",
        "            df.at[idx, 'title_id'] = translated_titles[i]\n",
        "\n",
        "        print(f\"‚úÖ Translasi {dataset_name} selesai!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Semua judul {dataset_name} sudah diterjemahkan sebelumnya!\")\n",
        "\n",
        "    # ========== SIMPAN HASIL ==========\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"üíæ File disimpan sebagai: {output_file}\")\n",
        "    print(f\"üìä Total baris: {len(df)}\")\n",
        "\n",
        "    # Tampilkan sampel singkat\n",
        "    print(f\"\\nüîç Sampel Hasil {dataset_name}:\")\n",
        "    print(df[['title', 'title_id']].head(3))\n",
        "\n",
        "    return df\n",
        "\n",
        "# ========== TRANSLATE HEALTH RELEASE TITLES ==========\n",
        "df_release = translate_titles(\n",
        "    input_file=INPUT_FILE_RELEASE,\n",
        "    output_file=OUTPUT_FILE_RELEASE,\n",
        "    dataset_name=\"HEALTH RELEASE\"\n",
        ")\n",
        "\n",
        "# ========== TRANSLATE HEALTH STORY TITLES ==========\n",
        "df_story = translate_titles(\n",
        "    input_file=INPUT_FILE_STORY,\n",
        "    output_file=OUTPUT_FILE_STORY,\n",
        "    dataset_name=\"HEALTH STORY\"\n",
        ")\n",
        "\n",
        "# ========== SUMMARY FINAL ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ SEMUA TRANSLASI TITLE SELESAI!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä STATISTIK FINAL:\")\n",
        "print(f\"   ‚Ä¢ Health Release: {len(df_release)} baris\")\n",
        "print(f\"   ‚Ä¢ Health Story: {len(df_story)} baris\")\n",
        "\n",
        "print(f\"\\nüìÅ FILE OUTPUT FINAL:\")\n",
        "print(f\"   ‚úÖ {OUTPUT_FILE_RELEASE}\")\n",
        "print(f\"   ‚úÖ {OUTPUT_FILE_STORY}\")\n",
        "\n",
        "print(f\"\\nüìã KOLOM TERSEDIA:\")\n",
        "print(f\"   ‚Ä¢ Health Release: {list(df_release.columns)}\")\n",
        "print(f\"   ‚Ä¢ Health Story: {list(df_story.columns)}\")\n",
        "\n",
        "# Verifikasi tidak ada yang kosong\n",
        "release_missing = df_release['title_id'].isna().sum()\n",
        "story_missing = df_story['title_id'].isna().sum()\n",
        "\n",
        "print(f\"\\nüîç VERIFIKASI:\")\n",
        "print(f\"   ‚Ä¢ Health Release - Title kosong: {release_missing}\")\n",
        "print(f\"   ‚Ä¢ Health Story - Title kosong: {story_missing}\")\n",
        "\n",
        "if release_missing == 0 and story_missing == 0:\n",
        "    print(\"\\n‚úÖ SEMUA TITLE BERHASIL DITERJEMAHKAN!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Ada beberapa title yang masih kosong, silakan cek data!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ PROSES SELESAI - DATASET TETAP TERPISAH!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "kXjfnIfvalat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAPIKAN"
      ],
      "metadata": {
        "id": "Fmi-I9vUZ7Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_cek = pd.read_csv('health_translated_id_COMPLETE.csv')\n",
        "print(df_cek.head(10)) # Melihat 10 baris pertama\n",
        "print(df_cek.info()) # Mengecek jumlah baris dan kolom yang terdeteksi"
      ],
      "metadata": {
        "id": "JHuN270hZ9mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_id_text(text):\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower() # Lowercase agar seragam\n",
        "\n",
        "    # 1. Hapus sisa-sisa karakter aneh hasil translasi\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', ' ', text)\n",
        "\n",
        "    # 2. Hapus spasi berlebih di tengah dan di ujung\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def prepare_final_dataset(input_file, output_file, dataset_name):\n",
        "    \"\"\"\n",
        "    Fungsi untuk memproses dataset final secara terpisah\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üßπ CLEANING & PREPARING FINAL DATASET - {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Load hasil translasi\n",
        "    df = pd.read_csv(input_file)\n",
        "    print(f\"üìÑ Loaded: {input_file} ({len(df)} baris)\")\n",
        "\n",
        "    # 1. Gabungkan Title dan Text (Title + separator + Text)\n",
        "    # Penggunaan titik setelah judul membantu model tahu batas kalimat\n",
        "    print(f\"üîó Menggabungkan title_id dan text_id untuk {dataset_name}...\")\n",
        "    df['full_text_id'] = df['title_id'].astype(str) + \". \" + df['text_id'].astype(str)\n",
        "\n",
        "    # 2. Jalankan pembersihan teks Indonesia\n",
        "    print(f\"üßπ Membersihkan teks bahasa Indonesia untuk {dataset_name}...\")\n",
        "    df['text_cleaned_id'] = df['full_text_id'].apply(clean_id_text)\n",
        "\n",
        "    # 3. Pilih kolom final saja\n",
        "    df_final = df[['text_cleaned_id', 'label']].copy()\n",
        "\n",
        "    # 4. Hapus baris yang mungkin kosong\n",
        "    before_drop = len(df_final)\n",
        "    df_final = df_final.dropna()\n",
        "    df_final = df_final[df_final['text_cleaned_id'].str.len() > 10]\n",
        "    after_drop = len(df_final)\n",
        "\n",
        "    dropped = before_drop - after_drop\n",
        "    if dropped > 0:\n",
        "        print(f\"üóëÔ∏è  Dihapus {dropped} baris kosong/pendek\")\n",
        "\n",
        "    # 5. Simpan dataset final\n",
        "    df_final.to_csv(output_file, index=False)\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"‚úÖ {dataset_name} Final Siap! Total data: {len(df_final)}\")\n",
        "    print(f\"\\nüìä Distribusi Label {dataset_name}:\")\n",
        "    print(df_final['label'].value_counts())\n",
        "    print(f\"\\nüíæ Disimpan sebagai: {output_file}\")\n",
        "\n",
        "    # Tampilkan sampel\n",
        "    print(f\"\\nüîç Sampel Data {dataset_name} (2 baris pertama):\")\n",
        "    for idx, row in df_final.head(2).iterrows():\n",
        "        print(f\"\\n--- Sampel {idx + 1} ---\")\n",
        "        print(f\"Text: {row['text_cleaned_id'][:200]}...\")\n",
        "        print(f\"Label: {row['label']}\")\n",
        "\n",
        "    return df_final\n",
        "\n",
        "# ========== FILE INPUT (hasil translasi title sebelumnya) ==========\n",
        "INPUT_FILE_RELEASE = \"health_release_translated_id_COMPLETE.csv\"\n",
        "INPUT_FILE_STORY = \"health_story_translated_id_COMPLETE.csv\"\n",
        "\n",
        "# ========== FILE OUTPUT FINAL (tetap terpisah) ==========\n",
        "OUTPUT_FILE_RELEASE = \"health_release_id_FINAL.csv\"\n",
        "OUTPUT_FILE_STORY = \"health_story_id_FINAL.csv\"\n",
        "\n",
        "# ========== PREPARE HEALTH RELEASE FINAL ==========\n",
        "df_release_final = prepare_final_dataset(\n",
        "    input_file=INPUT_FILE_RELEASE,\n",
        "    output_file=OUTPUT_FILE_RELEASE,\n",
        "    dataset_name=\"HEALTH RELEASE\"\n",
        ")\n",
        "\n",
        "# ========== PREPARE HEALTH STORY FINAL ==========\n",
        "df_story_final = prepare_final_dataset(\n",
        "    input_file=INPUT_FILE_STORY,\n",
        "    output_file=OUTPUT_FILE_STORY,\n",
        "    dataset_name=\"HEALTH STORY\"\n",
        ")\n",
        "\n",
        "# ========== SUMMARY FINAL ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ SEMUA DATASET FINAL SIAP!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä RINGKASAN STATISTIK:\")\n",
        "print(f\"\\n   Health Release:\")\n",
        "print(f\"      ‚Ä¢ Total data: {len(df_release_final)}\")\n",
        "print(f\"      ‚Ä¢ Distribusi: {dict(df_release_final['label'].value_counts())}\")\n",
        "\n",
        "print(f\"\\n   Health Story:\")\n",
        "print(f\"      ‚Ä¢ Total data: {len(df_story_final)}\")\n",
        "print(f\"      ‚Ä¢ Distribusi: {dict(df_story_final['label'].value_counts())}\")\n",
        "\n",
        "print(f\"\\nüìÅ FILE OUTPUT FINAL:\")\n",
        "print(f\"   ‚úÖ {OUTPUT_FILE_RELEASE}\")\n",
        "print(f\"   ‚úÖ {OUTPUT_FILE_STORY}\")\n",
        "\n",
        "print(f\"\\nüìã KOLOM DALAM DATASET FINAL:\")\n",
        "print(f\"   ‚Ä¢ {list(df_release_final.columns)}\")\n",
        "\n",
        "# Verifikasi tidak ada data kosong\n",
        "release_empty = (df_release_final['text_cleaned_id'].str.len() <= 10).sum()\n",
        "story_empty = (df_story_final['text_cleaned_id'].str.len() <= 10).sum()\n",
        "\n",
        "print(f\"\\nüîç VERIFIKASI KUALITAS DATA:\")\n",
        "print(f\"   ‚Ä¢ Health Release - Text terlalu pendek: {release_empty}\")\n",
        "print(f\"   ‚Ä¢ Health Story - Text terlalu pendek: {story_empty}\")\n",
        "\n",
        "if release_empty == 0 and story_empty == 0:\n",
        "    print(\"\\n‚úÖ SEMUA DATA BERKUALITAS BAIK!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Ada beberapa data yang mungkin perlu dicek ulang\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ PREPROCESSING LENGKAP - DATASET SIAP UNTUK MODELING!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüí° Dataset sudah dalam bahasa Indonesia dan siap digunakan untuk:\")\n",
        "print(\"   ‚Ä¢ Training IndoBERT\")\n",
        "print(\"   ‚Ä¢ Training BiLSTM\")\n",
        "print(\"   ‚Ä¢ Ensemble Model\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "CGVmsHdfaNso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "rvZaxN7if5g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DAFTAR KATA HUBUNG & KATA TUGAS (STOPWORDS) - LEBIH LENGKAP\n",
        "# ============================================================================\n",
        "# Daftar ini hanya digunakan untuk FILTER VISUALISASI, bukan menghapus data asli\n",
        "stopwords_id = set([\n",
        "    'yang', 'untuk', 'pada', 'ke', 'dari', 'dalam', 'dengan', 'ini', 'itu', 'di',\n",
        "    'dan', 'atau', 'adalah', 'akan', 'ada', 'juga', 'sudah', 'telah', 'saya',\n",
        "    'bahwa', 'oleh', 'tersebut', 'bisa', 'lebih', 'karena', 'sebagai', 'namun',\n",
        "    'serta', 'tetapi', 'tapi', 'maka', 'jika', 'bila', 'apabila', 'kalau', 'hanya',\n",
        "    'secara', 'menjadi', 'masih', 'belum', 'pernah', 'banyak', 'setiap', 'suatu',\n",
        "    'sebuah', 'ia', 'dia', 'mereka', 'kita', 'kami', 'anda', 'hal', 'paling',\n",
        "    'sangat', 'sekali', 'tentang', 'hingga', 'kepada', 'terhadap', 'atas', 'bagi',\n",
        "    'oleh', 'saat', 'ketika', 'setelah', 'sebelum', 'lalu', 'kemudian', 'yaitu',\n",
        "    'yakni', 'ialah', 'merupakan', 'mungkin', 'ingin', 'para', 'seperti', 'begitu', 'tidak','dapat', 'memiliki'\n",
        "])\n",
        "\n",
        "# ============================================\n",
        "# 2. FUNGSI PREPROCESSING KHUSUS VISUALISASI\n",
        "# ============================================\n",
        "def clean_text_for_viz(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text) # Hapus simbol/angka\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# ============================================\n",
        "# 3. FUNGSI TOP N WORDS\n",
        "# ============================================\n",
        "def get_top_n_words(corpus, n=20):\n",
        "    all_words = ' '.join(corpus).split()\n",
        "    # FILTER: Tidak ada di stopwords_id dan panjang kata > 3\n",
        "    filtered_words = [w for w in all_words if w not in stopwords_id and len(w) > 3]\n",
        "    return pd.DataFrame(Counter(filtered_words).most_common(n), columns=['Word', 'Freq'])\n",
        "\n",
        "# ============================================\n",
        "# 4. FUNGSI UTAMA UNTUK EDA\n",
        "# ============================================\n",
        "def perform_eda(df, dataset_name, output_prefix):\n",
        "    \"\"\"\n",
        "    Fungsi untuk melakukan Exploratory Data Analysis (EDA) pada satu dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üìä EXPLORATORY DATA ANALYSIS - {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Pastikan ada kolom label_name\n",
        "    if 'label_name' not in df.columns:\n",
        "        if df['label'].dtype in ['int64', 'int32']:\n",
        "            df['label_name'] = df['label'].map({1: 'Real', 0: 'Fake'})\n",
        "        else:\n",
        "            df['label_name'] = df['label']\n",
        "        print(\"‚úÖ Kolom 'label_name' dibuat\")\n",
        "\n",
        "    # Buat kolom untuk visualisasi (tidak merusak data asli)\n",
        "    df['eda_clean_id'] = df['text_cleaned_id'].apply(clean_text_for_viz)\n",
        "\n",
        "    # ============================================\n",
        "    # VISUALISASI 1: DISTRIBUSI KELAS\n",
        "    # ============================================\n",
        "    print(f\"\\nüìà Membuat visualisasi distribusi kelas untuk {dataset_name}...\")\n",
        "    plt.figure(figsize=(7, 5))\n",
        "    sns.countplot(data=df, x='label_name', palette=['#2ecc71', '#e74c3c'])\n",
        "    plt.title(f'Distribusi Data: Berita ASLI vs PALSU\\n({dataset_name})', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Label', fontsize=12)\n",
        "    plt.ylabel('Jumlah', fontsize=12)\n",
        "\n",
        "    # Tambahkan angka di atas bar\n",
        "    ax = plt.gca()\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{int(p.get_height())}',\n",
        "                   (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_prefix}_distribusi_kelas.png', dpi=300)\n",
        "    print(f\"‚úÖ Disimpan: {output_prefix}_distribusi_kelas.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================\n",
        "    # VISUALISASI 2: TOP 20 WORDS (TANPA KATA HUBUNG)\n",
        "    # ============================================\n",
        "    print(f\"\\nüìä Membuat visualisasi Top 20 Words untuk {dataset_name}...\")\n",
        "    top_real = get_top_n_words(df[df['label_name'] == 'Real']['eda_clean_id'])\n",
        "    top_fake = get_top_n_words(df[df['label_name'] == 'Fake']['eda_clean_id'])\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
        "    sns.barplot(data=top_real, x='Freq', y='Word', ax=ax[0], palette='Greens_r')\n",
        "    ax[0].set_title(f'Top 20 Kata Bermakna: Berita ASLI\\n({dataset_name})', fontsize=15, fontweight='bold')\n",
        "    ax[0].grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    sns.barplot(data=top_fake, x='Freq', y='Word', ax=ax[1], palette='Reds_r')\n",
        "    ax[1].set_title(f'Top 20 Kata Bermakna: Berita PALSU\\n({dataset_name})', fontsize=15, fontweight='bold')\n",
        "    ax[1].grid(axis='x', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_prefix}_top20_words.png', dpi=300)\n",
        "    print(f\"‚úÖ Disimpan: {output_prefix}_top20_words.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================\n",
        "    # VISUALISASI 3: WORD CLOUD (TANPA KATA HUBUNG)\n",
        "    # ============================================\n",
        "    print(f\"\\n‚òÅÔ∏è  Membuat WordCloud untuk {dataset_name}...\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    # WordCloud Real\n",
        "    real_text = ' '.join(df[df['label_name'] == 'Real']['eda_clean_id'])\n",
        "    wc_real = WordCloud(width=800, height=500, background_color='white',\n",
        "                        stopwords=stopwords_id, colormap='summer',\n",
        "                        min_font_size=10).generate(real_text)\n",
        "    axes[0].imshow(wc_real)\n",
        "    axes[0].set_title(f'WordCloud: Konten Berita ASLI\\n({dataset_name})', fontsize=18, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # WordCloud Fake\n",
        "    fake_text = ' '.join(df[df['label_name'] == 'Fake']['eda_clean_id'])\n",
        "    wc_fake = WordCloud(width=800, height=500, background_color='white',\n",
        "                        stopwords=stopwords_id, colormap='autumn',\n",
        "                        min_font_size=10).generate(fake_text)\n",
        "    axes[1].imshow(wc_fake)\n",
        "    axes[1].set_title(f'WordCloud: Konten Berita PALSU\\n({dataset_name})', fontsize=18, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_prefix}_wordcloud.png', dpi=300)\n",
        "    print(f\"‚úÖ Disimpan: {output_prefix}_wordcloud.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================\n",
        "    # VISUALISASI 4: PANJANG TEKS & RINGKASAN\n",
        "    # ============================================\n",
        "    print(f\"\\nüìè Membuat analisis panjang teks untuk {dataset_name}...\")\n",
        "    df['word_count'] = df['text_cleaned_id'].str.split().str.len()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    sns.boxplot(data=df, x='label_name', y='word_count', palette=['#2ecc71', '#e74c3c'])\n",
        "    plt.title(f'Perbandingan Panjang Kata per Artikel\\n({dataset_name})', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Label', fontsize=12)\n",
        "    plt.ylabel('Jumlah Kata', fontsize=12)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_prefix}_word_count.png', dpi=300)\n",
        "    print(f\"‚úÖ Disimpan: {output_prefix}_word_count.png\")\n",
        "    plt.show()\n",
        "\n",
        "    # ============================================\n",
        "    # RINGKASAN STATISTIK\n",
        "    # ============================================\n",
        "    print(f\"\\nüìä RINGKASAN STATISTIK DATASET - {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    summary = df.groupby('label_name')['word_count'].agg(['count', 'mean', 'median', 'min', 'max', 'std']).round(2)\n",
        "    print(summary)\n",
        "\n",
        "    # Simpan summary ke CSV\n",
        "    summary.to_csv(f'{output_prefix}_summary_stats.csv')\n",
        "    print(f\"\\nüíæ Summary statistik disimpan: {output_prefix}_summary_stats.csv\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD KEDUA DATASET\n",
        "# ============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"üìÇ LOADING DATASETS FOR EDA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# File paths\n",
        "file_release = 'health_release_id_FINAL.csv'\n",
        "file_story = 'health_story_id_FINAL.csv'\n",
        "\n",
        "# Check files\n",
        "if not os.path.exists(file_release):\n",
        "    print(f\"‚ùå File {file_release} tidak ditemukan!\")\n",
        "else:\n",
        "    df_release = pd.read_csv(file_release)\n",
        "    print(f\"‚úÖ Health Release loaded: {len(df_release)} baris\")\n",
        "\n",
        "if not os.path.exists(file_story):\n",
        "    print(f\"‚ùå File {file_story} tidak ditemukan!\")\n",
        "else:\n",
        "    df_story = pd.read_csv(file_story)\n",
        "    print(f\"‚úÖ Health Story loaded: {len(df_story)} baris\")\n",
        "\n",
        "# ============================================================================\n",
        "# PERFORM EDA - HEALTH RELEASE\n",
        "# ============================================================================\n",
        "df_release_analyzed = perform_eda(\n",
        "    df=df_release,\n",
        "    dataset_name=\"HEALTH RELEASE\",\n",
        "    output_prefix=\"health_release_eda\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# PERFORM EDA - HEALTH STORY\n",
        "# ============================================================================\n",
        "df_story_analyzed = perform_eda(\n",
        "    df=df_story,\n",
        "    dataset_name=\"HEALTH STORY\",\n",
        "    output_prefix=\"health_story_eda\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY PERBANDINGAN KEDUA DATASET\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéØ SUMMARY PERBANDINGAN KEDUA DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä HEALTH RELEASE:\")\n",
        "print(f\"   ‚Ä¢ Total data: {len(df_release_analyzed)}\")\n",
        "print(f\"   ‚Ä¢ Rata-rata panjang kata (Real): {df_release_analyzed[df_release_analyzed['label_name']=='Real']['word_count'].mean():.2f}\")\n",
        "print(f\"   ‚Ä¢ Rata-rata panjang kata (Fake): {df_release_analyzed[df_release_analyzed['label_name']=='Fake']['word_count'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nüìä HEALTH STORY:\")\n",
        "print(f\"   ‚Ä¢ Total data: {len(df_story_analyzed)}\")\n",
        "print(f\"   ‚Ä¢ Rata-rata panjang kata (Real): {df_story_analyzed[df_story_analyzed['label_name']=='Real']['word_count'].mean():.2f}\")\n",
        "print(f\"   ‚Ä¢ Rata-rata panjang kata (Fake): {df_story_analyzed[df_story_analyzed['label_name']=='Fake']['word_count'].mean():.2f}\")\n",
        "\n",
        "print(\"\\nüìÅ FILE OUTPUT YANG DIHASILKAN:\")\n",
        "print(\"\\n   Health Release:\")\n",
        "print(\"   ‚Ä¢ health_release_eda_distribusi_kelas.png\")\n",
        "print(\"   ‚Ä¢ health_release_eda_top20_words.png\")\n",
        "print(\"   ‚Ä¢ health_release_eda_wordcloud.png\")\n",
        "print(\"   ‚Ä¢ health_release_eda_word_count.png\")\n",
        "print(\"   ‚Ä¢ health_release_eda_summary_stats.csv\")\n",
        "\n",
        "print(\"\\n   Health Story:\")\n",
        "print(\"   ‚Ä¢ health_story_eda_distribusi_kelas.png\")\n",
        "print(\"   ‚Ä¢ health_story_eda_top20_words.png\")\n",
        "print(\"   ‚Ä¢ health_story_eda_wordcloud.png\")\n",
        "print(\"   ‚Ä¢ health_story_eda_word_count.png\")\n",
        "print(\"   ‚Ä¢ health_story_eda_summary_stats.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ EXPLORATORY DATA ANALYSIS SELESAI!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüí° Hasil visualisasi dapat digunakan untuk:\")\n",
        "print(\"   ‚Ä¢ Analisis karakteristik berita real vs fake\")\n",
        "print(\"   ‚Ä¢ Identifikasi pola kata kunci\")\n",
        "print(\"   ‚Ä¢ Lampiran skripsi/thesis (BAB III - Analisis Data)\")\n",
        "print(\"   ‚Ä¢ Presentasi hasil penelitian\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "fjaPHXnff8Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DEFINISI FILTER KATA (STOPWORDS) - UNTUK VISUALISASI\n",
        "# ============================================================================\n",
        "# Kita masukkan 'tidak', 'itu', 'ini', dan kata hubung lainnya di sini\n",
        "stopwords_id = set([\n",
        "    'yang', 'untuk', 'pada', 'ke', 'dari', 'dalam', 'dengan', 'ini', 'itu', 'di',\n",
        "    'dan', 'atau', 'adalah', 'akan', 'ada', 'juga', 'sudah', 'telah', 'saya',\n",
        "    'bahwa', 'oleh', 'tersebut', 'bisa', 'lebih', 'karena', 'sebagai', 'tidak',\n",
        "    'bukan', 'jangan', 'namun', 'serta', 'tetapi', 'tapi', 'maka', 'jika', 'bila',\n",
        "    'hanya', 'secara', 'menjadi', 'masih', 'belum', 'pernah', 'banyak', 'setiap',\n",
        "    'suatu', 'sebuah', 'ia', 'dia', 'mereka', 'kita', 'kami', 'anda', 'hal',\n",
        "    'paling', 'sangat', 'sekali', 'tentang', 'hingga', 'kepada', 'terhadap',\n",
        "    'atas', 'bagi', 'saat', 'ketika', 'setelah', 'sebelum', 'lalu', 'kemudian',\n",
        "    'yaitu', 'yakni', 'ialah', 'merupakan', 'mungkin', 'ingin', 'para', 'seperti', 'tidak', 'dapat', 'memiliki'\n",
        "])\n",
        "\n",
        "# ============================================================================\n",
        "# 2. FUNGSI UNTUK MENDAPATKAN FREKUENSI KATA\n",
        "# ============================================================================\n",
        "def get_top_20_words(corpus):\n",
        "    all_words = []\n",
        "    for text in corpus:\n",
        "        if pd.isna(text): continue\n",
        "        # Bersihkan teks (hanya huruf, lowercase)\n",
        "        text = re.sub(r'[^a-z\\s]', ' ', str(text).lower())\n",
        "        # Filter kata hubung & kata pendek (< 4 huruf seringkali bukan kata konten)\n",
        "        words = [w for w in text.split() if w not in stopwords_id and len(w) > 3]\n",
        "        all_words.extend(words)\n",
        "\n",
        "    # Hitung 20 terbanyak\n",
        "    return pd.DataFrame(Counter(all_words).most_common(20), columns=['Kata', 'Frekuensi'])\n",
        "\n",
        "def analyze_and_visualize_dataset(df, dataset_name, output_prefix):\n",
        "    \"\"\"\n",
        "    Fungsi untuk analisis dan visualisasi frekuensi kata untuk satu dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üìä ANALISIS FREKUENSI KATA - {dataset_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Pastikan ada kolom label_name (mapping 0/1 ke Real/Fake)\n",
        "    if 'label_name' not in df.columns:\n",
        "        df['label_name'] = df['label'].map({0: 'Fake', 1: 'Real'})\n",
        "        print(\"‚úÖ Kolom 'label_name' dibuat (0=Fake, 1=Real)\")\n",
        "\n",
        "    # Statistik dataset\n",
        "    print(f\"\\nüìã Statistik {dataset_name}:\")\n",
        "    print(f\"   ‚Ä¢ Total data: {len(df)}\")\n",
        "    print(f\"   ‚Ä¢ Distribusi label:\\n{df['label_name'].value_counts()}\")\n",
        "\n",
        "    # ============================================================================\n",
        "    # 3. PROSES DATA & VISUALISASI\n",
        "    # ============================================================================\n",
        "    # Ambil data berdasarkan label\n",
        "    print(f\"\\nüîç Menganalisis frekuensi kata untuk {dataset_name}...\")\n",
        "    top_real = get_top_20_words(df[df['label_name'] == 'Real']['text_cleaned_id'])\n",
        "    top_fake = get_top_20_words(df[df['label_name'] == 'Fake']['text_cleaned_id'])\n",
        "\n",
        "    # Plotting\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
        "\n",
        "    # Grafik Berita ASLI\n",
        "    sns.barplot(data=top_real, x='Frekuensi', y='Kata', ax=ax[0], palette='Greens_r')\n",
        "    ax[0].set_title(f'Top 20 Kata Bermakna: Berita Kesehatan ASLI\\n({dataset_name})',\n",
        "                    fontsize=15, fontweight='bold')\n",
        "    ax[0].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Grafik Berita PALSU\n",
        "    sns.barplot(data=top_fake, x='Frekuensi', y='Kata', ax=ax[1], palette='Reds_r')\n",
        "    ax[1].set_title(f'Top 20 Kata Bermakna: Berita Kesehatan PALSU\\n({dataset_name})',\n",
        "                    fontsize=15, fontweight='bold')\n",
        "    ax[1].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Simpan gambar\n",
        "    output_file = f'{output_prefix}_top_20_words_comparison.png'\n",
        "    plt.savefig(output_file, dpi=300)\n",
        "    print(f\"‚úÖ Visualisasi disimpan: {output_file}\")\n",
        "    plt.show()\n",
        "\n",
        "    # Menampilkan tabel frekuensi untuk lampiran skripsi\n",
        "    print(f\"\\n--- TABEL FREKUENSI KATA ASLI ({dataset_name}) ---\")\n",
        "    print(top_real.to_string(index=False))\n",
        "    print(f\"\\n--- TABEL FREKUENSI KATA PALSU ({dataset_name}) ---\")\n",
        "    print(top_fake.to_string(index=False))\n",
        "\n",
        "    # Simpan tabel ke CSV untuk lampiran\n",
        "    top_real.to_csv(f'{output_prefix}_freq_real.csv', index=False)\n",
        "    top_fake.to_csv(f'{output_prefix}_freq_fake.csv', index=False)\n",
        "    print(f\"\\nüíæ Tabel frekuensi disimpan:\")\n",
        "    print(f\"   ‚Ä¢ {output_prefix}_freq_real.csv\")\n",
        "    print(f\"   ‚Ä¢ {output_prefix}_freq_fake.csv\")\n",
        "\n",
        "    return top_real, top_fake\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD KEDUA DATASET\n",
        "# ============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"üìÇ LOADING DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load Health Release\n",
        "df_release = pd.read_csv('health_release_id_FINAL.csv')\n",
        "print(f\"‚úÖ Health Release loaded: {len(df_release)} baris\")\n",
        "\n",
        "# Load Health Story\n",
        "df_story = pd.read_csv('health_story_id_FINAL.csv')\n",
        "print(f\"‚úÖ Health Story loaded: {len(df_story)} baris\")\n",
        "\n",
        "# ============================================================================\n",
        "# ANALISIS HEALTH RELEASE\n",
        "# ============================================================================\n",
        "top_real_release, top_fake_release = analyze_and_visualize_dataset(\n",
        "    df=df_release,\n",
        "    dataset_name=\"HEALTH RELEASE\",\n",
        "    output_prefix=\"health_release\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# ANALISIS HEALTH STORY\n",
        "# ============================================================================\n",
        "top_real_story, top_fake_story = analyze_and_visualize_dataset(\n",
        "    df=df_story,\n",
        "    dataset_name=\"HEALTH STORY\",\n",
        "    output_prefix=\"health_story\"\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY PERBANDINGAN\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéØ SUMMARY ANALISIS FREKUENSI KATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìä HEALTH RELEASE:\")\n",
        "print(f\"   ‚Ä¢ Kata terpopuler (Real): {top_real_release.iloc[0]['Kata']} ({top_real_release.iloc[0]['Frekuensi']} kali)\")\n",
        "print(f\"   ‚Ä¢ Kata terpopuler (Fake): {top_fake_release.iloc[0]['Kata']} ({top_fake_release.iloc[0]['Frekuensi']} kali)\")\n",
        "\n",
        "print(\"\\nüìä HEALTH STORY:\")\n",
        "print(f\"   ‚Ä¢ Kata terpopuler (Real): {top_real_story.iloc[0]['Kata']} ({top_real_story.iloc[0]['Frekuensi']} kali)\")\n",
        "print(f\"   ‚Ä¢ Kata terpopuler (Fake): {top_fake_story.iloc[0]['Kata']} ({top_fake_story.iloc[0]['Frekuensi']} kali)\")\n",
        "\n",
        "print(\"\\nüìÅ FILE OUTPUT YANG DIHASILKAN:\")\n",
        "print(\"   Visualisasi:\")\n",
        "print(\"   ‚Ä¢ health_release_top_20_words_comparison.png\")\n",
        "print(\"   ‚Ä¢ health_story_top_20_words_comparison.png\")\n",
        "print(\"\\n   Tabel CSV:\")\n",
        "print(\"   ‚Ä¢ health_release_freq_real.csv\")\n",
        "print(\"   ‚Ä¢ health_release_freq_fake.csv\")\n",
        "print(\"   ‚Ä¢ health_story_freq_real.csv\")\n",
        "print(\"   ‚Ä¢ health_story_freq_fake.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ ANALISIS FREKUENSI KATA SELESAI!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüí° Hasil visualisasi dan tabel dapat digunakan untuk:\")\n",
        "print(\"   ‚Ä¢ Lampiran skripsi/thesis\")\n",
        "print(\"   ‚Ä¢ Analisis pola kata pada berita real vs fake\")\n",
        "print(\"   ‚Ä¢ Identifikasi keyword khas masing-masing kategori\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "S9s02YlEhcsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SPLIT & DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "ptgLlJJpDh36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load Dataset Final\n",
        "# Pastikan file 'health_release_id_FINAL.csv' berada di folder yang sama\n",
        "df = pd.read_csv('health_release_id_FINAL.csv')\n",
        "\n",
        "# 2. Split Data (70% Train, 15% Val, 15% Test)\n",
        "# Stratify memastikan proporsi label di tiap set sama dengan dataset asli\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# Split sisa 30% menjadi Val (15%) dan Test (15%)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä UKURAN DATASET SEBELUM BALANCING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Total: {len(df)}\")\n",
        "\n",
        "# 3. Data Augmentation Sederhana (Oversampling) pada DATA TRAIN\n",
        "# Memisahkan data berdasarkan label\n",
        "train_label_0 = train_df[train_df['label'] == 0]\n",
        "train_label_1 = train_df[train_df['label'] == 1]\n",
        "\n",
        "print(\"\\nDistribusi awal data Train:\")\n",
        "print(f\"Label 0: {len(train_label_0)}\")\n",
        "print(f\"Label 1: {len(train_label_1)}\")\n",
        "\n",
        "# LOGIKA DINAMIS: Menentukan label mana yang perlu ditambah\n",
        "if len(train_label_0) > len(train_label_1):\n",
        "    # Jika Label 1 lebih sedikit\n",
        "    n_samples_to_add = len(train_label_0) - len(train_label_1)\n",
        "    train_label_1_augmented = train_label_1.sample(n_samples_to_add, replace=True, random_state=42)\n",
        "    train_df_balanced = pd.concat([train_label_0, train_label_1, train_label_1_augmented])\n",
        "    print(f\"--> Menambahkan {n_samples_to_add} sampel pada Label 1\")\n",
        "\n",
        "elif len(train_label_1) > len(train_label_0):\n",
        "    # Jika Label 0 lebih sedikit\n",
        "    n_samples_to_add = len(train_label_1) - len(train_label_0)\n",
        "    train_label_0_augmented = train_label_0.sample(n_samples_to_add, replace=True, random_state=42)\n",
        "    train_df_balanced = pd.concat([train_label_0, train_label_0_augmented, train_label_1])\n",
        "    print(f\"--> Menambahkan {n_samples_to_add} sampel pada Label 0\")\n",
        "\n",
        "else:\n",
        "    # Jika data sudah seimbang\n",
        "    train_df_balanced = train_df.copy()\n",
        "    print(\"--> Data sudah seimbang, tidak ada perubahan.\")\n",
        "\n",
        "# Mengacak urutan data agar label tidak berkelompok\n",
        "train_df_balanced = shuffle(train_df_balanced, random_state=42)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ UKURAN SETELAH BALANCING (TRAIN ONLY)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Label 0: {len(train_df_balanced[train_df_balanced['label']==0])}\")\n",
        "print(f\"Label 1: {len(train_df_balanced[train_df_balanced['label']==1])}\")\n",
        "print(f\"Total Train: {len(train_df_balanced)}\")\n",
        "\n",
        "# 4. Simpan ke CSV Terpisah\n",
        "train_df_balanced.to_csv('train_dataset_augmented.csv', index=False)\n",
        "val_df.to_csv('val_dataset.csv', index=False)\n",
        "test_df.to_csv('test_dataset.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üíæ FILE BERHASIL DISIMPAN\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ train_dataset_augmented.csv\")\n",
        "print(\"‚úÖ val_dataset.csv\")\n",
        "print(\"‚úÖ test_dataset.csv\")"
      ],
      "metadata": {
        "id": "EIFc9lpzDmhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Pengaturan gaya visualisasi\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# --- VISUALISASI 1: PROPORSI PEMBAGIAN DATA ---\n",
        "plt.subplot(1, 2, 1)\n",
        "split_counts = [len(train_df), len(val_df), len(test_df)]\n",
        "split_labels = ['Train (Original)', 'Validation', 'Test']\n",
        "colors = ['#4e79a7', '#f28e2b', '#e15759']\n",
        "\n",
        "plt.pie(split_counts, labels=split_labels, autopct='%1.1f%%',\n",
        "        startangle=140, colors=colors, pctdistance=0.85,\n",
        "        explode=[0.05, 0, 0])\n",
        "\n",
        "# Membuat lubang di tengah agar jadi Donut Chart\n",
        "centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "plt.title(\"Proporsi Pembagian Dataset Utama\", fontsize=14, pad=20)\n",
        "\n",
        "\n",
        "# --- VISUALISASI 2: PERBANDINGAN LABEL (BEFORE VS AFTER BALANCING) ---\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Data untuk plotting\n",
        "labels = ['Label 0 (Real)', 'Label 1 (Fake)']\n",
        "before_balancing = [len(train_label_0), len(train_label_1)]\n",
        "after_balancing = [\n",
        "    len(train_df_balanced[train_df_balanced['label']==0]),\n",
        "    len(train_df_balanced[train_df_balanced['label']==1])\n",
        "]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, before_balancing, width, label='Sebelum Balancing', color='#a0cbe8')\n",
        "plt.bar(x + width/2, after_balancing, width, label='Sesudah Balancing', color='#4e79a7')\n",
        "\n",
        "plt.ylabel('Jumlah Sampel')\n",
        "plt.title('Dampak Balancing pada Data Training', fontsize=14)\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "\n",
        "# Tambahkan label angka di atas bar\n",
        "for i, v in enumerate(before_balancing):\n",
        "    plt.text(i - width/2, v + 5, str(v), ha='center', fontweight='bold')\n",
        "for i, v in enumerate(after_balancing):\n",
        "    plt.text(i + width/2, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hNB-kLKCrOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data untuk visualisasi\n",
        "datasets = ['Train\\n(Sebelum)', 'Train\\n(Sesudah)', 'Val', 'Test']\n",
        "label_0 = [\n",
        "    len(train_label_0),\n",
        "    len(train_df_balanced[train_df_balanced['label']==0]),\n",
        "    len(val_df[val_df['label']==0]),\n",
        "    len(test_df[test_df['label']==0])\n",
        "]\n",
        "label_1 = [\n",
        "    len(train_label_1),\n",
        "    len(train_df_balanced[train_df_balanced['label']==1]),\n",
        "    len(val_df[val_df['label']==1]),\n",
        "    len(test_df[test_df['label']==1])\n",
        "]\n",
        "\n",
        "# Buat visualisasi\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Grafik 1: Stacked Bar Chart\n",
        "x = np.arange(len(datasets))\n",
        "width = 0.6\n",
        "\n",
        "ax1.bar(x, label_0, width, label='Label 0 (Real)', color='#4CAF50', alpha=0.8)\n",
        "ax1.bar(x, label_1, width, bottom=label_0, label='Label 1 (Fake)', color='#F44336', alpha=0.8)\n",
        "\n",
        "ax1.set_ylabel('Jumlah Sampel', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Distribusi Dataset Sebelum & Sesudah Balancing', fontsize=12, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(datasets)\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Tambahkan angka di bar\n",
        "for i, (l0, l1) in enumerate(zip(label_0, label_1)):\n",
        "    ax1.text(i, l0/2, str(l0), ha='center', va='center', fontweight='bold', color='white')\n",
        "    ax1.text(i, l0 + l1/2, str(l1), ha='center', va='center', fontweight='bold', color='white')\n",
        "\n",
        "# Grafik 2: Pie Chart Perbandingan\n",
        "sizes_before = [len(train_label_0), len(train_label_1)]\n",
        "sizes_after = [len(train_df_balanced[train_df_balanced['label']==0]),\n",
        "               len(train_df_balanced[train_df_balanced['label']==1])]\n",
        "\n",
        "ax2_left = plt.subplot(1, 2, 2)\n",
        "colors = ['#4CAF50', '#F44336']\n",
        "\n",
        "# Pie sebelum balancing\n",
        "wedges1, texts1, autotexts1 = ax2_left.pie(\n",
        "    sizes_before, labels=['Label 0', 'Label 1'], autopct='%1.1f%%',\n",
        "    colors=colors, startangle=90, explode=(0.05, 0.05)\n",
        ")\n",
        "for autotext in autotexts1:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontweight('bold')\n",
        "\n",
        "ax2_left.set_title('Train - Sebelum vs Sesudah Balancing\\n(Outer: Sebelum | Inner: Sesudah)',\n",
        "                   fontsize=11, fontweight='bold')\n",
        "\n",
        "# Pie sesudah balancing (inner)\n",
        "wedges2, texts2, autotexts2 = ax2_left.pie(\n",
        "    sizes_after, autopct='%1.1f%%', colors=colors,\n",
        "    startangle=90, radius=0.7, textprops={'size': 9}\n",
        ")\n",
        "for autotext in autotexts2:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontweight('bold')\n",
        "    autotext.set_fontsize(9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary Statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà RINGKASAN VISUALISASI\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Peningkatan Label 1 (Train): {len(train_label_1)} ‚Üí {len(train_df_balanced[train_df_balanced['label']==1])} \"\n",
        "      f\"(+{n_samples_to_add} sampel)\")\n",
        "print(f\"Rasio Label 0:1 (Train Sebelum): {len(train_label_0)/len(train_label_1):.2f}:1\")\n",
        "print(f\"Rasio Label 0:1 (Train Sesudah): 1:1 (Balanced ‚úÖ)\")"
      ],
      "metadata": {
        "id": "PuvV6juvDTdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENISASI"
      ],
      "metadata": {
        "id": "3_uUi2mHER1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üî§ STEP 5: UNIFIED TOKENIZATION FOR INDOBERT & BILSTM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. SETUP & LOAD TOKENIZER\n",
        "MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
        "print(f\"\\n1Ô∏è‚É£ Loading IndoBERT Tokenizer: {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 2. LOAD DATASETS\n",
        "print(\"\\n2Ô∏è‚É£ Loading CSV datasets...\")\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset_augmented.csv')\n",
        "    val_df = pd.read_csv('val_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    print(f\"‚úÖ Data Loaded: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error: {e}. Pastikan file CSV hasil split sudah ada.\")\n",
        "    raise\n",
        "\n",
        "# 3. FUNGSI TOKENISASI\n",
        "def tokenize_process(texts, max_length=512):\n",
        "    print(f\"   Tokenizing {len(texts)} samples...\")\n",
        "    encoded = tokenizer(\n",
        "        texts.tolist(),\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='np'\n",
        "    )\n",
        "    return encoded['input_ids'], encoded['attention_mask']\n",
        "\n",
        "# 4. EKSEKUSI TOKENISASI\n",
        "print(\"\\n3Ô∏è‚É£ Running Tokenization (Max Length: 512)...\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Kolom target: text_cleaned_id\n",
        "text_col = 'text_cleaned_id'\n",
        "\n",
        "print(\"üìù Processing Train Set...\")\n",
        "train_ids, train_mask = tokenize_process(train_df[text_col])\n",
        "\n",
        "print(\"üìù Processing Validation Set...\")\n",
        "val_ids, val_mask = tokenize_process(val_df[text_col])\n",
        "\n",
        "print(\"üìù Processing Test Set...\")\n",
        "test_ids, test_mask = tokenize_process(test_df[text_col])\n",
        "\n",
        "# 5. SIMPAN DATA KE .NPY\n",
        "print(\"\\n4Ô∏è‚É£ Saving tokenized data to /content/tokenized_data/...\")\n",
        "os.makedirs('/content/tokenized_data', exist_ok=True)\n",
        "path = '/content/tokenized_data/'\n",
        "\n",
        "# Train\n",
        "np.save(path + 'train_ids.npy', train_ids)\n",
        "np.save(path + 'train_mask.npy', train_mask)\n",
        "np.save(path + 'train_labels.npy', train_df['label'].values)\n",
        "\n",
        "# Validation\n",
        "np.save(path + 'val_ids.npy', val_ids)\n",
        "np.save(path + 'val_mask.npy', val_mask)\n",
        "np.save(path + 'val_labels.npy', val_df['label'].values)\n",
        "\n",
        "# Test\n",
        "np.save(path + 'test_ids.npy', test_ids)\n",
        "np.save(path + 'test_mask.npy', test_mask)\n",
        "np.save(path + 'test_labels.npy', test_df['label'].values)\n",
        "\n",
        "# Simpan Tokenizer (Penting untuk Inference nanti)\n",
        "with open(path + 'indobert_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(f\"\\n‚úÖ SEMUA DATA BERHASIL DISIMPAN DI {path}\")\n",
        "\n",
        "# 6. VERIFIKASI AKHIR\n",
        "print(\"\\nüîç VERIFIKASI SHAPE:\")\n",
        "print(f\"   Train IDs: {train_ids.shape}\")\n",
        "print(f\"   Train Labels: {train_df['label'].values.shape}\")\n",
        "print(f\"   Vocab Size: {tokenizer.vocab_size}\")\n",
        "\n",
        "print(\"\\nüöÄ SIAP UNTUK STEP 6: MODEL TRAINING!\")"
      ],
      "metadata": {
        "id": "SU80wf1ZDzzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Membuat folder results jika belum ada\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "    print(\"‚úÖ Folder 'results' berhasil dibuat!\")"
      ],
      "metadata": {
        "id": "fYd-QrLDBpIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INDOBERT"
      ],
      "metadata": {
        "id": "jCcvm1WUEiJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from transformers import TFBertModel\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import gc\n",
        "\n",
        "# 1. SETUP & MEMORY CLEANUP\n",
        "keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"üîß Configuring Robust Training (10 Epochs, 6 Layers Frozen, LR 2e-5)...\")\n",
        "\n",
        "# 2. LOAD & PREPARE DATA\n",
        "PATH = '/content/tokenized_data/'\n",
        "train_ids = np.load(PATH + 'train_ids.npy')[:, :256]\n",
        "train_mask = np.load(PATH + 'train_mask.npy')[:, :256]\n",
        "train_labels = np.load(PATH + 'train_labels.npy')\n",
        "\n",
        "val_ids = np.load(PATH + 'val_ids.npy')[:, :256]\n",
        "val_mask = np.load(PATH + 'val_mask.npy')[:, :256]\n",
        "val_labels = np.load(PATH + 'val_labels.npy')\n",
        "\n",
        "test_ids = np.load(PATH + 'test_ids.npy')[:, :256]\n",
        "test_mask = np.load(PATH + 'test_mask.npy')[:, :256]\n",
        "test_labels = np.load(PATH + 'test_labels.npy')\n",
        "\n",
        "# 3. CALCULATE CLASS WEIGHTS\n",
        "weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights = {0: weights[0], 1: weights[1]}\n",
        "print(f\"‚öñÔ∏è Class Weights applied: {class_weights}\")\n",
        "\n",
        "# 4. ROBUST ARCHITECTURE\n",
        "class IndoBERTConnector(keras.layers.Layer):\n",
        "    def __init__(self, bert_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.bert = bert_model\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return output.last_hidden_state\n",
        "\n",
        "def build_robust_indobert(learning_rate=2e-5):\n",
        "    # Load Pre-trained IndoBERT\n",
        "    bert_backbone = TFBertModel.from_pretrained('indobenchmark/indobert-base-p1', from_pt=True)\n",
        "\n",
        "    # --- FREEZING STRATEGY ---\n",
        "    # Membekukan 6 layer awal (0-5)\n",
        "    for i in range(6):\n",
        "        bert_backbone.bert.encoder.layer[i].trainable = False\n",
        "        print(f\"Layer {i} is now frozen.\")\n",
        "\n",
        "    # Membekukan pooler untuk menghindari gradient warning\n",
        "    bert_backbone.bert.pooler.trainable = False\n",
        "\n",
        "    # Pastikan backbone utama tetap aktif untuk layer sisa (6-11)\n",
        "    bert_backbone.trainable = True\n",
        "\n",
        "    input_ids = keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    # BERT Layer\n",
        "    sequence_output = IndoBERTConnector(bert_backbone)([input_ids, attention_mask])\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "\n",
        "    # Robust Classification Head\n",
        "    x = keras.layers.Dense(512, activation='relu')(cls_token)\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    x = keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 5. EXECUTE TRAINING\n",
        "# Menggunakan LR 2e-5 untuk mengoptimalkan layer yang tidak di-freeze\n",
        "model = build_robust_indobert(learning_rate=2e-5)\n",
        "\n",
        "# EarlyStopping dihapus sesuai permintaan\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        './models/indobert_robust_final.weights.h5',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüöÄ Memulai Training (10 Epochs, Batch 8)...\")\n",
        "history = model.fit(\n",
        "    x=[train_ids, train_mask],\n",
        "    y=train_labels,\n",
        "    validation_data=([val_ids, val_mask], val_labels),\n",
        "    epochs=10, # Diubah menjadi 10 Epoch\n",
        "    batch_size=8,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 6. EVALUATION\n",
        "print(\"\\nüìä Evaluasi Akhir:\")\n",
        "test_pred_proba = model.predict([test_ids, test_mask], batch_size=8)\n",
        "test_pred = (test_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\nüéØ Accuracy: {accuracy_score(test_labels, test_pred):.4f}\")\n",
        "print(classification_report(test_labels, test_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan prediksi\n",
        "if not os.path.exists('./results'): os.makedirs('./results')\n",
        "np.save('./results/indobert_robust_10ep_preds.npy', test_pred_proba)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "7FmtMATbEj8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìä Evaluasi Akhir (Robust Model):\")\n",
        "test_pred_proba = model.predict([test_ids, test_mask], batch_size=8)\n",
        "test_pred = (test_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\nüéØ Accuracy: {accuracy_score(test_labels, test_pred):.4f}\")\n",
        "print(classification_report(test_labels, test_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan prediksi TEST untuk Ensemble\n",
        "np.save('./results/indobert_robust_test_preds.npy', test_pred_proba)\n",
        "\n",
        "# ‚ú® TAMBAHAN: Simpan prediksi TRAINING untuk Stacking Ensemble ‚ú®\n",
        "print(\"\\nüíæ Menyimpan prediksi training set untuk Stacking...\")\n",
        "train_pred_proba = model.predict([train_ids, train_mask], batch_size=8)\n",
        "np.save('./results/indobert_robust_train_preds.npy', train_pred_proba)\n",
        "print(\"‚úÖ Training predictions saved: ./results/indobert_robust_train_preds.npy\")\n",
        "\n",
        "# ‚ú® BONUS: Simpan prediksi VALIDATION juga (opsional, untuk analisis) ‚ú®\n",
        "val_pred_proba = model.predict([val_ids, val_mask], batch_size=8)\n",
        "np.save('./results/indobert_robust_val_preds.npy', val_pred_proba)\n",
        "print(\"‚úÖ Validation predictions saved: ./results/indobert_robust_val_preds.npy\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "1YdO4aGWmuzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI HASIL TRAINING (10 EPOCHS)\n",
        "# ============================================================================\n",
        "\n",
        "# Mengambil data dari objek history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "# Membuat Frame Gambar\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# 1. Plot Training & Validation Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, 'b-o', label='Training Accuracy', markersize=6) # 'b-o' artinya biru, garis, dan titik\n",
        "plt.plot(epochs_range, val_acc, 'r-s', label='Validation Accuracy', markersize=6) # 'r-s' artinya merah, garis, dan kotak\n",
        "plt.title('Training and Validation Accuracy', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.xticks(epochs_range) # Memastikan angka epoch muncul semua di sumbu X\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. Plot Training & Validation Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, 'b-o', label='Training Loss', markersize=6)\n",
        "plt.plot(epochs_range, val_loss, 'r-s', label='Validation Loss', markersize=6)\n",
        "plt.title('Training and Validation Loss', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.xticks(epochs_range)\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Menyimpan hasil visualisasi untuk Skripsi\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/training_curves_indobert_10ep.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualisasi selesai dibuat dan disimpan di folder './results/'\")"
      ],
      "metadata": {
        "id": "q6DrudYRdXzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Set style untuk visualisasi\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (16, 12)\n",
        "\n",
        "# ========== 1. TRAINING & VALIDATION CURVES ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training & Validation Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='#e74c3c')\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='#3498db')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Tambahkan marker untuk best epoch\n",
        "best_epoch = np.argmin(history.history['val_loss'])\n",
        "axes[0, 0].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "\n",
        "# Plot 2: Training & Validation Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='#e74c3c')\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#3498db')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Plot 3: Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
        "            ax=axes[1, 0], annot_kws={'size': 16, 'weight': 'bold'})\n",
        "axes[1, 0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Tambahkan persentase di dalam cell\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        percentage = cm[i, j] / cm[i].sum() * 100\n",
        "        axes[1, 0].text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                       ha='center', va='center', fontsize=10, color='gray')\n",
        "\n",
        "# Plot 4: ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "axes[1, 1].plot(fpr, tpr, color='#e74c3c', linewidth=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "axes[1, 1].plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
        "axes[1, 1].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_title('ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].fill_between(fpr, tpr, alpha=0.2, color='#e74c3c')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/indobert_training_visualization.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi disimpan: ./results/indobert_training_visualization.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== 2. METRICS SUMMARY TABLE ==========\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RINGKASAN METRIK EVALUASI\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics_data = {\n",
        "    'Metric': ['Accuracy', 'Precision (Real)', 'Precision (Fake)',\n",
        "               'Recall (Real)', 'Recall (Fake)', 'F1-Score (Real)', 'F1-Score (Fake)', 'AUC-ROC'],\n",
        "    'Value': [\n",
        "        accuracy_score(test_labels, test_pred),\n",
        "        precision_score(test_labels, test_pred, pos_label=0),\n",
        "        precision_score(test_labels, test_pred, pos_label=1),\n",
        "        recall_score(test_labels, test_pred, pos_label=0),\n",
        "        recall_score(test_labels, test_pred, pos_label=1),\n",
        "        f1_score(test_labels, test_pred, pos_label=0),\n",
        "        f1_score(test_labels, test_pred, pos_label=1),\n",
        "        roc_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simpan metrics ke CSV\n",
        "metrics_df.to_csv('./results/indobert_metrics_summary.csv', index=False)\n",
        "print(\"‚úÖ Metrics disimpan: ./results/indobert_metrics_summary.csv\")\n",
        "\n",
        "# ========== 3. PREDICTION DISTRIBUTION ==========\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot distribusi probabilitas prediksi untuk Real vs Fake\n",
        "real_probs = test_pred_proba[test_labels == 0].flatten()\n",
        "fake_probs = test_pred_proba[test_labels == 1].flatten()\n",
        "\n",
        "axes[0].hist(real_probs, bins=30, alpha=0.7, color='#3498db', edgecolor='black', label='Real News')\n",
        "axes[0].hist(fake_probs, bins=30, alpha=0.7, color='#e74c3c', edgecolor='black', label='Fake News')\n",
        "axes[0].axvline(x=0.5, color='green', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "axes[0].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot persentase prediksi benar vs salah\n",
        "correct = (test_pred == test_labels).sum()\n",
        "incorrect = len(test_labels) - correct\n",
        "labels_pie = ['Correct', 'Incorrect']\n",
        "sizes = [correct, incorrect]\n",
        "colors_pie = ['#2ecc71', '#e74c3c']\n",
        "explode = (0.05, 0)\n",
        "\n",
        "axes[1].pie(sizes, explode=explode, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%',\n",
        "            shadow=True, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
        "axes[1].set_title(f'Prediction Accuracy: {correct}/{len(test_labels)} samples', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/indobert_prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi distribusi disimpan: ./results/indobert_prediction_distribution.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéâ SEMUA VISUALISASI SELESAI!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "h1kR7It0eNhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BILSTM BASELINE"
      ],
      "metadata": {
        "id": "CVmeC334gZN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "\n",
        "# 1. PREPARE DATA & CLASS WEIGHTS\n",
        "# Asumsi: train_ids, train_labels, val_ids, dsb. sudah dimuat di sesi Colab\n",
        "print(\"‚öñÔ∏è Calculating Class Weights...\")\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "print(f\"Weights: {class_weights}\")\n",
        "\n",
        "# 2. BUILD OPTIMIZED BILSTM MODEL\n",
        "def build_optimized_bilstm():\n",
        "    # Pastikan sesi bersih\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    inputs = keras.layers.Input(shape=(256,), dtype='int32')\n",
        "\n",
        "    # Embedding Layer (input_dim disesuaikan dengan vocab size IndoBERT jika menggunakan token yang sama)\n",
        "    x = keras.layers.Embedding(input_dim=30522, output_dim=128)(inputs)\n",
        "\n",
        "    # Bidirectional LSTM - Menangkap konteks dari dua arah (maju & mundur)\n",
        "    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "    # Dual Pooling Strategy untuk menangkap fitur paling menonjol (Max) dan rata-rata konteks (Avg)\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    gmp = keras.layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Gabungkan fitur dari kedua pooling\n",
        "    concat = keras.layers.Concatenate()([gap, gmp])\n",
        "\n",
        "    # Dense Head yang Robust\n",
        "    x = keras.layers.Dense(128, activation='relu')(concat)\n",
        "    x = keras.layers.BatchNormalization()(x) # Menjaga stabilitas gradien\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = keras.layers.Dense(64, activation='relu')(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output (Sigmoid untuk klasifikasi biner)\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Optimizer dengan Learning Rate yang stabil\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"\\nüöÄ Building Optimized BiLSTM model...\")\n",
        "model_bilstm = build_optimized_bilstm()\n",
        "model_bilstm.summary()\n",
        "\n",
        "# 3. TRAINING CONFIGURATION\n",
        "# Update: EarlyStopping dihapus, hanya menggunakan Checkpoint dan LRScheduler\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        './models/bilstm_best_opt.weights.h5',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss'\n",
        "    ),\n",
        "    # Strategi pencegahan kenaikan Validation Loss:\n",
        "    # Jika val_loss macet dalam 2 epoch, turunkan LR sebesar 80% (factor=0.2)\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüî• Memulai Training BiLSTM (10 Epochs)...\")\n",
        "history = model_bilstm.fit(\n",
        "    train_ids, train_labels,\n",
        "    validation_data=(val_ids, val_labels),\n",
        "    epochs=10,        # Sesuai permintaan: 10 Epoch\n",
        "    batch_size=8,     # Sedikit dinaikkan dari 6 ke 8 agar gradien lebih stabil\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 4. EVALUATION\n",
        "print(\"\\nüìä Evaluating BiLSTM on Test Set...\")\n",
        "test_pred_proba = model_bilstm.predict(test_ids, batch_size=8)\n",
        "test_pred = (test_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\n‚úÖ BiLSTM Accuracy: {accuracy_score(test_labels, test_pred):.4f}\")\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(test_labels, test_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan hasil untuk keperluan Ensemble\n",
        "if not os.path.exists('./results'): os.makedirs('./results')\n",
        "np.save('./results/bilstm_test_preds.npy', test_pred_proba)\n",
        "\n",
        "print(\"\\nüíæ Prediksi berhasil disimpan di ./results/bilstm_test_preds.npy\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "zpGLTrIMgC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI HASIL TRAINING BILSTM (FIXED VERSION)\n",
        "# ============================================================================\n",
        "\n",
        "acc_lstm = history.history['accuracy']\n",
        "val_acc_lstm = history.history['val_accuracy']\n",
        "loss_lstm = history.history['loss']\n",
        "val_loss_lstm = history.history['val_loss']\n",
        "epochs_range = range(1, len(acc_lstm) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# 1. Plot Training & Validation Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "# Menggunakan parameter terpisah (color, marker, linestyle) agar tidak error\n",
        "plt.plot(epochs_range, acc_lstm, color='green', marker='o', linestyle='-', label='Training Accuracy', markersize=6)\n",
        "plt.plot(epochs_range, val_acc_lstm, color='orange', marker='s', linestyle='-', label='Validation Accuracy', markersize=6)\n",
        "\n",
        "plt.title('BiLSTM: Training and Validation Accuracy', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.xticks(epochs_range)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. Plot Training & Validation Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss_lstm, color='green', marker='o', linestyle='-', label='Training Loss', markersize=6)\n",
        "plt.plot(epochs_range, val_loss_lstm, color='orange', marker='s', linestyle='-', label='Validation Loss', markersize=6)\n",
        "\n",
        "plt.title('BiLSTM: Training and Validation Loss', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.xticks(epochs_range)\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/training_curves_bilstm_10ep_fixed.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualisasi BiLSTM diperbaiki dan disimpan di './results/training_curves_bilstm_10ep_fixed.png'\")"
      ],
      "metadata": {
        "id": "8xs7fff8dmtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n5Ô∏è Generating predictions on Training Set...\")\n",
        "train_pred_proba = model_bilstm.predict(train_ids)\n",
        "train_pred = (train_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\n‚úÖ BiLSTM Training Accuracy: {accuracy_score(train_labels, train_pred):.4f}\")\n",
        "\n",
        "# Simpan prediksi probabilitas training untuk Ensemble\n",
        "np.save('./results/bilstm_train_preds.npy', train_pred_proba)\n",
        "print(\"üíæ Saved training predictions to './results/bilstm_train_preds.npy'\")\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n6Ô∏è‚É£ Plotting Training History...\")\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('BiLSTM Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('BiLSTM Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/bilstm_training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"üìä Training history plot saved to './results/bilstm_training_history.png'\")\n",
        "\n",
        "# 7. VERIFIKASI FILE YANG TERSIMPAN\n",
        "print(\"\\n7Ô∏è‚É£ Verifying saved files...\")\n",
        "files_to_check = [\n",
        "    './results/bilstm_test_preds.npy',\n",
        "    './results/bilstm_train_preds.npy',\n",
        "    './models/bilstm_best_opt.weights.h5'\n",
        "]\n",
        "\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"‚úÖ {file_path} exists ({size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\" {file_path} NOT FOUND!\")\n",
        "\n",
        "print(\"\\n BiLSTM Training Complete! Ready for ensemble.\")"
      ],
      "metadata": {
        "id": "rPIF1jb4nPrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 6. VISUALISASI LENGKAP ==========\n",
        "print(\"\\n6Ô∏è‚É£ Creating Comprehensive Visualizations...\")\n",
        "\n",
        "# Set style untuk visualisasi\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (16, 12)\n",
        "\n",
        "# ========== VISUALISASI 1: Training & Validation Curves + CM + ROC ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training & Validation Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='#e74c3c')\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='#3498db')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title('BiLSTM: Training & Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Tambahkan marker untuk best epoch\n",
        "best_epoch = np.argmin(history.history['val_loss'])\n",
        "axes[0, 0].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7,\n",
        "                   label=f'Best Epoch ({best_epoch+1})')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "\n",
        "# Plot 2: Training & Validation Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='#e74c3c')\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#3498db')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('BiLSTM: Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Plot 3: Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
        "            ax=axes[1, 0], annot_kws={'size': 16, 'weight': 'bold'})\n",
        "axes[1, 0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('BiLSTM: Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Tambahkan persentase di dalam cell\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        percentage = cm[i, j] / cm[i].sum() * 100\n",
        "        axes[1, 0].text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                       ha='center', va='center', fontsize=10, color='gray')\n",
        "\n",
        "# Plot 4: ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "axes[1, 1].plot(fpr, tpr, color='#e74c3c', linewidth=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "axes[1, 1].plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
        "axes[1, 1].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_title('BiLSTM: ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].fill_between(fpr, tpr, alpha=0.2, color='#e74c3c')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/bilstm_training_visualization.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi disimpan: ./results/bilstm_training_visualization.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== VISUALISASI 2: Metrics Summary Table ==========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RINGKASAN METRIK EVALUASI BiLSTM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics_data = {\n",
        "    'Metric': ['Accuracy', 'Precision (Real)', 'Precision (Fake)',\n",
        "               'Recall (Real)', 'Recall (Fake)', 'F1-Score (Real)', 'F1-Score (Fake)', 'AUC-ROC'],\n",
        "    'Value': [\n",
        "        accuracy_score(test_labels, test_pred),\n",
        "        precision_score(test_labels, test_pred, pos_label=0),\n",
        "        precision_score(test_labels, test_pred, pos_label=1),\n",
        "        recall_score(test_labels, test_pred, pos_label=0),\n",
        "        recall_score(test_labels, test_pred, pos_label=1),\n",
        "        f1_score(test_labels, test_pred, pos_label=0),\n",
        "        f1_score(test_labels, test_pred, pos_label=1),\n",
        "        roc_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simpan metrics ke CSV\n",
        "metrics_df.to_csv('./results/bilstm_metrics_summary.csv', index=False)\n",
        "print(\"‚úÖ Metrics disimpan: ./results/bilstm_metrics_summary.csv\")\n",
        "\n",
        "# ========== VISUALISASI 3: Prediction Distribution ==========\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot distribusi probabilitas prediksi untuk Real vs Fake\n",
        "real_probs = test_pred_proba[test_labels == 0].flatten()\n",
        "fake_probs = test_pred_proba[test_labels == 1].flatten()\n",
        "\n",
        "axes[0].hist(real_probs, bins=30, alpha=0.7, color='#3498db', edgecolor='black', label='Real News')\n",
        "axes[0].hist(fake_probs, bins=30, alpha=0.7, color='#e74c3c', edgecolor='black', label='Fake News')\n",
        "axes[0].axvline(x=0.5, color='green', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "axes[0].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('BiLSTM: Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot persentase prediksi benar vs salah\n",
        "correct = (test_pred == test_labels).sum()\n",
        "incorrect = len(test_labels) - correct\n",
        "labels_pie = ['Correct', 'Incorrect']\n",
        "sizes = [correct, incorrect]\n",
        "colors_pie = ['#2ecc71', '#e74c3c']\n",
        "explode = (0.05, 0)\n",
        "\n",
        "axes[1].pie(sizes, explode=explode, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%',\n",
        "            shadow=True, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
        "axes[1].set_title(f'BiLSTM Prediction Accuracy: {correct}/{len(test_labels)} samples',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/bilstm_prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi distribusi disimpan: ./results/bilstm_prediction_distribution.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== 7. VERIFIKASI FILE YANG TERSIMPAN ==========\n",
        "print(\"\\n7Ô∏è‚É£ Verifying saved files...\")\n",
        "files_to_check = [\n",
        "    './results/bilstm_test_preds.npy',\n",
        "    './results/bilstm_train_preds.npy',\n",
        "    './models/bilstm_best_opt.weights.h5',\n",
        "    './results/bilstm_training_visualization.png',\n",
        "    './results/bilstm_metrics_summary.csv',\n",
        "    './results/bilstm_prediction_distribution.png'\n",
        "]\n",
        "\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"‚úÖ {file_path} exists ({size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file_path} NOT FOUND!\")\n",
        "\n",
        "print(\"\\nüéâ SEMUA VISUALISASI BiLSTM SELESAI!\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚ú® BiLSTM Training Complete! Ready for ensemble.\")"
      ],
      "metadata": {
        "id": "3kc36mVCnGpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE WEIGHTED AVG"
      ],
      "metadata": {
        "id": "uyj1ZIDulCA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîó STEP 7: HYBRID ENSEMBLE (INDOBERT + BILSTM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. LOAD PREDIKSI PROBABILITAS (Hasil dari Step 6A dan 6B)\n",
        "try:\n",
        "    # Pastikan file ini sudah disimpan di step sebelumnya\n",
        "    bert_probs = np.load('./results/indobert_robust_test_preds.npy')\n",
        "    lstm_probs = np.load('./results/bilstm_test_preds.npy')\n",
        "\n",
        "    # Load Label asli untuk evaluasi\n",
        "    test_labels = np.load('/content/tokenized_data/test_labels.npy')\n",
        "\n",
        "    print(\"‚úÖ Berhasil memuat prediksi dari kedua model.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: File prediksi .npy tidak ditemukan. Pastikan Step 6A dan 6B sudah dijalankan sampai selesai.\")\n",
        "    raise\n",
        "\n",
        "# 2. MENCARI BOBOT OPTIMAL (Simple Grid Search)\n",
        "best_weight = 0\n",
        "max_acc = 0\n",
        "\n",
        "print(\"\\nüîç Mencari kombinasi bobot terbaik...\")\n",
        "for i in np.arange(0, 1.1, 0.1):\n",
        "    w1 = i\n",
        "    w2 = 1 - i\n",
        "    # Gabungkan probabilitas\n",
        "    ensemble_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "    acc = accuracy_score(test_labels, ensemble_preds)\n",
        "    if acc > max_acc:\n",
        "        max_acc = acc\n",
        "        best_weight = w1\n",
        "    print(f\"   Bobot IndoBERT: {w1:.1f} | Bobot BiLSTM: {w2:.1f} -> Accuracy: {acc:.4f}\")\n",
        "\n",
        "# 3. EVALUASI AKHIR ENSEMBLE DENGAN BOBOT TERBAIK\n",
        "w1 = best_weight\n",
        "w2 = 1 - best_weight\n",
        "print(f\"\\nüèÜ Bobot Terbaik: IndoBERT ({w1:.1f}), BiLSTM ({w2:.1f})\")\n",
        "\n",
        "final_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "final_preds = (final_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"üìä HASIL AKHIR ENSEMBLE\")\n",
        "print(\"=\"*30)\n",
        "print(f\"üéØ Final Accuracy: {accuracy_score(test_labels, final_preds):.4f}\")\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# 4. VISUALISASI CONFUSION MATRIX\n",
        "cm = confusion_matrix(test_labels, final_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Asli')\n",
        "plt.title('Confusion Matrix: Hybrid Ensemble')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yK-AYaqniwoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (classification_report, accuracy_score, confusion_matrix,\n",
        "                            roc_curve, auc, precision_score, recall_score, f1_score)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîó STEP 7: HYBRID ENSEMBLE (INDOBERT + BILSTM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. LOAD PREDIKSI PROBABILITAS (Hasil dari Step 6A dan 6B)\n",
        "try:\n",
        "    # Pastikan file ini sudah disimpan di step sebelumnya\n",
        "    bert_probs = np.load('./results/indobert_robust_test_preds.npy')\n",
        "    lstm_probs = np.load('./results/bilstm_test_preds.npy')\n",
        "\n",
        "    # Load Label asli untuk evaluasi\n",
        "    test_labels = np.load('/content/tokenized_data/test_labels.npy')\n",
        "\n",
        "    print(\"‚úÖ Berhasil memuat prediksi dari kedua model.\")\n",
        "    print(f\"   - IndoBERT predictions shape: {bert_probs.shape}\")\n",
        "    print(f\"   - BiLSTM predictions shape: {lstm_probs.shape}\")\n",
        "    print(f\"   - Test labels shape: {test_labels.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: File prediksi .npy tidak ditemukan. Pastikan Step 6A dan 6B sudah dijalankan sampai selesai.\")\n",
        "    raise\n",
        "\n",
        "# 2. MENCARI BOBOT OPTIMAL (Simple Grid Search)\n",
        "best_weight = 0\n",
        "max_acc = 0\n",
        "weight_results = []\n",
        "\n",
        "print(\"\\nüîç Mencari kombinasi bobot terbaik...\")\n",
        "print(\"-\" * 60)\n",
        "for i in np.arange(0, 1.1, 0.1):\n",
        "    w1 = i\n",
        "    w2 = 1 - i\n",
        "    # Gabungkan probabilitas\n",
        "    ensemble_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "    acc = accuracy_score(test_labels, ensemble_preds)\n",
        "    weight_results.append({'IndoBERT_Weight': w1, 'BiLSTM_Weight': w2, 'Accuracy': acc})\n",
        "\n",
        "    if acc > max_acc:\n",
        "        max_acc = acc\n",
        "        best_weight = w1\n",
        "    print(f\"   Bobot IndoBERT: {w1:.1f} | Bobot BiLSTM: {w2:.1f} -> Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Convert ke DataFrame untuk visualisasi\n",
        "weights_df = pd.DataFrame(weight_results)\n",
        "\n",
        "# 3. EVALUASI AKHIR ENSEMBLE DENGAN BOBOT TERBAIK\n",
        "w1 = best_weight\n",
        "w2 = 1 - best_weight\n",
        "print(f\"\\nüèÜ Bobot Optimal Ditemukan!\")\n",
        "print(f\"   - IndoBERT: {w1:.2f} ({w1*100:.1f}%)\")\n",
        "print(f\"   - BiLSTM: {w2:.2f} ({w2*100:.1f}%)\")\n",
        "\n",
        "final_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "final_preds = (final_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "# Prediksi individual untuk perbandingan\n",
        "bert_preds = (bert_probs >= 0.5).astype(int).flatten()\n",
        "lstm_preds = (lstm_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä HASIL AKHIR ENSEMBLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hitung metrik untuk semua model\n",
        "models_comparison = {\n",
        "    'Model': ['IndoBERT (Solo)', 'BiLSTM (Solo)', 'Hybrid Ensemble'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(test_labels, bert_preds),\n",
        "        accuracy_score(test_labels, lstm_preds),\n",
        "        accuracy_score(test_labels, final_preds)\n",
        "    ],\n",
        "    'Precision': [\n",
        "        precision_score(test_labels, bert_preds, average='weighted'),\n",
        "        precision_score(test_labels, lstm_preds, average='weighted'),\n",
        "        precision_score(test_labels, final_preds, average='weighted')\n",
        "    ],\n",
        "    'Recall': [\n",
        "        recall_score(test_labels, bert_preds, average='weighted'),\n",
        "        recall_score(test_labels, lstm_preds, average='weighted'),\n",
        "        recall_score(test_labels, final_preds, average='weighted')\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        f1_score(test_labels, bert_preds, average='weighted'),\n",
        "        f1_score(test_labels, lstm_preds, average='weighted'),\n",
        "        f1_score(test_labels, final_preds, average='weighted')\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(models_comparison)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüéØ Final Ensemble Accuracy: {accuracy_score(test_labels, final_preds):.4f}\")\n",
        "print(f\"üìà Improvement over IndoBERT: {(accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, bert_preds)):.4f}\")\n",
        "print(f\"üìà Improvement over BiLSTM: {(accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, lstm_preds)):.4f}\")\n",
        "\n",
        "print(\"\\nüìã Detailed Classification Report (Ensemble):\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan comparison ke CSV\n",
        "comparison_df.to_csv('./results/ensemble_models_comparison.csv', index=False)\n",
        "print(\"\\n‚úÖ Model comparison saved to: ./results/ensemble_models_comparison.csv\")\n",
        "\n",
        "# ========== VISUALISASI LENGKAP ==========\n",
        "print(\"\\nüìä Generating Comprehensive Visualizations...\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (18, 12)\n",
        "\n",
        "# ========== VISUALISASI 1: Main Dashboard (3x3) ==========\n",
        "fig = plt.figure(figsize=(18, 14))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Weight Search Results\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.plot(weights_df['IndoBERT_Weight'], weights_df['Accuracy'],\n",
        "         marker='o', linewidth=2.5, markersize=8, color='#3498db')\n",
        "ax1.axvline(x=best_weight, color='#e74c3c', linestyle='--', linewidth=2,\n",
        "           label=f'Optimal Weight ({best_weight:.2f})')\n",
        "ax1.axhline(y=max_acc, color='#2ecc71', linestyle=':', linewidth=1.5, alpha=0.7)\n",
        "ax1.set_xlabel('IndoBERT Weight', fontsize=11, fontweight='bold')\n",
        "ax1.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Weight Optimization Curve', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=9)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Model Comparison Bar Chart\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "x_pos = np.arange(len(comparison_df['Model']))\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "bars = ax2.bar(x_pos, comparison_df['Accuracy'], color=colors, alpha=0.8, edgecolor='black')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels(comparison_df['Model'], rotation=15, ha='right', fontsize=10)\n",
        "ax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('Model Performance Comparison', fontsize=13, fontweight='bold')\n",
        "ax2.set_ylim([0, 1.05])\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Tambahkan nilai di atas bar\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "            f'{height:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Plot 3: Ensemble Weight Distribution (Pie Chart)\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "sizes = [w1, w2]\n",
        "labels = ['IndoBERT', 'BiLSTM']\n",
        "colors_pie = ['#3498db', '#e74c3c']\n",
        "explode = (0.05, 0.05)\n",
        "wedges, texts, autotexts = ax3.pie(sizes, explode=explode, labels=labels, colors=colors_pie,\n",
        "                                     autopct='%1.1f%%', shadow=True, startangle=90,\n",
        "                                     textprops={'fontsize': 11, 'weight': 'bold'})\n",
        "ax3.set_title('Optimal Ensemble Weights', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Plot 4-6: Confusion Matrices (3 models)\n",
        "cms = [\n",
        "    (confusion_matrix(test_labels, bert_preds), 'IndoBERT', gs[1, 0]),\n",
        "    (confusion_matrix(test_labels, lstm_preds), 'BiLSTM', gs[1, 1]),\n",
        "    (confusion_matrix(test_labels, final_preds), 'Hybrid Ensemble', gs[1, 2])\n",
        "]\n",
        "\n",
        "for cm, title, grid_pos in cms:\n",
        "    ax = fig.add_subplot(grid_pos)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "               xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
        "               ax=ax, annot_kws={'size': 14, 'weight': 'bold'})\n",
        "    ax.set_xlabel('Predicted Label', fontsize=10, fontweight='bold')\n",
        "    ax.set_ylabel('True Label', fontsize=10, fontweight='bold')\n",
        "    ax.set_title(f'{title} - Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Tambahkan persentase\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            percentage = cm[i, j] / cm[i].sum() * 100\n",
        "            ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                   ha='center', va='center', fontsize=9, color='gray')\n",
        "\n",
        "# Plot 7: ROC Curves Comparison\n",
        "ax7 = fig.add_subplot(gs[2, :2])\n",
        "\n",
        "# Hitung ROC untuk semua model\n",
        "fpr_bert, tpr_bert, _ = roc_curve(test_labels, bert_probs)\n",
        "roc_auc_bert = auc(fpr_bert, tpr_bert)\n",
        "\n",
        "fpr_lstm, tpr_lstm, _ = roc_curve(test_labels, lstm_probs)\n",
        "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
        "\n",
        "fpr_ensemble, tpr_ensemble, _ = roc_curve(test_labels, final_probs)\n",
        "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
        "\n",
        "# Plot ROC curves\n",
        "ax7.plot(fpr_bert, tpr_bert, linewidth=2.5, label=f'IndoBERT (AUC = {roc_auc_bert:.4f})',\n",
        "         color='#3498db')\n",
        "ax7.plot(fpr_lstm, tpr_lstm, linewidth=2.5, label=f'BiLSTM (AUC = {roc_auc_lstm:.4f})',\n",
        "         color='#e74c3c')\n",
        "ax7.plot(fpr_ensemble, tpr_ensemble, linewidth=3, label=f'Ensemble (AUC = {roc_auc_ensemble:.4f})',\n",
        "         color='#2ecc71', linestyle='-')\n",
        "ax7.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
        "\n",
        "ax7.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
        "ax7.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
        "ax7.set_title('ROC Curves Comparison: All Models', fontsize=13, fontweight='bold')\n",
        "ax7.legend(loc='lower right', fontsize=10)\n",
        "ax7.grid(True, alpha=0.3)\n",
        "ax7.fill_between(fpr_ensemble, tpr_ensemble, alpha=0.2, color='#2ecc71')\n",
        "\n",
        "# Plot 8: Prediction Probability Distribution\n",
        "ax8 = fig.add_subplot(gs[2, 2])\n",
        "real_probs = final_probs[test_labels == 0].flatten()\n",
        "fake_probs = final_probs[test_labels == 1].flatten()\n",
        "\n",
        "ax8.hist(real_probs, bins=25, alpha=0.7, color='#3498db', edgecolor='black', label='Real News')\n",
        "ax8.hist(fake_probs, bins=25, alpha=0.7, color='#e74c3c', edgecolor='black', label='Fake News')\n",
        "ax8.axvline(x=0.5, color='green', linestyle='--', linewidth=2.5, label='Threshold (0.5)')\n",
        "ax8.set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\n",
        "ax8.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "ax8.set_title('Ensemble Probability Distribution', fontsize=12, fontweight='bold')\n",
        "ax8.legend(fontsize=9)\n",
        "ax8.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('üîó HYBRID ENSEMBLE: Comprehensive Performance Analysis',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.savefig('./results/ensemble_comprehensive_visualization.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Main visualization saved: ./results/ensemble_comprehensive_visualization.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== VISUALISASI 2: Detailed Metrics Comparison ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Plot 1: Accuracy Comparison with Error Margins\n",
        "ax = axes[0, 0]\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x_pos = np.arange(len(comparison_df['Model']))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    offset = (i - 1.5) * width\n",
        "    bars = ax.bar(x_pos + offset, comparison_df[metric], width,\n",
        "                  label=metric, alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(comparison_df['Model'], fontsize=10, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('All Metrics Comparison Across Models', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Per-Class Performance (Real vs Fake)\n",
        "ax = axes[0, 1]\n",
        "real_f1 = [\n",
        "    f1_score(test_labels, bert_preds, pos_label=0),\n",
        "    f1_score(test_labels, lstm_preds, pos_label=0),\n",
        "    f1_score(test_labels, final_preds, pos_label=0)\n",
        "]\n",
        "fake_f1 = [\n",
        "    f1_score(test_labels, bert_preds, pos_label=1),\n",
        "    f1_score(test_labels, lstm_preds, pos_label=1),\n",
        "    f1_score(test_labels, final_preds, pos_label=1)\n",
        "]\n",
        "\n",
        "x = np.arange(len(comparison_df['Model']))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, real_f1, width, label='Real News', color='#3498db', alpha=0.8, edgecolor='black')\n",
        "bars2 = ax.bar(x + width/2, fake_f1, width, label='Fake News', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison_df['Model'], fontsize=10, fontweight='bold')\n",
        "ax.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Per-Class F1-Score Comparison', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Tambahkan nilai di atas bar\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "               f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plot 3: AUC-ROC Comparison\n",
        "ax = axes[1, 0]\n",
        "auc_scores = [roc_auc_bert, roc_auc_lstm, roc_auc_ensemble]\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "bars = ax.barh(comparison_df['Model'], auc_scores, color=colors, alpha=0.8, edgecolor='black')\n",
        "ax.set_xlabel('AUC-ROC Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('AUC-ROC Comparison', fontsize=13, fontweight='bold')\n",
        "ax.set_xlim([0, 1.1])\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Tambahkan nilai di samping bar\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
        "           f'{auc_scores[i]:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 4: Improvement Summary\n",
        "ax = axes[1, 1]\n",
        "improvement_data = {\n",
        "    'Metric': ['vs IndoBERT', 'vs BiLSTM'],\n",
        "    'Accuracy Gain': [\n",
        "        accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, bert_preds),\n",
        "        accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, lstm_preds)\n",
        "    ]\n",
        "}\n",
        "improvement_colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in improvement_data['Accuracy Gain']]\n",
        "bars = ax.bar(improvement_data['Metric'], improvement_data['Accuracy Gain'],\n",
        "             color=improvement_colors, alpha=0.8, edgecolor='black')\n",
        "ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "ax.set_ylabel('Accuracy Improvement', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Ensemble Improvement Over Individual Models', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Tambahkan nilai\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "           f'{height:+.4f}', ha='center', va='bottom' if height > 0 else 'top',\n",
        "           fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('üìä Detailed Performance Metrics Analysis', fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/ensemble_detailed_metrics.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Detailed metrics visualization saved: ./results/ensemble_detailed_metrics.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== SIMPAN SEMUA HASIL ==========\n",
        "print(\"\\nüíæ Saving all results...\")\n",
        "\n",
        "# Simpan prediksi final\n",
        "np.save('./results/ensemble_final_predictions.npy', final_preds)\n",
        "np.save('./results/ensemble_final_probabilities.npy', final_probs)\n",
        "\n",
        "# Simpan weight results\n",
        "weights_df.to_csv('./results/ensemble_weight_search_results.csv', index=False)\n",
        "\n",
        "# Simpan detailed metrics\n",
        "detailed_metrics = {\n",
        "    'Model': ['IndoBERT', 'BiLSTM', 'Ensemble'],\n",
        "    'Accuracy': [accuracy_score(test_labels, bert_preds),\n",
        "                accuracy_score(test_labels, lstm_preds),\n",
        "                accuracy_score(test_labels, final_preds)],\n",
        "    'Precision_Real': [precision_score(test_labels, bert_preds, pos_label=0),\n",
        "                      precision_score(test_labels, lstm_preds, pos_label=0),\n",
        "                      precision_score(test_labels, final_preds, pos_label=0)],\n",
        "    'Precision_Fake': [precision_score(test_labels, bert_preds, pos_label=1),\n",
        "                      precision_score(test_labels, lstm_preds, pos_label=1),\n",
        "                      precision_score(test_labels, final_preds, pos_label=1)],\n",
        "    'Recall_Real': [recall_score(test_labels, bert_preds, pos_label=0),\n",
        "                   recall_score(test_labels, lstm_preds, pos_label=0),\n",
        "                   recall_score(test_labels, final_preds, pos_label=0)],\n",
        "    'Recall_Fake': [recall_score(test_labels, bert_preds, pos_label=1),\n",
        "                   recall_score(test_labels, lstm_preds, pos_label=1),\n",
        "                   recall_score(test_labels, final_preds, pos_label=1)],\n",
        "    'F1_Real': [f1_score(test_labels, bert_preds, pos_label=0),\n",
        "               f1_score(test_labels, lstm_preds, pos_label=0),\n",
        "               f1_score(test_labels, final_preds, pos_label=0)],\n",
        "    'F1_Fake': [f1_score(test_labels, bert_preds, pos_label=1),\n",
        "               f1_score(test_labels, lstm_preds, pos_label=1),\n",
        "               f1_score(test_labels, final_preds, pos_label=1)],\n",
        "    'AUC_ROC': [roc_auc_bert, roc_auc_lstm, roc_auc_ensemble]\n",
        "}\n",
        "\n",
        "detailed_df = pd.DataFrame(detailed_metrics)\n",
        "detailed_df.to_csv('./results/ensemble_detailed_metrics.csv', index=False)\n",
        "\n",
        "print(\"\\n‚úÖ All files saved successfully!\")\n",
        "print(\"   üìÅ ./results/ensemble_final_predictions.npy\")\n",
        "print(\"   üìÅ ./results/ensemble_final_probabilities.npy\")\n",
        "print(\"   üìÅ ./results/ensemble_weight_search_results.csv\")\n",
        "print(\"   üìÅ ./results/ensemble_models_comparison.csv\")\n",
        "print(\"   üìÅ ./results/ensemble_detailed_metrics.csv\")\n",
        "print(\"   üìÅ ./results/ensemble_comprehensive_visualization.png\")\n",
        "print(\"   üìÅ ./results/ensemble_detailed_metrics.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ HYBRID ENSEMBLE ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üèÜ Best Model: Hybrid Ensemble\")\n",
        "print(f\"üéØ Final Accuracy: {accuracy_score(test_labels, final_preds):.4f}\")\n",
        "print(f\"üìà AUC-ROC: {roc_auc_ensemble:.4f}\")\n",
        "print(f\"‚öñÔ∏è  Optimal Weights: IndoBERT ({w1:.2f}) + BiLSTM ({w2:.2f})\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "wnbQRPHplGu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STACKING"
      ],
      "metadata": {
        "id": "lMCKVtfxlbIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîó ENSEMBLE METHODS: STACKING & MAJORITY VOTING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "try:\n",
        "    # Load prediksi probabilitas dari kedua model\n",
        "    bert_probs = np.load('./results/indobert_robust_test_preds.npy')\n",
        "    lstm_probs = np.load('./results/bilstm_test_preds.npy')\n",
        "\n",
        "    # Load label asli\n",
        "    test_labels = np.load('/content/tokenized_data/test_labels.npy')\n",
        "\n",
        "    # Load juga data training untuk stacking (PENTING!)\n",
        "    bert_train_probs = np.load('./results/indobert_robust_train_preds.npy')  # Harus dibuat dulu\n",
        "    lstm_train_probs = np.load('./results/bilstm_train_preds.npy')  # Harus dibuat dulu\n",
        "    train_labels = np.load('/content/tokenized_data/train_labels.npy')\n",
        "\n",
        "    print(\"‚úÖ Berhasil memuat prediksi dari kedua model.\")\n",
        "    print(f\"   Test samples: {len(test_labels)}\")\n",
        "    print(f\"   Train samples: {len(train_labels)}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"‚ö†Ô∏è  Untuk Stacking, Anda perlu menyimpan prediksi training set juga!\")\n",
        "    print(\"   Tambahkan di script training: np.save('results/indobert_robust_train_preds.npy', train_preds)\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "i-ff8Au-lbwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "# ============================================================================\n",
        "# PREPARASI DATA STACKING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìö MULTI-MODEL STACKING ENSEMBLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Gabungkan probabilitas sebagai fitur untuk meta-learner\n",
        "X_train_stack = np.column_stack([bert_train_probs.flatten(), lstm_train_probs.flatten()])\n",
        "X_test_stack = np.column_stack([bert_probs.flatten(), lstm_probs.flatten()])\n",
        "\n",
        "# Definisi Meta-Learners\n",
        "meta_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM (RBF Kernel)\": SVC(probability=True, kernel='rbf', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Dictionary untuk menyimpan hasil\n",
        "results = {}\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING & EVALUATION LOOP\n",
        "# ============================================================================\n",
        "for name, model in meta_models.items():\n",
        "    print(f\"\\nüéØ Training Meta-Learner: {name}...\")\n",
        "    model.fit(X_train_stack, train_labels)\n",
        "\n",
        "    # Prediksi\n",
        "    preds = model.predict(X_test_stack)\n",
        "\n",
        "    # Hitung Metrik\n",
        "    acc = accuracy_score(test_labels, preds)\n",
        "    f1 = f1_score(test_labels, preds)\n",
        "    results[name] = {'Accuracy': acc, 'F1-Score': f1, 'Predictions': preds}\n",
        "\n",
        "    print(f\"‚úÖ {name} Done. Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(test_labels, preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI 1: PERBANDINGAN AKURASI\n",
        "# ============================================================================\n",
        "df_results = pd.DataFrame(results).T[['Accuracy', 'F1-Score']]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = df_results.plot(kind='bar', figsize=(10, 6), color=['#3498db', '#e74c3c'])\n",
        "plt.title('Perbandingan Performa Meta-Learner pada Stacking', fontsize=14)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Meta-Learner Model')\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Tambahkan label angka di atas bar\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_height():.4f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI 2: CONFUSION MATRICES (Side-by-Side)\n",
        "# ============================================================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "fig.suptitle('Confusion Matrices: Stacking Ensemble Models', fontsize=16)\n",
        "\n",
        "for i, (name, res) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(test_labels, res['Predictions'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    axes[i].set_title(f\"{name}\\nAcc: {res['Accuracy']:.2%}\")\n",
        "    axes[i].set_xlabel('Prediksi')\n",
        "    axes[i].set_ylabel('Aktual')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EMUr36X2bKnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRWCE5snbNyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNAPSHOT EMBENDING"
      ],
      "metadata": {
        "id": "iSspSHm0utwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# 1. Menghitung Rata-rata Probabilitas (Ensemble Averaging)\n",
        "# Kita asumsikan output adalah probabilitas untuk kelas positif (Fake)\n",
        "ensemble_probs = (bert_probs + lstm_probs) / 2\n",
        "\n",
        "# 2. Konversi Probabilitas ke Label (Threshold 0.5)\n",
        "ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
        "\n",
        "# 3. Hitung Metrik untuk Perbandingan\n",
        "models = ['IndoBERT', 'BiLSTM', 'Ensemble']\n",
        "accuracies = [\n",
        "    accuracy_score(test_labels, (bert_probs > 0.5).astype(int)),\n",
        "    accuracy_score(test_labels, (lstm_probs > 0.5).astype(int)),\n",
        "    accuracy_score(test_labels, ensemble_preds)\n",
        "]\n",
        "f1_scores = [\n",
        "    f1_score(test_labels, (bert_probs > 0.5).astype(int)),\n",
        "    f1_score(test_labels, (lstm_probs > 0.5).astype(int)),\n",
        "    f1_score(test_labels, ensemble_preds)\n",
        "]\n",
        "\n",
        "print(\"üìä HASIL EVALUASI ENSEMBLE\")\n",
        "print(\"-\" * 30)\n",
        "print(classification_report(test_labels, ensemble_preds, target_names=['Real', 'Fake']))"
      ],
      "metadata": {
        "id": "5V2Cz7BypMhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualisasi 1: Perbandingan Akurasi & F1-Score ---\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db')\n",
        "rects2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score', color='#e74c3c')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Perbandingan Performa: Individu vs Ensemble')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1) # Memberi ruang untuk label\n",
        "\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Visualisasi 2: Confusion Matrix Ensemble ---\n",
        "cm = confusion_matrix(test_labels, ensemble_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Real', 'Fake'],\n",
        "            yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Aktual')\n",
        "plt.title('Confusion Matrix - Ensemble Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HW5q0pLCtgVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAVE DAN JALANKAN DI DRIVE"
      ],
      "metadata": {
        "id": "dL8z_2ITuw9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Buat folder backup di Google Drive\n",
        "backup_path = '/content/drive/MyDrive/fake_healthrelease_project'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "\n",
        "# 3. Daftar folder yang akan disimpan\n",
        "folders_to_backup = ['models', 'results', 'tokenized_data']\n",
        "\n",
        "print(\"üöÄ Memulai backup ke Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for folder in folders_to_backup:\n",
        "    source = f'/content/{folder}'\n",
        "    destination = f'{backup_path}/{folder}'\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        # Hapus folder lama di Drive jika ada agar tidak error saat copytree\n",
        "        if os.path.exists(destination):\n",
        "            shutil.rmtree(destination)\n",
        "\n",
        "        # Copy folder baru\n",
        "        shutil.copytree(source, destination)\n",
        "\n",
        "        # Hitung ukuran folder\n",
        "        size_bytes = 0\n",
        "        for dirpath, _, filenames in os.walk(destination):\n",
        "            for filename in filenames:\n",
        "                fp = os.path.join(dirpath, filename)\n",
        "                size_bytes += os.path.getsize(fp)\n",
        "\n",
        "        size_mb = size_bytes / (1024 * 1024)\n",
        "\n",
        "        # Emoji di sini aman karena di dalam tanda kutip (string)\n",
        "        print(f\"‚úÖ {folder:20s} ‚Üí {size_mb:8.2f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  {folder} tidak ditemukan, skip...\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ Backup selesai!\")\n",
        "print(f\"üìÅ Lokasi: {backup_path}\")"
      ],
      "metadata": {
        "id": "LCZo34BZuyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Path sumber dari Google Drive\n",
        "backup_path = '/content/drive/MyDrive/fake_healthrelease_project'\n",
        "\n",
        "# 3. Daftar folder yang akan dipulihkan\n",
        "folders_to_restore = ['models', 'results', 'tokenized_data']\n",
        "\n",
        "print(\"üîÑ Memulai restore dari Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for folder in folders_to_restore:\n",
        "    source = f'{backup_path}/{folder}'\n",
        "    destination = f'/content/{folder}'\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        # Hapus folder lama di Colab jika ada\n",
        "        if os.path.exists(destination):\n",
        "            shutil.rmtree(destination)\n",
        "\n",
        "        # Copy folder dari Drive ke Colab\n",
        "        shutil.copytree(source, destination)\n",
        "\n",
        "        # Hitung ukuran\n",
        "        size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
        "                     for dirpath, _, filenames in os.walk(destination)\n",
        "                     for filename in filenames) / (1024 * 1024)\n",
        "\n",
        "        print(f\"‚úÖ {folder:20s} ‚Üí {size_mb:8.2f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ùå {folder} tidak ditemukan di Drive!\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ Restore selesai!\")\n",
        "print(\"\\nFolder yang dimuat:\")\n",
        "print(\"  - /content/models/\")\n",
        "print(\"  - /content/results/\")\n",
        "print(\"  - /content/tokenized_data/\")\n",
        "print(\"\\n‚ö° Siap digunakan untuk inference atau training lanjutan!\")"
      ],
      "metadata": {
        "id": "S87n_mY4v1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"üìã Verifikasi file yang dimuat:\\n\")\n",
        "\n",
        "# Cek models\n",
        "print(\"üîπ Models:\")\n",
        "if os.path.exists('/content/models'):\n",
        "    for file in os.listdir('/content/models'):\n",
        "        print(f\"   ‚úì {file}\")\n",
        "\n",
        "# Cek results\n",
        "print(\"\\nüîπ Results:\")\n",
        "if os.path.exists('/content/results'):\n",
        "    for file in os.listdir('/content/results'):\n",
        "        print(f\"   ‚úì {file}\")\n",
        "\n",
        "# Cek tokenized_data\n",
        "print(\"\\nüîπ Tokenized Data:\")\n",
        "if os.path.exists('/content/tokenized_data'):\n",
        "    for file in os.listdir('/content/tokenized_data'):\n",
        "        print(f\"   ‚úì {file}\")"
      ],
      "metadata": {
        "id": "UfJw17OTv81Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}