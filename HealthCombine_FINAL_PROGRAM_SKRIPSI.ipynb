{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wUAQOKWx0XXP",
        "_QVl-z5a0-Kh",
        "Yzz8mXEz1qT1",
        "_sCdbU6edF2c",
        "Oy2It-x07Xcy",
        "YHTXZFc6AKCm",
        "Fmi-I9vUZ7Cx",
        "rvZaxN7if5g1",
        "ptgLlJJpDh36",
        "3_uUi2mHER1a",
        "jCcvm1WUEiJb",
        "CVmeC334gZN0",
        "uyj1ZIDulCA9",
        "lMCKVtfxlbIe",
        "iSspSHm0utwY",
        "dL8z_2ITuw9g"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yobangs/Ensemble-Indobert-BiLSTM-Fake-Health/blob/main/HealthCombine_FINAL_PROGRAM_SKRIPSI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece torch pandas tqdm\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HSVxTpBEz_CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALISIS_DATASET Health RELEASE"
      ],
      "metadata": {
        "id": "wUAQOKWx0XXP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rjcL4LdzqZb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthRelease\"\n",
        "output_csv = \"dataset_release.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Baca file pertama untuk ambil header\n",
        "with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "\n",
        "    # Fungsi flatten nested dict\n",
        "def flatten(d, parent=''):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        key = f\"{parent}_{k}\" if parent else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten(v, key).items())\n",
        "        else:\n",
        "            items.append((key, v))\n",
        "    return dict(items)\n",
        "\n",
        "    # Proses semua file\n",
        "data = []\n",
        "for file in json_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        data.append(flatten(obj))\n",
        "\n",
        "        # Tulis ke CSV\n",
        "if data:\n",
        "    keys = set()\n",
        "    for d in data:\n",
        "        keys.update(d.keys())\n",
        "\n",
        "    with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted(keys))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Berhasil! Dataset disimpan ke {output_csv}\")\n",
        "    print(f\"Total baris: {len(data)}, Total kolom: {len(keys)}\")\n",
        "else:\n",
        "    print(\"Tidak ada data untuk diproses\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('dataset_release.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ANALISIS DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal baris: {len(df)}\")\n",
        "print(f\"Total kolom: {len(df.columns)}\")\n",
        "\n",
        "# Tampilkan semua nama kolom\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DAFTAR SEMUA KOLOM:\")\n",
        "print(\"=\"*60)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    # Hitung nilai non-null\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = (non_null/len(df)*100)\n",
        "    print(f\"{i:3d}. {col:50s} - {non_null:3d} ({pct:5.1f}%)\")\n"
      ],
      "metadata": {
        "id": "SzWXBqGx0N0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan contoh 3 baris pertama\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONTOH DATA (3 baris pertama):\")\n",
        "print(\"=\"*60)\n",
        "print(df.head(3).to_string())\n",
        "\n",
        "# Analisis kolom dengan nilai terbanyak\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 KOLOM DENGAN DATA TERLENGKAP:\")\n",
        "print(\"=\"*60)\n",
        "completeness = df.notna().sum().sort_values(ascending=False).head(20)\n",
        "for col, count in completeness.items():\n",
        "    pct = (count/len(df)*100)\n",
        "    print(f\"{col:50s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Silakan tentukan kolom mana yang ingin dipertahankan!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "0RhY9Iam0qfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALISIS_DATASET Health STORY"
      ],
      "metadata": {
        "id": "_QVl-z5a0-Kh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_G5NYxu60-Ki"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthStory\"\n",
        "output_csv = \"dataset_story.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Baca file pertama untuk ambil header\n",
        "with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "    sample = json.load(f)\n",
        "\n",
        "    # Fungsi flatten nested dict\n",
        "def flatten(d, parent=''):\n",
        "    items = []\n",
        "    for k, v in d.items():\n",
        "        key = f\"{parent}_{k}\" if parent else k\n",
        "        if isinstance(v, dict):\n",
        "            items.extend(flatten(v, key).items())\n",
        "        else:\n",
        "            items.append((key, v))\n",
        "    return dict(items)\n",
        "\n",
        "    # Proses semua file\n",
        "data = []\n",
        "for file in json_files:\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        obj = json.load(f)\n",
        "        data.append(flatten(obj))\n",
        "\n",
        "        # Tulis ke CSV\n",
        "if data:\n",
        "    keys = set()\n",
        "    for d in data:\n",
        "        keys.update(d.keys())\n",
        "\n",
        "    with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=sorted(keys))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f\"Berhasil! Dataset disimpan ke {output_csv}\")\n",
        "    print(f\"Total baris: {len(data)}, Total kolom: {len(keys)}\")\n",
        "else:\n",
        "    print(\"Tidak ada data untuk diproses\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baca dataset\n",
        "df = pd.read_csv('dataset_story.csv')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ANALISIS DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nTotal baris: {len(df)}\")\n",
        "print(f\"Total kolom: {len(df.columns)}\")\n",
        "\n",
        "# Tampilkan semua nama kolom\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DAFTAR SEMUA KOLOM:\")\n",
        "print(\"=\"*60)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    # Hitung nilai non-null\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = (non_null/len(df)*100)\n",
        "    print(f\"{i:3d}. {col:50s} - {non_null:3d} ({pct:5.1f}%)\")\n"
      ],
      "metadata": {
        "id": "N7Gsi2Bt0-Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan contoh 3 baris pertama\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONTOH DATA (3 baris pertama):\")\n",
        "print(\"=\"*60)\n",
        "print(df.head(3).to_string())\n",
        "\n",
        "# Analisis kolom dengan nilai terbanyak\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 KOLOM DENGAN DATA TERLENGKAP:\")\n",
        "print(\"=\"*60)\n",
        "completeness = df.notna().sum().sort_values(ascending=False).head(20)\n",
        "for col, count in completeness.items():\n",
        "    pct = (count/len(df)*100)\n",
        "    print(f\"{col:50s}: {count:3d} ({pct:5.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Silakan tentukan kolom mana yang ingin dipertahankan!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "3g049An_0-Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REKAP MENJADI CSV"
      ],
      "metadata": {
        "id": "Yzz8mXEz1qT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthRelease\"\n",
        "output_csv = \"HealthRelease_Cleaned.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"üìÅ Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Proses semua file\n",
        "data = []\n",
        "errors = []\n",
        "\n",
        "for file in json_files:\n",
        "    try:\n",
        "        file_id = file.stem\n",
        "\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            obj = json.load(f)\n",
        "\n",
        "            data.append({\n",
        "                'file_id': file_id,\n",
        "                'title': obj.get('title', ''),\n",
        "                'text': obj.get('text', ''),\n",
        "                'url': obj.get('url', ''),\n",
        "                'source': obj.get('source', ''),\n",
        "                'publish_date': obj.get('publish_date', '')\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(f\"{file.name}: {e}\")\n",
        "\n",
        "# Buat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bersihkan data\n",
        "df['title'] = df['title'].str.strip()\n",
        "df['text'] = df['text'].str.strip()\n",
        "\n",
        "# Hapus baris kosong\n",
        "df_original_len = len(df)\n",
        "df = df[(df['title'] != '') & (df['text'] != '')]\n",
        "df = df[df['text'].str.len() >= 100]\n",
        "df = df.sort_values('file_id').reset_index(drop=True)\n",
        "\n",
        "# Simpan\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Output ringkas dan rapi\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úÖ DATASET BERHASIL DIBUAT: {output_csv}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total file JSON     : {len(json_files)}\")\n",
        "print(f\"Artikel valid       : {len(df)} ({len(df)/len(json_files)*100:.1f}%)\")\n",
        "print(f\"Artikel dibuang     : {df_original_len - len(df)} (text < 100 chars atau kosong)\")\n",
        "if errors:\n",
        "    print(f\"File error          : {len(errors)}\")\n",
        "\n",
        "# Statistik kolom (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"KELENGKAPAN DATA:\")\n",
        "for col in df.columns:\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = non_null/len(df)*100\n",
        "    bar = \"‚ñà\" * int(pct/5) + \"‚ñë\" * (20 - int(pct/5))\n",
        "    print(f\"  {col:15s} [{bar}] {pct:5.1f}% ({non_null}/{len(df)})\")\n",
        "\n",
        "# Statistik teks\n",
        "print(\"-\" * 80)\n",
        "print(\"STATISTIK TEKS:\")\n",
        "text_lengths = df['text'].str.len()\n",
        "print(f\"  Rata-rata panjang  : {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Terpendek          : {text_lengths.min()} karakter\")\n",
        "print(f\"  Terpanjang         : {text_lengths.max()} karakter\")\n",
        "\n",
        "# Preview data (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"PREVIEW 5 ARTIKEL PERTAMA:\")\n",
        "for idx, row in df.head(5).iterrows():\n",
        "    title_preview = row['title'][:65] + \"...\" if len(row['title']) > 65 else row['title']\n",
        "    print(f\"  [{row['file_id']}] {title_preview}\")\n",
        "    print(f\"    ‚îî‚îÄ {len(row['text'])} chars | {row['source'][:40]}\")\n",
        "\n",
        "# Error summary\n",
        "if errors:\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"‚ö†Ô∏è  FILE DENGAN ERROR ({len(errors)}):\")\n",
        "    for err in errors[:5]:  # Show first 5 errors\n",
        "        print(f\"  ‚Ä¢ {err}\")\n",
        "    if len(errors) > 5:\n",
        "        print(f\"  ... dan {len(errors)-5} error lainnya\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "JmfWCsYs1vDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Konfigurasi\n",
        "folder = \"HealthStory\"\n",
        "output_csv = \"HealthStory_Cleaned.csv\"\n",
        "\n",
        "# Ambil semua file JSON\n",
        "json_files = list(Path(folder).glob(\"*.json\"))\n",
        "print(f\"üìÅ Ditemukan {len(json_files)} file JSON\")\n",
        "\n",
        "# Proses semua file\n",
        "data = []\n",
        "errors = []\n",
        "\n",
        "for file in json_files:\n",
        "    try:\n",
        "        file_id = file.stem\n",
        "\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            obj = json.load(f)\n",
        "\n",
        "            data.append({\n",
        "                'file_id': file_id,\n",
        "                'title': obj.get('title', ''),\n",
        "                'text': obj.get('text', ''),\n",
        "                'url': obj.get('url', ''),\n",
        "                'source': obj.get('source', ''),\n",
        "                'publish_date': obj.get('publish_date', '')\n",
        "            })\n",
        "\n",
        "    except Exception as e:\n",
        "        errors.append(f\"{file.name}: {e}\")\n",
        "\n",
        "# Buat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Bersihkan data\n",
        "df['title'] = df['title'].str.strip()\n",
        "df['text'] = df['text'].str.strip()\n",
        "\n",
        "# Hapus baris kosong\n",
        "df_original_len = len(df)\n",
        "df = df[(df['title'] != '') & (df['text'] != '')]\n",
        "df = df[df['text'].str.len() >= 100]\n",
        "df = df.sort_values('file_id').reset_index(drop=True)\n",
        "\n",
        "# Simpan\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "# Output ringkas dan rapi\n",
        "print(\"=\" * 80)\n",
        "print(f\"‚úÖ DATASET BERHASIL DIBUAT: {output_csv}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total file JSON     : {len(json_files)}\")\n",
        "print(f\"Artikel valid       : {len(df)} ({len(df)/len(json_files)*100:.1f}%)\")\n",
        "print(f\"Artikel dibuang     : {df_original_len - len(df)} (text < 100 chars atau kosong)\")\n",
        "if errors:\n",
        "    print(f\"File error          : {len(errors)}\")\n",
        "\n",
        "# Statistik kolom (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"KELENGKAPAN DATA:\")\n",
        "for col in df.columns:\n",
        "    non_null = df[col].notna().sum()\n",
        "    pct = non_null/len(df)*100\n",
        "    bar = \"‚ñà\" * int(pct/5) + \"‚ñë\" * (20 - int(pct/5))\n",
        "    print(f\"  {col:15s} [{bar}] {pct:5.1f}% ({non_null}/{len(df)})\")\n",
        "\n",
        "# Statistik teks\n",
        "print(\"-\" * 80)\n",
        "print(\"STATISTIK TEKS:\")\n",
        "text_lengths = df['text'].str.len()\n",
        "print(f\"  Rata-rata panjang  : {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Terpendek          : {text_lengths.min()} karakter\")\n",
        "print(f\"  Terpanjang         : {text_lengths.max()} karakter\")\n",
        "\n",
        "# Preview data (compact)\n",
        "print(\"-\" * 80)\n",
        "print(\"PREVIEW 5 ARTIKEL PERTAMA:\")\n",
        "for idx, row in df.head(5).iterrows():\n",
        "    title_preview = row['title'][:65] + \"...\" if len(row['title']) > 65 else row['title']\n",
        "    print(f\"  [{row['file_id']}] {title_preview}\")\n",
        "    print(f\"    ‚îî‚îÄ {len(row['text'])} chars | {row['source'][:40]}\")\n",
        "\n",
        "# Error summary\n",
        "if errors:\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"‚ö†Ô∏è  FILE DENGAN ERROR ({len(errors)}):\")\n",
        "    for err in errors[:5]:  # Show first 5 errors\n",
        "        print(f\"  ‚Ä¢ {err}\")\n",
        "    if len(errors) > 5:\n",
        "        print(f\"  ... dan {len(errors)-5} error lainnya\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "G1II8Jm51uoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sCdbU6edF2c"
      },
      "source": [
        "# DATA LABELLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmIzLZJRDeg2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "def extract_reviews(folder_path):\n",
        "    \"\"\"Ekstrak label dari folder reviews\"\"\"\n",
        "    data = []\n",
        "\n",
        "    for file in Path(folder_path).glob('*.json'):\n",
        "        try:\n",
        "            with open(file, 'r', encoding='utf-8') as f:\n",
        "                d = json.load(f)\n",
        "\n",
        "            file_id = file.stem\n",
        "            rating = d.get('rating', '')\n",
        "            satisfactory = d.get('satisfactory', '')\n",
        "            label_text = d.get('label', '')\n",
        "            label_raw = (satisfactory or rating or label_text or '').lower()\n",
        "\n",
        "            data.append({'file_id': file_id, 'label_raw': label_raw})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error {file.name}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def map_label(label_raw):\n",
        "    \"\"\"Convert label text ke binary\"\"\"\n",
        "    label_raw = str(label_raw).lower()\n",
        "\n",
        "    if 'satisfactory' in label_raw and 'not' not in label_raw:\n",
        "        return 0\n",
        "    elif 'not satisfactory' in label_raw or 'notsatisfactory' in label_raw:\n",
        "        return 1\n",
        "    elif 'notapplicable' in label_raw or 'not applicable' in label_raw:\n",
        "        return -1\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "def label_dataset(content_csv, reviews_folder, output_csv):\n",
        "    \"\"\"Labeling dataset dengan merge reviews\"\"\"\n",
        "\n",
        "    print(f\"\\nProcessing: {content_csv}\")\n",
        "\n",
        "    df_content = pd.read_csv(content_csv)\n",
        "    print(f\"Content: {len(df_content)} rows\")\n",
        "\n",
        "    df_reviews = extract_reviews(reviews_folder)\n",
        "    print(f\"Reviews: {len(df_reviews)} rows\")\n",
        "\n",
        "    df_reviews['label'] = df_reviews['label_raw'].apply(map_label)\n",
        "\n",
        "    print(f\"Label: Credible={(df_reviews['label']==0).sum()}, Fake={(df_reviews['label']==1).sum()}, N/A={(df_reviews['label']==-1).sum()}\")\n",
        "\n",
        "    df_labeled = df_content.merge(df_reviews[['file_id', 'label', 'label_raw']], on='file_id', how='left')\n",
        "\n",
        "    df_final = df_labeled[df_labeled['label'].isin([0, 1])].copy()\n",
        "    df_final['label'] = df_final['label'].astype(int)\n",
        "\n",
        "    print(f\"Final: {len(df_final)} rows (Credible={(df_final['label']==0).sum()}, Fake={(df_final['label']==1).sum()})\")\n",
        "\n",
        "    df_final.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\"‚úì Saved: {output_csv}\\n\")\n",
        "\n",
        "    return df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3M6Z3maHtLF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Cek HealthRelease.json\n",
        "print(\"=\"*50)\n",
        "print(\"STRUKTUR HEALTHRELEASE.JSON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "with open('/content/reviews/HealthRelease.json', 'r') as f:\n",
        "    data_release = json.load(f)\n",
        "\n",
        "print(f\"Type: {type(data_release)}\")\n",
        "print(f\"Length: {len(data_release) if isinstance(data_release, (list, dict)) else 'N/A'}\")\n",
        "\n",
        "# Jika dict, tampilkan keys\n",
        "if isinstance(data_release, dict):\n",
        "    print(f\"\\nKeys: {list(data_release.keys())[:10]}\")\n",
        "    # Ambil sample 1 item\n",
        "    first_key = list(data_release.keys())[0]\n",
        "    print(f\"\\nSample (key: {first_key}):\")\n",
        "    print(json.dumps(data_release[first_key], indent=2)[:500])\n",
        "\n",
        "# Jika list, tampilkan item pertama\n",
        "elif isinstance(data_release, list):\n",
        "    print(f\"\\nFirst item:\")\n",
        "    print(json.dumps(data_release[0], indent=2)[:500])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STRUKTUR HEALTHSTORY.JSON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "with open('/content/reviews/HealthStory.json', 'r') as f:\n",
        "    data_story = json.load(f)\n",
        "\n",
        "print(f\"Type: {type(data_story)}\")\n",
        "print(f\"Length: {len(data_story) if isinstance(data_story, (list, dict)) else 'N/A'}\")\n",
        "\n",
        "if isinstance(data_story, dict):\n",
        "    print(f\"\\nKeys: {list(data_story.keys())[:10]}\")\n",
        "    first_key = list(data_story.keys())[0]\n",
        "    print(f\"\\nSample (key: {first_key}):\")\n",
        "    print(json.dumps(data_story[first_key], indent=2)[:500])\n",
        "elif isinstance(data_story, list):\n",
        "    print(f\"\\nFirst item:\")\n",
        "    print(json.dumps(data_story[0], indent=2)[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuUDc0izH8Hp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Baca file\n",
        "with open('/content/reviews/HealthRelease.json', 'r') as f:\n",
        "    data_release = json.load(f)\n",
        "\n",
        "with open('/content/reviews/HealthStory.json', 'r') as f:\n",
        "    data_story = json.load(f)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"HEALTHRELEASE - Sample Item (FULL)\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(data_release[0], indent=2))\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"HEALTHSTORY - Sample Item (FULL)\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(data_story[0], indent=2))\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"SEMUA KEYS YANG ADA DI HEALTHRELEASE\")\n",
        "print(\"=\"*60)\n",
        "all_keys = set()\n",
        "for item in data_release[:50]:\n",
        "    all_keys.update(item.keys())\n",
        "print(sorted(all_keys))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SEMUA KEYS YANG ADA DI HEALTHSTORY\")\n",
        "print(\"=\"*60)\n",
        "all_keys = set()\n",
        "for item in data_story[:50]:\n",
        "    all_keys.update(item.keys())\n",
        "print(sorted(all_keys))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO-jY2fHIUq2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def extract_reviews_from_json(json_file):\n",
        "    \"\"\"Ekstrak label dari file JSON reviews\"\"\"\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for item in data:\n",
        "        news_id = item.get('news_id', '')\n",
        "        rating = item.get('rating', 0)\n",
        "\n",
        "        # Hitung Satisfactory vs Not Satisfactory dari criteria\n",
        "        satisfactory_count = 0\n",
        "        not_satisfactory_count = 0\n",
        "        not_applicable_count = 0\n",
        "\n",
        "        criteria = item.get('criteria', [])\n",
        "        for criterion in criteria:\n",
        "            answer = criterion.get('answer', '').lower()\n",
        "            if 'satisfactory' in answer and 'not' not in answer:\n",
        "                satisfactory_count += 1\n",
        "            elif 'not satisfactory' in answer:\n",
        "                not_satisfactory_count += 1\n",
        "            elif 'not applicable' in answer:\n",
        "                not_applicable_count += 1\n",
        "\n",
        "        # Hitung persentase satisfactory\n",
        "        total_applicable = satisfactory_count + not_satisfactory_count\n",
        "        if total_applicable > 0:\n",
        "            satisfactory_ratio = satisfactory_count / total_applicable\n",
        "        else:\n",
        "            satisfactory_ratio = 0\n",
        "\n",
        "        # Tentukan label berdasarkan ratio\n",
        "        # Jika >= 50% satisfactory = Credible (0)\n",
        "        # Jika < 50% satisfactory = Fake/Not Credible (1)\n",
        "        if satisfactory_ratio >= 0.5:\n",
        "            label = 0  # Credible\n",
        "        else:\n",
        "            label = 1  # Not Credible/Fake\n",
        "\n",
        "        results.append({\n",
        "            'news_id': news_id,\n",
        "            'rating': rating,\n",
        "            'satisfactory': satisfactory_count,\n",
        "            'not_satisfactory': not_satisfactory_count,\n",
        "            'not_applicable': not_applicable_count,\n",
        "            'satisfactory_ratio': round(satisfactory_ratio, 2),\n",
        "            'label': label\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def label_dataset(content_csv, reviews_json, output_csv, dataset_name):\n",
        "    \"\"\"Merge content dengan reviews untuk labeling\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LABELING {dataset_name.upper()}\")\n",
        "    print('='*60)\n",
        "\n",
        "    # Baca content\n",
        "    df_content = pd.read_csv(content_csv)\n",
        "    print(f\"Content: {len(df_content)} rows\")\n",
        "\n",
        "    # Ekstrak reviews\n",
        "    df_reviews = extract_reviews_from_json(reviews_json)\n",
        "    print(f\"Reviews: {len(df_reviews)} rows\")\n",
        "\n",
        "    # Rename 'news_id' ke 'file_id' untuk matching\n",
        "    df_reviews = df_reviews.rename(columns={'news_id': 'file_id'})\n",
        "\n",
        "    # Merge\n",
        "    df_labeled = df_content.merge(df_reviews, on='file_id', how='inner')\n",
        "\n",
        "    print(f\"\\nMerged: {len(df_labeled)} rows\")\n",
        "    print(f\"\\nLabel Distribution:\")\n",
        "    print(f\"  Credible (0): {(df_labeled['label']==0).sum()} ({(df_labeled['label']==0).sum()/len(df_labeled)*100:.1f}%)\")\n",
        "    print(f\"  Fake (1): {(df_labeled['label']==1).sum()} ({(df_labeled['label']==1).sum()/len(df_labeled)*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nRating Statistics:\")\n",
        "    print(f\"  Mean rating: {df_labeled['rating'].mean():.2f}\")\n",
        "    print(f\"  Min rating: {df_labeled['rating'].min()}\")\n",
        "    print(f\"  Max rating: {df_labeled['rating'].max()}\")\n",
        "\n",
        "    # Simpan\n",
        "    df_labeled.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n‚úì Saved: {output_csv}\")\n",
        "\n",
        "    # Preview\n",
        "    print(f\"\\nPreview (3 rows):\")\n",
        "    print(df_labeled[['file_id', 'title', 'rating', 'satisfactory', 'not_satisfactory', 'label']].head(3))\n",
        "\n",
        "    return df_labeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ3iGEw1IYdI"
      },
      "outputs": [],
      "source": [
        "# MAIN EXECUTION\n",
        "print(\"=\"*60)\n",
        "print(\"DATASET LABELING PROCESS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# HealthRelease\n",
        "df_release = label_dataset(\n",
        "    content_csv='/content/HealthRelease_Cleaned.csv',\n",
        "    reviews_json='/content/reviews/HealthRelease.json',\n",
        "    output_csv='health_release_LABELED.csv',\n",
        "    dataset_name='HealthRelease'\n",
        ")\n",
        "\n",
        "# HealthStory\n",
        "df_story = label_dataset(\n",
        "    content_csv='/content/HealthStory_Cleaned.csv',\n",
        "    reviews_json='/content/reviews/HealthStory.json',\n",
        "    output_csv='health_story_LABELED.csv',\n",
        "    dataset_name='HealthStory'\n",
        ")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nHealthRelease:\")\n",
        "print(f\"  Total: {len(df_release)} rows\")\n",
        "print(f\"  Credible (0): {(df_release['label']==0).sum()}\")\n",
        "print(f\"  Fake (1): {(df_release['label']==1).sum()}\")\n",
        "\n",
        "print(f\"\\nHealthStory:\")\n",
        "print(f\"  Total: {len(df_story)} rows\")\n",
        "print(f\"  Credible (0): {(df_story['label']==0).sum()}\")\n",
        "print(f\"  Fake (1): {(df_story['label']==1).sum()}\")\n",
        "\n",
        "print(\"\\n‚úÖ Output files (terpisah):\")\n",
        "print(\"  - health_release_LABELED.csv\")\n",
        "print(\"  - health_story_LABELED.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bjy4rVMA7WLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREPROCESSING"
      ],
      "metadata": {
        "id": "Oy2It-x07Xcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def light_preprocessing(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text)\n",
        "\n",
        "    # 1. Normalisasi Unicode (Mengubah bentuk karakter aneh ke bentuk standar)\n",
        "    # Contoh: Mengubah '√©' menjadi 'e' atau memastikan simbol tetap terbaca\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "    # 2. Menghapus HTML Tags\n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "\n",
        "    # 3. Menghapus URL/Link (Penting agar model fokus pada konten berita)\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "    # 4. Menghapus Email\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "\n",
        "    # 5. Membersihkan Newlines, Tabs, dan karakter aneh lainnya\n",
        "    text = text.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')\n",
        "\n",
        "    # 6. Menghapus karakter non-printable (karakter kontrol yang tidak terlihat)\n",
        "    text = \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
        "\n",
        "    # 7. Mempertahankan tanda baca penting & Angka\n",
        "    # Kita hanya menghapus simbol-simbol dekoratif (seperti emoji atau simbol hiasan)\n",
        "    # Tetap pertahankan: . , ! ? : ; - ( ) %\n",
        "    text = re.sub(r'[^\\w\\s.,!?;:\\-\\(\\)%]', ' ', text)\n",
        "\n",
        "    # 8. Lowercase (Gunakan jika menggunakan model IndoBERT-uncased)\n",
        "    text = text.lower()\n",
        "\n",
        "    # 9. Menghapus spasi ganda\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# ========== LOAD DATASETS ==========\n",
        "print(\"=\" * 60)\n",
        "print(\"1Ô∏è‚É£  LOADING LABELED DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    release_df = pd.read_csv('health_release_LABELED.csv')\n",
        "    story_df = pd.read_csv('health_story_LABELED.csv')\n",
        "\n",
        "    print(f\"üìÑ health_release_LABELED.csv: {len(release_df)} sampel\")\n",
        "    print(f\"üìÑ health_story_LABELED.csv: {len(story_df)} sampel\")\n",
        "\n",
        "    # Gabungkan kedua dataset\n",
        "    df = pd.concat([release_df, story_df], ignore_index=True)\n",
        "    print(f\"‚úÖ Berhasil menggabungkan data. Total: {len(df)} sampel.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå File tidak ditemukan: {e}\")\n",
        "    print(\"üí° Pastikan file CSV ada di direktori yang sama dengan script ini.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error saat load file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# ========== CEK KOLOM ==========\n",
        "print(f\"\\nüìã Kolom yang tersedia: {list(df.columns)}\")\n",
        "\n",
        "# Tentukan kolom teks (sesuaikan dengan nama kolom di dataset Anda)\n",
        "# Kemungkinan nama: 'text', 'content', 'title', 'article', dll.\n",
        "column_to_clean = 'text'  # ‚¨ÖÔ∏è GANTI JIKA NAMA KOLOM BERBEDA\n",
        "\n",
        "if column_to_clean not in df.columns:\n",
        "    print(f\"\\n‚ö†Ô∏è  Kolom '{column_to_clean}' tidak ditemukan!\")\n",
        "    print(f\"üí° Ganti variabel 'column_to_clean' dengan salah satu dari: {list(df.columns)}\")\n",
        "    exit()\n",
        "\n",
        "# ========== CEK DATA KOSONG SEBELUM PROSES ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2Ô∏è‚É£  ANALISIS DATA SEBELUM PREPROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "null_count = df[column_to_clean].isna().sum()\n",
        "empty_count = (df[column_to_clean] == '').sum()\n",
        "\n",
        "print(f\"üîç Jumlah nilai NULL: {null_count}\")\n",
        "print(f\"üîç Jumlah teks kosong: {empty_count}\")\n",
        "print(f\"üîç Total data valid: {len(df) - null_count - empty_count}\")\n",
        "\n",
        "# ========== PROSES PEMBERSIHAN ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3Ô∏è‚É£  MENJALANKAN LIGHT PREPROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df['text_cleaned'] = df[column_to_clean].apply(light_preprocessing)\n",
        "\n",
        "# Hapus baris yang kosong setelah dibersihkan\n",
        "before_drop = len(df)\n",
        "df = df[df['text_cleaned'] != ''].reset_index(drop=True)\n",
        "after_drop = len(df)\n",
        "\n",
        "dropped = before_drop - after_drop\n",
        "print(f\"‚úÖ Preprocessing selesai!\")\n",
        "print(f\"üóëÔ∏è  Dihapus {dropped} baris kosong setelah cleaning\")\n",
        "print(f\"üìä Total data final: {len(df)} sampel\")\n",
        "\n",
        "# ========== CONTOH HASIL ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4Ô∏è‚É£  CONTOH HASIL PREPROCESSING (3 sampel acak)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "sample_df = df.sample(min(3, len(df)))\n",
        "for idx, row in sample_df.iterrows():\n",
        "    print(f\"\\n--- SAMPEL {idx + 1} ---\")\n",
        "    print(f\"SEBELUM: {row[column_to_clean][:200]}...\")\n",
        "    print(f\"SESUDAH: {row['text_cleaned'][:200]}...\")\n",
        "    print(f\"LABEL: {row['label']}\")\n",
        "\n",
        "# ========== SIMPAN HASIL ==========\n",
        "output_file = 'health_combined_CLEANED.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"5Ô∏è‚É£  MENYIMPAN HASIL\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üíæ File disimpan: {output_file}\")\n",
        "\n",
        "# ========== STATISTIK AKHIR ==========\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"6Ô∏è‚É£  STATISTIK DISTRIBUSI LABEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if 'label' in df.columns:\n",
        "    print(df['label'].value_counts())\n",
        "    print(f\"\\nüìä Persentase:\")\n",
        "    print(df['label'].value_counts(normalize=True) * 100)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kolom 'label' tidak ditemukan di dataset!\")\n",
        "\n",
        "print(\"\\n‚úÖ PREPROCESSING SELESAI!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "cBVZNgbS7aPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pilih hanya kolom yang diperlukan\n",
        "df_to_translate = df[['title', 'text_cleaned', 'label']].copy()\n",
        "\n",
        "# Simpan ke file baru untuk diterjemahkan\n",
        "df_to_translate.to_csv('ready_to_translate_en.csv', index=False)\n",
        "print(\"‚úÖ Kolom sudah dirampingkan. Siap untuk diterjemahkan.\")"
      ],
      "metadata": {
        "id": "pUD_Vupb_6Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSLASI"
      ],
      "metadata": {
        "id": "YHTXZFc6AKCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import re\n",
        "\n",
        "# ========== KONFIGURASI ==========\n",
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-id\"\n",
        "INPUT_FILE = \"ready_to_translate_en.csv\"\n",
        "OUTPUT_FILE = \"health_translated_id.csv\"\n",
        "CHECKPOINT_FILE = \"health_translated_id_checkpoint.csv\"\n",
        "\n",
        "# BATCH_SIZE 32-64 biasanya optimal untuk GPU dengan VRAM 8GB+\n",
        "BATCH_SIZE = 32\n",
        "# Berapa banyak baris berita yang diproses sekaligus sebelum disimpan\n",
        "CHUNK_ROWS = 50\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"üñ•Ô∏è  Menggunakan: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# ========== LOAD MODEL ==========\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Pipeline diatur dengan batch_size internal\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "def split_sentences(text):\n",
        "    if pd.isna(text) or str(text).strip() == \"\": return []\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', str(text))\n",
        "    return [s.strip() for s in sentences if len(s.strip()) > 2]\n",
        "\n",
        "# ========== PROSES UTAMA ==========\n",
        "if os.path.exists(CHECKPOINT_FILE):\n",
        "    df = pd.read_csv(CHECKPOINT_FILE)\n",
        "else:\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    df['text_id'] = \"\"\n",
        "\n",
        "indices_todo = df[df['text_id'].isna() | (df['text_id'] == \"\")].index.tolist()\n",
        "print(f\"üöÄ Sisa yang harus diterjemahkan: {len(indices_todo)} baris\")\n",
        "\n",
        "# Proses dalam potongan (Chunks) agar tidak memakan RAM berlebih\n",
        "for i in range(0, len(indices_todo), CHUNK_ROWS):\n",
        "    current_indices = indices_todo[i:i + CHUNK_ROWS]\n",
        "    rows_to_process = df.loc[current_indices, 'text_cleaned'].tolist()\n",
        "\n",
        "    # 1. Flatten: Kumpulkan semua kalimat dan catat asalnya\n",
        "    all_sentences = []\n",
        "    row_map = [] # Mencatat kalimat ini milik baris ke berapa\n",
        "\n",
        "    for idx, row_text in enumerate(rows_to_process):\n",
        "        sents = split_sentences(row_text)\n",
        "        all_sentences.extend(sents)\n",
        "        row_map.extend([idx] * len(sents))\n",
        "\n",
        "    if not all_sentences:\n",
        "        continue\n",
        "\n",
        "    # 2. Batch Translate: Kirim SEMUA kalimat ke GPU sekaligus\n",
        "    # Ini bagian paling cepat karena memanfaatkan paralelisme GPU\n",
        "    translated_sentences = []\n",
        "    for out in translator(all_sentences, truncation=True, max_length=512):\n",
        "        translated_sentences.append(out['translation_text'])\n",
        "\n",
        "    # 3. Reassemble: Satukan kembali kalimat ke baris asalnya\n",
        "    reassembled_texts = [\"\"] * len(rows_to_process)\n",
        "    for sent_idx, row_idx in enumerate(row_map):\n",
        "        reassembled_texts[row_idx] += translated_sentences[sent_idx] + \" \"\n",
        "\n",
        "    # 4. Update DataFrame\n",
        "    for idx, final_text in enumerate(reassembled_texts):\n",
        "        df.at[current_indices[idx], 'text_id'] = final_text.strip()\n",
        "\n",
        "    # Save Checkpoint per Chunk\n",
        "    df.to_csv(CHECKPOINT_FILE, index=False)\n",
        "    print(f\"‚úÖ Baris {current_indices[-1]} selesai...\")\n",
        "\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "print(\"üéØ Semua selesai!\")"
      ],
      "metadata": {
        "id": "ZmePrqcrALnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# ========== KONFIGURASI ==========\n",
        "MODEL_NAME = \"Helsinki-NLP/opus-mt-en-id\"\n",
        "INPUT_FILE = \"health_translated_id.csv\"\n",
        "OUTPUT_FILE = \"health_translated_id_COMPLETE.csv\" # File baru agar aman\n",
        "\n",
        "# Batch size bisa lebih besar untuk title (misal 64 atau 128) karena teksnya pendek\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# ========== SETUP DEVICE ==========\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"üñ•Ô∏è  Menggunakan: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# ========== LOAD MODEL & TOKENIZER ==========\n",
        "print(f\"üì¶ Loading model: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "\n",
        "translator = pipeline(\n",
        "    \"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# ========== LOAD DATA ==========\n",
        "df = pd.read_csv(INPUT_FILE)\n",
        "\n",
        "# Inisialisasi kolom title_id jika belum ada\n",
        "if 'title_id' not in df.columns:\n",
        "    df['title_id'] = \"\"\n",
        "\n",
        "# Cari indeks yang title_id nya masih kosong\n",
        "indices_to_process = df[df['title_id'].isna() | (df['title_id'] == \"\")].index.tolist()\n",
        "\n",
        "print(f\"üöÄ Memulai translasi {len(indices_to_process)} judul...\")\n",
        "\n",
        "# ========== PROSES TRANSLASI (BATCH MODE) ==========\n",
        "if len(indices_to_process) > 0:\n",
        "    # Ambil list judul yang akan diterjemahkan\n",
        "    titles_en = df.loc[indices_to_process, 'title'].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    translated_titles = []\n",
        "\n",
        "    # Gunakan tqdm untuk memantau proses\n",
        "    # translator() bisa menerima list langsung untuk batching otomatis di internal pipeline\n",
        "    for out in tqdm(translator(titles_en, truncation=True, max_length=512), total=len(titles_en), desc=\"Translating Titles\"):\n",
        "        translated_titles.append(out['translation_text'])\n",
        "\n",
        "    # Masukkan kembali hasil translasi ke DataFrame\n",
        "    for i, idx in enumerate(indices_to_process):\n",
        "        df.at[idx, 'title_id'] = translated_titles[i]\n",
        "\n",
        "# ========== SIMPAN HASIL ==========\n",
        "# Urutan kolom yang diminta: title, text_cleaned, label, text_id, title_id\n",
        "# Kita simpan semuanya tanpa menghapus apapun\n",
        "df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Selesai! Judul telah diterjemahkan.\")\n",
        "print(f\"üíæ File disimpan sebagai: {OUTPUT_FILE}\")\n",
        "print(f\"üìä Total baris: {len(df)}\")\n",
        "\n",
        "# Tampilkan sampel singkat\n",
        "print(\"\\nüîç Sampel Hasil:\")\n",
        "print(df[['title', 'title_id']].head(3))"
      ],
      "metadata": {
        "id": "kXjfnIfvalat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAPIKAN"
      ],
      "metadata": {
        "id": "Fmi-I9vUZ7Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_cek = pd.read_csv('health_translated_id_COMPLETE.csv')\n",
        "print(df_cek.head(10)) # Melihat 10 baris pertama\n",
        "print(df_cek.info()) # Mengecek jumlah baris dan kolom yang terdeteksi"
      ],
      "metadata": {
        "id": "JHuN270hZ9mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def clean_id_text(text):\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower() # Lowercase agar seragam\n",
        "\n",
        "    # 1. Hapus sisa-sisa karakter aneh hasil translasi\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', ' ', text)\n",
        "\n",
        "    # 2. Hapus spasi berlebih di tengah dan di ujung\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Load hasil translasi Anda\n",
        "df = pd.read_csv('health_translated_id_COMPLETE.csv')\n",
        "\n",
        "# 1. Gabungkan Title dan Text (Title + separator + Text)\n",
        "# Penggunaan titik setelah judul membantu model tahu batas kalimat\n",
        "df['full_text_id'] = df['title_id'].astype(str) + \". \" + df['text_id'].astype(str)\n",
        "\n",
        "# 2. Jalankan pembersihan teks Indonesia\n",
        "print(\"üßπ Membersihkan teks bahasa Indonesia...\")\n",
        "df['text_cleaned_id'] = df['full_text_id'].apply(clean_id_text)\n",
        "\n",
        "# 3. Pilih kolom final saja\n",
        "df_final = df[['text_cleaned_id', 'label']].copy()\n",
        "\n",
        "# 4. Hapus baris yang mungkin kosong\n",
        "df_final = df_final.dropna()\n",
        "df_final = df_final[df_final['text_cleaned_id'].str.len() > 10]\n",
        "\n",
        "# 5. Simpan dataset final\n",
        "df_final.to_csv('health_id_FINAL.csv', index=False)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"‚úÖ Dataset Final Siap! Total data: {len(df_final)}\")\n",
        "print(f\"Distribusi Label:\\n{df_final['label'].value_counts()}\")"
      ],
      "metadata": {
        "id": "CGVmsHdfaNso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_cek = pd.read_csv('health_id_FINAL.csv')\n",
        "print(df_cek.head(10)) # Melihat 10 baris pertama\n",
        "print(df_cek.info()) # Mengecek jumlah baris dan kolom yang terdeteksi"
      ],
      "metadata": {
        "id": "TKevBoOuB_A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "rvZaxN7if5g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import re\n",
        "import os\n",
        "\n",
        "# 1. LOAD DATA\n",
        "file_path = 'health_translated_id_COMPLETE.csv'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"‚ùå File {file_path} tidak ditemukan!\")\n",
        "else:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "# 2. PENYESUAIAN LABEL & KOLOM\n",
        "col_en = 'text_cleaned'\n",
        "col_id = 'text_id'\n",
        "\n",
        "if df['label'].dtype in ['int64', 'int32']:\n",
        "    df['label_name'] = df['label'].map({1: 'Real', 0: 'Fake'})\n",
        "else:\n",
        "    df['label_name'] = df['label']\n",
        "\n",
        "# ============================================================================\n",
        "# 3. DAFTAR KATA HUBUNG & KATA TUGAS (STOPWORDS) - LEBIH LENGKAP\n",
        "# ============================================================================\n",
        "# Daftar ini hanya digunakan untuk FILTER VISUALISASI, bukan menghapus data asli\n",
        "stopwords_id = set([\n",
        "    'yang', 'untuk', 'pada', 'ke', 'dari', 'dalam', 'dengan', 'ini', 'itu', 'di',\n",
        "    'dan', 'atau', 'adalah', 'akan', 'ada', 'juga', 'sudah', 'telah', 'saya',\n",
        "    'bahwa', 'oleh', 'tersebut', 'bisa', 'lebih', 'karena', 'sebagai', 'namun',\n",
        "    'serta', 'tetapi', 'tapi', 'maka', 'jika', 'bila', 'apabila', 'kalau', 'hanya',\n",
        "    'secara', 'menjadi', 'masih', 'belum', 'pernah', 'banyak', 'setiap', 'suatu',\n",
        "    'sebuah', 'ia', 'dia', 'mereka', 'kita', 'kami', 'anda', 'hal', 'paling',\n",
        "    'sangat', 'sekali', 'tentang', 'hingga', 'kepada', 'terhadap', 'atas', 'bagi',\n",
        "    'oleh', 'saat', 'ketika', 'setelah', 'sebelum', 'lalu', 'kemudian', 'yaitu',\n",
        "    'yakni', 'ialah', 'merupakan', 'mungkin', 'ingin', 'para', 'seperti', 'begitu', 'tidak','dapat', 'memiliki'\n",
        "])\n",
        "\n",
        "# ============================================\n",
        "# 4. FUNGSI PREPROCESSING KHUSUS VISUALISASI\n",
        "# ============================================\n",
        "def clean_text_for_viz(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text) # Hapus simbol/angka\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Kita buat kolom sementara hanya untuk EDA (tidak merusak data asli)\n",
        "df['eda_clean_id'] = df[col_id].apply(clean_text_for_viz)\n",
        "\n",
        "# ============================================\n",
        "# VISUALISASI 1: DISTRIBUSI KELAS\n",
        "# ============================================\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.countplot(data=df, x='label_name', palette=['#2ecc71', '#e74c3c'])\n",
        "plt.title('Distribusi Data: Berita ASLI vs PALSU', fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# VISUALISASI 2: TOP 20 WORDS (TANPA KATA HUBUNG)\n",
        "# ============================================\n",
        "def get_top_n_words(corpus, n=20):\n",
        "    all_words = ' '.join(corpus).split()\n",
        "    # FILTER: Tidak ada di stopwords_id dan panjang kata > 3\n",
        "    filtered_words = [w for w in all_words if w not in stopwords_id and len(w) > 3]\n",
        "    return pd.DataFrame(Counter(filtered_words).most_common(n), columns=['Word', 'Freq'])\n",
        "\n",
        "top_real = get_top_n_words(df[df['label_name'] == 'Real']['eda_clean_id'])\n",
        "top_fake = get_top_n_words(df[df['label_name'] == 'Fake']['eda_clean_id'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
        "sns.barplot(data=top_real, x='Freq', y='Word', ax=ax[0], palette='Greens_r')\n",
        "ax[0].set_title('Top 20 Kata Bermakna: Berita ASLI', fontsize=15)\n",
        "sns.barplot(data=top_fake, x='Freq', y='Word', ax=ax[1], palette='Reds_r')\n",
        "ax[1].set_title('Top 20 Kata Bermakna: Berita PALSU', fontsize=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# VISUALISASI 3: WORD CLOUD (TANPA KATA HUBUNG)\n",
        "# ============================================\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# WordCloud Real\n",
        "real_text = ' '.join(df[df['label_name'] == 'Real']['eda_clean_id'])\n",
        "wc_real = WordCloud(width=800, height=500, background_color='white',\n",
        "                    stopwords=stopwords_id, colormap='summer',\n",
        "                    min_font_size=10).generate(real_text)\n",
        "axes[0].imshow(wc_real)\n",
        "axes[0].set_title('WordCloud: Konten Berita ASLI', fontsize=18)\n",
        "axes[0].axis('off')\n",
        "\n",
        "# WordCloud Fake\n",
        "fake_text = ' '.join(df[df['label_name'] == 'Fake']['eda_clean_id'])\n",
        "wc_fake = WordCloud(width=800, height=500, background_color='white',\n",
        "                    stopwords=stopwords_id, colormap='autumn',\n",
        "                    min_font_size=10).generate(fake_text)\n",
        "axes[1].imshow(wc_fake)\n",
        "axes[1].set_title('WordCloud: Konten Berita PALSU', fontsize=18)\n",
        "axes[1].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# VISUALISASI 4: PANJANG TEKS & RINGKASAN\n",
        "# ============================================\n",
        "df['word_count'] = df[col_id].str.split().str.len()\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(data=df, x='label_name', y='word_count', palette=['#2ecc71', '#e74c3c'])\n",
        "plt.title('Perbandingan Panjang Kata per Artikel')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä RINGKASAN STATISTIK DATASET\")\n",
        "print(df.groupby('label_name')['word_count'].agg(['count', 'mean', 'max']).round(2))"
      ],
      "metadata": {
        "id": "fjaPHXnff8Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DEFINISI FILTER KATA (STOPWORDS) - UNTUK VISUALISASI\n",
        "# ============================================================================\n",
        "# Kita masukkan 'tidak', 'itu', 'ini', dan kata hubung lainnya di sini\n",
        "stopwords_id = set([\n",
        "    'yang', 'untuk', 'pada', 'ke', 'dari', 'dalam', 'dengan', 'ini', 'itu', 'di',\n",
        "    'dan', 'atau', 'adalah', 'akan', 'ada', 'juga', 'sudah', 'telah', 'saya',\n",
        "    'bahwa', 'oleh', 'tersebut', 'bisa', 'lebih', 'karena', 'sebagai', 'tidak',\n",
        "    'bukan', 'jangan', 'namun', 'serta', 'tetapi', 'tapi', 'maka', 'jika', 'bila',\n",
        "    'hanya', 'secara', 'menjadi', 'masih', 'belum', 'pernah', 'banyak', 'setiap',\n",
        "    'suatu', 'sebuah', 'ia', 'dia', 'mereka', 'kita', 'kami', 'anda', 'hal',\n",
        "    'paling', 'sangat', 'sekali', 'tentang', 'hingga', 'kepada', 'terhadap',\n",
        "    'atas', 'bagi', 'saat', 'ketika', 'setelah', 'sebelum', 'lalu', 'kemudian',\n",
        "    'yaitu', 'yakni', 'ialah', 'merupakan', 'mungkin', 'ingin', 'para', 'seperti', 'tidak', 'dapat', 'memiliki'\n",
        "])\n",
        "\n",
        "# ============================================================================\n",
        "# 2. FUNGSI UNTUK MENDAPATKAN FREKUENSI KATA\n",
        "# ============================================================================\n",
        "def get_top_20_words(corpus):\n",
        "    all_words = []\n",
        "    for text in corpus:\n",
        "        if pd.isna(text): continue\n",
        "        # Bersihkan teks (hanya huruf, lowercase)\n",
        "        text = re.sub(r'[^a-z\\s]', ' ', str(text).lower())\n",
        "        # Filter kata hubung & kata pendek (< 4 huruf seringkali bukan kata konten)\n",
        "        words = [w for w in text.split() if w not in stopwords_id and len(w) > 3]\n",
        "        all_words.extend(words)\n",
        "\n",
        "    # Hitung 20 terbanyak\n",
        "    return pd.DataFrame(Counter(all_words).most_common(20), columns=['Kata', 'Frekuensi'])\n",
        "\n",
        "# ============================================================================\n",
        "# 3. PROSES DATA & VISUALISASI\n",
        "# ============================================================================\n",
        "# Ambil data berdasarkan label\n",
        "top_real = get_top_20_words(df[df['label_name'] == 'Real']['text_id'])\n",
        "top_fake = get_top_20_words(df[df['label_name'] == 'Fake']['text_id'])\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
        "\n",
        "# Grafik Berita ASLI\n",
        "sns.barplot(data=top_real, x='Frekuensi', y='Kata', ax=ax[0], palette='Greens_r')\n",
        "ax[0].set_title('Top 20 Kata Bermakna: Berita Kesehatan ASLI', fontsize=15, fontweight='bold')\n",
        "ax[0].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Grafik Berita PALSU\n",
        "sns.barplot(data=top_fake, x='Frekuensi', y='Kata', ax=ax[1], palette='Reds_r')\n",
        "ax[1].set_title('Top 20 Kata Bermakna: Berita Kesehatan PALSU', fontsize=15, fontweight='bold')\n",
        "ax[1].grid(axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_20_words_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Menampilkan tabel frekuensi untuk lampiran skripsi\n",
        "print(\"\\n--- TABEL FREKUENSI KATA (ASLI) ---\")\n",
        "print(top_real)\n",
        "print(\"\\n--- TABEL FREKUENSI KATA (PALSU) ---\")\n",
        "print(top_fake)"
      ],
      "metadata": {
        "id": "S9s02YlEhcsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SPLIT & DATA AUGMENTATION"
      ],
      "metadata": {
        "id": "ptgLlJJpDh36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load Dataset Final\n",
        "df = pd.read_csv('health_id_FINAL.csv')\n",
        "\n",
        "# 2. Split Data (70% Train, 15% Val, 15% Test)\n",
        "# Kita gunakan stratify agar distribusi label tetap sama di tiap potongan\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, test_size=0.3, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# Split sisa 30% menjadi Val (15%) dan Test (15%)\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä UKURAN DATASET SEBELUM BALANCING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "print(f\"Total: {len(df)}\")\n",
        "\n",
        "# 3. Data Augmentation Sederhana (Oversampling) pada DATA TRAIN\n",
        "# Kita buat jumlah Label 1 sama dengan Label 0 di data training\n",
        "train_label_0 = train_df[train_df['label'] == 0]\n",
        "train_label_1 = train_df[train_df['label'] == 1]\n",
        "\n",
        "# Hitung selisihnya\n",
        "n_samples_to_add = len(train_label_0) - len(train_label_1)\n",
        "\n",
        "# Ambil sampel tambahan dari label 1 secara acak (duplikasi kreatif)\n",
        "train_label_1_augmented = train_label_1.sample(n_samples_to_add, replace=True, random_state=42)\n",
        "\n",
        "# Gabungkan kembali\n",
        "train_df_balanced = pd.concat([train_label_0, train_label_1, train_label_1_augmented])\n",
        "train_df_balanced = shuffle(train_df_balanced, random_state=42)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ UKURAN SETELAH BALANCING (TRAIN ONLY)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Label 0: {len(train_df_balanced[train_df_balanced['label']==0])}\")\n",
        "print(f\"Label 1: {len(train_df_balanced[train_df_balanced['label']==1])}\")\n",
        "print(f\"Total Train: {len(train_df_balanced)}\")\n",
        "\n",
        "# 4. Simpan ke CSV Terpisah\n",
        "train_df_balanced.to_csv('train_dataset_augmented.csv', index=False)\n",
        "val_df.to_csv('val_dataset.csv', index=False)\n",
        "test_df.to_csv('test_dataset.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üíæ FILE BERHASIL DISIMPAN\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ train_dataset_augmented.csv\")\n",
        "print(\"‚úÖ val_dataset.csv\")\n",
        "print(\"‚úÖ test_dataset.csv\")"
      ],
      "metadata": {
        "id": "EIFc9lpzDmhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Pengaturan gaya visualisasi\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# --- VISUALISASI 1: PROPORSI PEMBAGIAN DATA ---\n",
        "plt.subplot(1, 2, 1)\n",
        "split_counts = [len(train_df), len(val_df), len(test_df)]\n",
        "split_labels = ['Train (Original)', 'Validation', 'Test']\n",
        "colors = ['#4e79a7', '#f28e2b', '#e15759']\n",
        "\n",
        "plt.pie(split_counts, labels=split_labels, autopct='%1.1f%%',\n",
        "        startangle=140, colors=colors, pctdistance=0.85,\n",
        "        explode=[0.05, 0, 0])\n",
        "\n",
        "# Membuat lubang di tengah agar jadi Donut Chart\n",
        "centre_circle = plt.Circle((0,0), 0.70, fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "plt.title(\"Proporsi Pembagian Dataset Utama\", fontsize=14, pad=20)\n",
        "\n",
        "\n",
        "# --- VISUALISASI 2: PERBANDINGAN LABEL (BEFORE VS AFTER BALANCING) ---\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Data untuk plotting\n",
        "labels = ['Label 0 (Real)', 'Label 1 (Fake)']\n",
        "before_balancing = [len(train_label_0), len(train_label_1)]\n",
        "after_balancing = [\n",
        "    len(train_df_balanced[train_df_balanced['label']==0]),\n",
        "    len(train_df_balanced[train_df_balanced['label']==1])\n",
        "]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, before_balancing, width, label='Sebelum Balancing', color='#a0cbe8')\n",
        "plt.bar(x + width/2, after_balancing, width, label='Sesudah Balancing', color='#4e79a7')\n",
        "\n",
        "plt.ylabel('Jumlah Sampel')\n",
        "plt.title('Dampak Balancing pada Data Training', fontsize=14)\n",
        "plt.xticks(x, labels)\n",
        "plt.legend()\n",
        "\n",
        "# Tambahkan label angka di atas bar\n",
        "for i, v in enumerate(before_balancing):\n",
        "    plt.text(i - width/2, v + 5, str(v), ha='center', fontweight='bold')\n",
        "for i, v in enumerate(after_balancing):\n",
        "    plt.text(i + width/2, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3hNB-kLKCrOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data untuk visualisasi\n",
        "datasets = ['Train\\n(Sebelum)', 'Train\\n(Sesudah)', 'Val', 'Test']\n",
        "label_0 = [\n",
        "    len(train_label_0),\n",
        "    len(train_df_balanced[train_df_balanced['label']==0]),\n",
        "    len(val_df[val_df['label']==0]),\n",
        "    len(test_df[test_df['label']==0])\n",
        "]\n",
        "label_1 = [\n",
        "    len(train_label_1),\n",
        "    len(train_df_balanced[train_df_balanced['label']==1]),\n",
        "    len(val_df[val_df['label']==1]),\n",
        "    len(test_df[test_df['label']==1])\n",
        "]\n",
        "\n",
        "# Buat visualisasi\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Grafik 1: Stacked Bar Chart\n",
        "x = np.arange(len(datasets))\n",
        "width = 0.6\n",
        "\n",
        "ax1.bar(x, label_0, width, label='Label 0 (Real)', color='#4CAF50', alpha=0.8)\n",
        "ax1.bar(x, label_1, width, bottom=label_0, label='Label 1 (Fake)', color='#F44336', alpha=0.8)\n",
        "\n",
        "ax1.set_ylabel('Jumlah Sampel', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Distribusi Dataset Sebelum & Sesudah Balancing', fontsize=12, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(datasets)\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Tambahkan angka di bar\n",
        "for i, (l0, l1) in enumerate(zip(label_0, label_1)):\n",
        "    ax1.text(i, l0/2, str(l0), ha='center', va='center', fontweight='bold', color='white')\n",
        "    ax1.text(i, l0 + l1/2, str(l1), ha='center', va='center', fontweight='bold', color='white')\n",
        "\n",
        "# Grafik 2: Pie Chart Perbandingan\n",
        "sizes_before = [len(train_label_0), len(train_label_1)]\n",
        "sizes_after = [len(train_df_balanced[train_df_balanced['label']==0]),\n",
        "               len(train_df_balanced[train_df_balanced['label']==1])]\n",
        "\n",
        "ax2_left = plt.subplot(1, 2, 2)\n",
        "colors = ['#4CAF50', '#F44336']\n",
        "\n",
        "# Pie sebelum balancing\n",
        "wedges1, texts1, autotexts1 = ax2_left.pie(\n",
        "    sizes_before, labels=['Label 0', 'Label 1'], autopct='%1.1f%%',\n",
        "    colors=colors, startangle=90, explode=(0.05, 0.05)\n",
        ")\n",
        "for autotext in autotexts1:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontweight('bold')\n",
        "\n",
        "ax2_left.set_title('Train - Sebelum vs Sesudah Balancing\\n(Outer: Sebelum | Inner: Sesudah)',\n",
        "                   fontsize=11, fontweight='bold')\n",
        "\n",
        "# Pie sesudah balancing (inner)\n",
        "wedges2, texts2, autotexts2 = ax2_left.pie(\n",
        "    sizes_after, autopct='%1.1f%%', colors=colors,\n",
        "    startangle=90, radius=0.7, textprops={'size': 9}\n",
        ")\n",
        "for autotext in autotexts2:\n",
        "    autotext.set_color('white')\n",
        "    autotext.set_fontweight('bold')\n",
        "    autotext.set_fontsize(9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary Statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìà RINGKASAN VISUALISASI\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Peningkatan Label 1 (Train): {len(train_label_1)} ‚Üí {len(train_df_balanced[train_df_balanced['label']==1])} \"\n",
        "      f\"(+{n_samples_to_add} sampel)\")\n",
        "print(f\"Rasio Label 0:1 (Train Sebelum): {len(train_label_0)/len(train_label_1):.2f}:1\")\n",
        "print(f\"Rasio Label 0:1 (Train Sesudah): 1:1 (Balanced ‚úÖ)\")"
      ],
      "metadata": {
        "id": "PuvV6juvDTdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENISASI"
      ],
      "metadata": {
        "id": "3_uUi2mHER1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üî§ STEP 5: UNIFIED TOKENIZATION FOR INDOBERT & BILSTM\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. SETUP & LOAD TOKENIZER\n",
        "MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
        "print(f\"\\n1Ô∏è‚É£ Loading IndoBERT Tokenizer: {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# 2. LOAD DATASETS\n",
        "print(\"\\n2Ô∏è‚É£ Loading CSV datasets...\")\n",
        "try:\n",
        "    train_df = pd.read_csv('train_dataset_augmented.csv')\n",
        "    val_df = pd.read_csv('val_dataset.csv')\n",
        "    test_df = pd.read_csv('test_dataset.csv')\n",
        "    print(f\"‚úÖ Data Loaded: Train({len(train_df)}), Val({len(val_df)}), Test({len(test_df)})\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error: {e}. Pastikan file CSV hasil split sudah ada.\")\n",
        "    raise\n",
        "\n",
        "# 3. FUNGSI TOKENISASI\n",
        "def tokenize_process(texts, max_length=512):\n",
        "    print(f\"   Tokenizing {len(texts)} samples...\")\n",
        "    encoded = tokenizer(\n",
        "        texts.tolist(),\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='np'\n",
        "    )\n",
        "    return encoded['input_ids'], encoded['attention_mask']\n",
        "\n",
        "# 4. EKSEKUSI TOKENISASI\n",
        "print(\"\\n3Ô∏è‚É£ Running Tokenization (Max Length: 512)...\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Kolom target: text_cleaned_id\n",
        "text_col = 'text_cleaned_id'\n",
        "\n",
        "print(\"üìù Processing Train Set...\")\n",
        "train_ids, train_mask = tokenize_process(train_df[text_col])\n",
        "\n",
        "print(\"üìù Processing Validation Set...\")\n",
        "val_ids, val_mask = tokenize_process(val_df[text_col])\n",
        "\n",
        "print(\"üìù Processing Test Set...\")\n",
        "test_ids, test_mask = tokenize_process(test_df[text_col])\n",
        "\n",
        "# 5. SIMPAN DATA KE .NPY\n",
        "print(\"\\n4Ô∏è‚É£ Saving tokenized data to /content/tokenized_data/...\")\n",
        "os.makedirs('/content/tokenized_data', exist_ok=True)\n",
        "path = '/content/tokenized_data/'\n",
        "\n",
        "# Train\n",
        "np.save(path + 'train_ids.npy', train_ids)\n",
        "np.save(path + 'train_mask.npy', train_mask)\n",
        "np.save(path + 'train_labels.npy', train_df['label'].values)\n",
        "\n",
        "# Validation\n",
        "np.save(path + 'val_ids.npy', val_ids)\n",
        "np.save(path + 'val_mask.npy', val_mask)\n",
        "np.save(path + 'val_labels.npy', val_df['label'].values)\n",
        "\n",
        "# Test\n",
        "np.save(path + 'test_ids.npy', test_ids)\n",
        "np.save(path + 'test_mask.npy', test_mask)\n",
        "np.save(path + 'test_labels.npy', test_df['label'].values)\n",
        "\n",
        "# Simpan Tokenizer (Penting untuk Inference nanti)\n",
        "with open(path + 'indobert_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "print(f\"\\n‚úÖ SEMUA DATA BERHASIL DISIMPAN DI {path}\")\n",
        "\n",
        "# 6. VERIFIKASI AKHIR\n",
        "print(\"\\nüîç VERIFIKASI SHAPE:\")\n",
        "print(f\"   Train IDs: {train_ids.shape}\")\n",
        "print(f\"   Train Labels: {train_df['label'].values.shape}\")\n",
        "print(f\"   Vocab Size: {tokenizer.vocab_size}\")\n",
        "\n",
        "print(\"\\nüöÄ SIAP UNTUK STEP 6: MODEL TRAINING!\")"
      ],
      "metadata": {
        "id": "SU80wf1ZDzzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Membuat folder results jika belum ada\n",
        "if not os.path.exists('results'):\n",
        "    os.makedirs('results')\n",
        "    print(\"‚úÖ Folder 'results' berhasil dibuat!\")"
      ],
      "metadata": {
        "id": "fYd-QrLDBpIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INDOBERT"
      ],
      "metadata": {
        "id": "jCcvm1WUEiJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from transformers import TFBertModel\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import gc\n",
        "\n",
        "# 1. SETUP & MEMORY CLEANUP\n",
        "keras.backend.clear_session()\n",
        "gc.collect()\n",
        "\n",
        "print(\"üîß Configuring Robust Training (10 Epochs, 6 Layers Frozen, LR 2e-5)...\")\n",
        "\n",
        "# 2. LOAD & PREPARE DATA\n",
        "PATH = '/content/tokenized_data/'\n",
        "train_ids = np.load(PATH + 'train_ids.npy')[:, :256]\n",
        "train_mask = np.load(PATH + 'train_mask.npy')[:, :256]\n",
        "train_labels = np.load(PATH + 'train_labels.npy')\n",
        "\n",
        "val_ids = np.load(PATH + 'val_ids.npy')[:, :256]\n",
        "val_mask = np.load(PATH + 'val_mask.npy')[:, :256]\n",
        "val_labels = np.load(PATH + 'val_labels.npy')\n",
        "\n",
        "test_ids = np.load(PATH + 'test_ids.npy')[:, :256]\n",
        "test_mask = np.load(PATH + 'test_mask.npy')[:, :256]\n",
        "test_labels = np.load(PATH + 'test_labels.npy')\n",
        "\n",
        "# 3. CALCULATE CLASS WEIGHTS\n",
        "weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights = {0: weights[0], 1: weights[1]}\n",
        "print(f\"‚öñÔ∏è Class Weights applied: {class_weights}\")\n",
        "\n",
        "# 4. ROBUST ARCHITECTURE\n",
        "class IndoBERTConnector(keras.layers.Layer):\n",
        "    def __init__(self, bert_model, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.bert = bert_model\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return output.last_hidden_state\n",
        "\n",
        "def build_robust_indobert(learning_rate=2e-5):\n",
        "    # Load Pre-trained IndoBERT\n",
        "    bert_backbone = TFBertModel.from_pretrained('indobenchmark/indobert-base-p1', from_pt=True)\n",
        "\n",
        "    # --- FREEZING STRATEGY ---\n",
        "    # Membekukan 6 layer awal (0-5)\n",
        "    for i in range(6):\n",
        "        bert_backbone.bert.encoder.layer[i].trainable = False\n",
        "        print(f\"Layer {i} is now frozen.\")\n",
        "\n",
        "    # Membekukan pooler untuk menghindari gradient warning\n",
        "    bert_backbone.bert.pooler.trainable = False\n",
        "\n",
        "    # Pastikan backbone utama tetap aktif untuk layer sisa (6-11)\n",
        "    bert_backbone.trainable = True\n",
        "\n",
        "    input_ids = keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
        "    attention_mask = keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "    # BERT Layer\n",
        "    sequence_output = IndoBERTConnector(bert_backbone)([input_ids, attention_mask])\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "\n",
        "    # Robust Classification Head\n",
        "    x = keras.layers.Dense(512, activation='relu')(cls_token)\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "    x = keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# 5. EXECUTE TRAINING\n",
        "# Menggunakan LR 2e-5 untuk mengoptimalkan layer yang tidak di-freeze\n",
        "model = build_robust_indobert(learning_rate=2e-5)\n",
        "\n",
        "# EarlyStopping dihapus sesuai permintaan\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        './models/indobert_robust_final.weights.h5',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüöÄ Memulai Training (10 Epochs, Batch 8)...\")\n",
        "history = model.fit(\n",
        "    x=[train_ids, train_mask],\n",
        "    y=train_labels,\n",
        "    validation_data=([val_ids, val_mask], val_labels),\n",
        "    epochs=10, # Diubah menjadi 10 Epoch\n",
        "    batch_size=8,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 6. EVALUATION\n",
        "print(\"\\nüìä Evaluasi Akhir:\")\n",
        "test_pred_proba = model.predict([test_ids, test_mask], batch_size=8)\n",
        "test_pred = (test_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\nüéØ Accuracy: {accuracy_score(test_labels, test_pred):.4f}\")\n",
        "print(classification_report(test_labels, test_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan prediksi\n",
        "if not os.path.exists('./results'): os.makedirs('./results')\n",
        "np.save('./results/indobert_robust_10ep_preds.npy', test_pred_proba)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "7FmtMATbEj8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüìä Evaluasi Akhir (Robust Model):\")\n",
        "test_pred_proba = model.predict([test_ids, test_mask], batch_size=8)\n",
        "test_pred = (test_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\nüéØ Accuracy: {accuracy_score(test_labels, test_pred):.4f}\")\n",
        "print(classification_report(test_labels, test_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan prediksi TEST untuk Ensemble\n",
        "np.save('./results/indobert_robust_test_preds.npy', test_pred_proba)\n",
        "\n",
        "# ‚ú® TAMBAHAN: Simpan prediksi TRAINING untuk Stacking Ensemble ‚ú®\n",
        "print(\"\\nüíæ Menyimpan prediksi training set untuk Stacking...\")\n",
        "train_pred_proba = model.predict([train_ids, train_mask], batch_size=8)\n",
        "np.save('./results/indobert_robust_train_preds.npy', train_pred_proba)\n",
        "print(\"‚úÖ Training predictions saved: ./results/indobert_robust_train_preds.npy\")\n",
        "\n",
        "# ‚ú® BONUS: Simpan prediksi VALIDATION juga (opsional, untuk analisis) ‚ú®\n",
        "val_pred_proba = model.predict([val_ids, val_mask], batch_size=8)\n",
        "np.save('./results/indobert_robust_val_preds.npy', val_pred_proba)\n",
        "print(\"‚úÖ Validation predictions saved: ./results/indobert_robust_val_preds.npy\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "1YdO4aGWmuzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI HASIL TRAINING (10 EPOCHS)\n",
        "# ============================================================================\n",
        "\n",
        "# Mengambil data dari objek history\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "# Membuat Frame Gambar\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# 1. Plot Training & Validation Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, 'b-o', label='Training Accuracy', markersize=6) # 'b-o' artinya biru, garis, dan titik\n",
        "plt.plot(epochs_range, val_acc, 'r-s', label='Validation Accuracy', markersize=6) # 'r-s' artinya merah, garis, dan kotak\n",
        "plt.title('Training and Validation Accuracy', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.xticks(epochs_range) # Memastikan angka epoch muncul semua di sumbu X\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. Plot Training & Validation Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, 'b-o', label='Training Loss', markersize=6)\n",
        "plt.plot(epochs_range, val_loss, 'r-s', label='Validation Loss', markersize=6)\n",
        "plt.title('Training and Validation Loss', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.xticks(epochs_range)\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Menyimpan hasil visualisasi untuk Skripsi\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/training_curves_indobert_10ep.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualisasi selesai dibuat dan disimpan di folder './results/'\")"
      ],
      "metadata": {
        "id": "q6DrudYRdXzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Set style untuk visualisasi\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (16, 12)\n",
        "\n",
        "# ========== 1. TRAINING & VALIDATION CURVES ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training & Validation Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='#e74c3c')\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='#3498db')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Tambahkan marker untuk best epoch\n",
        "best_epoch = np.argmin(history.history['val_loss'])\n",
        "axes[0, 0].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7, label=f'Best Epoch ({best_epoch+1})')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "\n",
        "# Plot 2: Training & Validation Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='#e74c3c')\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#3498db')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Plot 3: Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
        "            ax=axes[1, 0], annot_kws={'size': 16, 'weight': 'bold'})\n",
        "axes[1, 0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Tambahkan persentase di dalam cell\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        percentage = cm[i, j] / cm[i].sum() * 100\n",
        "        axes[1, 0].text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                       ha='center', va='center', fontsize=10, color='gray')\n",
        "\n",
        "# Plot 4: ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "axes[1, 1].plot(fpr, tpr, color='#e74c3c', linewidth=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "axes[1, 1].plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
        "axes[1, 1].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_title('ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].fill_between(fpr, tpr, alpha=0.2, color='#e74c3c')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/indobert_training_visualization.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi disimpan: ./results/indobert_training_visualization.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== 2. METRICS SUMMARY TABLE ==========\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RINGKASAN METRIK EVALUASI\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics_data = {\n",
        "    'Metric': ['Accuracy', 'Precision (Real)', 'Precision (Fake)',\n",
        "               'Recall (Real)', 'Recall (Fake)', 'F1-Score (Real)', 'F1-Score (Fake)', 'AUC-ROC'],\n",
        "    'Value': [\n",
        "        accuracy_score(test_labels, test_pred),\n",
        "        precision_score(test_labels, test_pred, pos_label=0),\n",
        "        precision_score(test_labels, test_pred, pos_label=1),\n",
        "        recall_score(test_labels, test_pred, pos_label=0),\n",
        "        recall_score(test_labels, test_pred, pos_label=1),\n",
        "        f1_score(test_labels, test_pred, pos_label=0),\n",
        "        f1_score(test_labels, test_pred, pos_label=1),\n",
        "        roc_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simpan metrics ke CSV\n",
        "metrics_df.to_csv('./results/indobert_metrics_summary.csv', index=False)\n",
        "print(\"‚úÖ Metrics disimpan: ./results/indobert_metrics_summary.csv\")\n",
        "\n",
        "# ========== 3. PREDICTION DISTRIBUTION ==========\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot distribusi probabilitas prediksi untuk Real vs Fake\n",
        "real_probs = test_pred_proba[test_labels == 0].flatten()\n",
        "fake_probs = test_pred_proba[test_labels == 1].flatten()\n",
        "\n",
        "axes[0].hist(real_probs, bins=30, alpha=0.7, color='#3498db', edgecolor='black', label='Real News')\n",
        "axes[0].hist(fake_probs, bins=30, alpha=0.7, color='#e74c3c', edgecolor='black', label='Fake News')\n",
        "axes[0].axvline(x=0.5, color='green', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "axes[0].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot persentase prediksi benar vs salah\n",
        "correct = (test_pred == test_labels).sum()\n",
        "incorrect = len(test_labels) - correct\n",
        "labels_pie = ['Correct', 'Incorrect']\n",
        "sizes = [correct, incorrect]\n",
        "colors_pie = ['#2ecc71', '#e74c3c']\n",
        "explode = (0.05, 0)\n",
        "\n",
        "axes[1].pie(sizes, explode=explode, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%',\n",
        "            shadow=True, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
        "axes[1].set_title(f'Prediction Accuracy: {correct}/{len(test_labels)} samples', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/indobert_prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi distribusi disimpan: ./results/indobert_prediction_distribution.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéâ SEMUA VISUALISASI SELESAI!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "h1kR7It0eNhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BILSTM BASELINE"
      ],
      "metadata": {
        "id": "CVmeC334gZN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "\n",
        "# 1. PREPARE DATA & CLASS WEIGHTS\n",
        "# Asumsi: train_ids, train_labels, val_ids, dsb. sudah dimuat di sesi Colab\n",
        "print(\"‚öñÔ∏è Calculating Class Weights...\")\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "print(f\"Weights: {class_weights}\")\n",
        "\n",
        "# 2. BUILD OPTIMIZED BILSTM MODEL\n",
        "def build_optimized_bilstm():\n",
        "    # Pastikan sesi bersih\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    inputs = keras.layers.Input(shape=(256,), dtype='int32')\n",
        "\n",
        "    # Embedding Layer (input_dim disesuaikan dengan vocab size IndoBERT jika menggunakan token yang sama)\n",
        "    x = keras.layers.Embedding(input_dim=30522, output_dim=128)(inputs)\n",
        "\n",
        "    # Bidirectional LSTM - Menangkap konteks dari dua arah (maju & mundur)\n",
        "    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "    # Dual Pooling Strategy untuk menangkap fitur paling menonjol (Max) dan rata-rata konteks (Avg)\n",
        "    gap = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    gmp = keras.layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Gabungkan fitur dari kedua pooling\n",
        "    concat = keras.layers.Concatenate()([gap, gmp])\n",
        "\n",
        "    # Dense Head yang Robust\n",
        "    x = keras.layers.Dense(128, activation='relu')(concat)\n",
        "    x = keras.layers.BatchNormalization()(x) # Menjaga stabilitas gradien\n",
        "    x = keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "    x = keras.layers.Dense(64, activation='relu')(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output (Sigmoid untuk klasifikasi biner)\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # Optimizer dengan Learning Rate yang stabil\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=2e-4)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"\\nüöÄ Building Optimized BiLSTM model...\")\n",
        "model_bilstm = build_optimized_bilstm()\n",
        "model_bilstm.summary()\n",
        "\n",
        "# 3. TRAINING CONFIGURATION\n",
        "# Update: EarlyStopping dihapus, hanya menggunakan Checkpoint dan LRScheduler\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        './models/bilstm_best_opt.weights.h5',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_loss'\n",
        "    ),\n",
        "    # Strategi pencegahan kenaikan Validation Loss:\n",
        "    # Jika val_loss macet dalam 2 epoch, turunkan LR sebesar 80% (factor=0.2)\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\nüî• Memulai Training BiLSTM (10 Epochs)...\")\n",
        "history = model_bilstm.fit(\n",
        "    train_ids, train_labels,\n",
        "    validation_data=(val_ids, val_labels),\n",
        "    epochs=10,        # Sesuai permintaan: 10 Epoch\n",
        "    batch_size=8,     # Sedikit dinaikkan dari 6 ke 8 agar gradien lebih stabil\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 4. EVALUATION\n",
        "print(\"\\nüìä Evaluating BiLSTM on Test Set...\")\n",
        "test_pred_proba = model_bilstm.predict(test_ids, batch_size=8)\n",
        "test_pred = (test_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\n‚úÖ BiLSTM Accuracy: {accuracy_score(test_labels, test_pred):.4f}\")\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(test_labels, test_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan hasil untuk keperluan Ensemble\n",
        "if not os.path.exists('./results'): os.makedirs('./results')\n",
        "np.save('./results/bilstm_test_preds.npy', test_pred_proba)\n",
        "\n",
        "print(\"\\nüíæ Prediksi berhasil disimpan di ./results/bilstm_test_preds.npy\")\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "zpGLTrIMgC66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI HASIL TRAINING BILSTM (FIXED VERSION)\n",
        "# ============================================================================\n",
        "\n",
        "acc_lstm = history.history['accuracy']\n",
        "val_acc_lstm = history.history['val_accuracy']\n",
        "loss_lstm = history.history['loss']\n",
        "val_loss_lstm = history.history['val_loss']\n",
        "epochs_range = range(1, len(acc_lstm) + 1)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# 1. Plot Training & Validation Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "# Menggunakan parameter terpisah (color, marker, linestyle) agar tidak error\n",
        "plt.plot(epochs_range, acc_lstm, color='green', marker='o', linestyle='-', label='Training Accuracy', markersize=6)\n",
        "plt.plot(epochs_range, val_acc_lstm, color='orange', marker='s', linestyle='-', label='Validation Accuracy', markersize=6)\n",
        "\n",
        "plt.title('BiLSTM: Training and Validation Accuracy', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.xticks(epochs_range)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. Plot Training & Validation Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss_lstm, color='green', marker='o', linestyle='-', label='Training Loss', markersize=6)\n",
        "plt.plot(epochs_range, val_loss_lstm, color='orange', marker='s', linestyle='-', label='Validation Loss', markersize=6)\n",
        "\n",
        "plt.title('BiLSTM: Training and Validation Loss', fontsize=14)\n",
        "plt.xlabel('Epochs', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.xticks(epochs_range)\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/training_curves_bilstm_10ep_fixed.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Visualisasi BiLSTM diperbaiki dan disimpan di './results/training_curves_bilstm_10ep_fixed.png'\")"
      ],
      "metadata": {
        "id": "8xs7fff8dmtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n5Ô∏è Generating predictions on Training Set...\")\n",
        "train_pred_proba = model_bilstm.predict(train_ids)\n",
        "train_pred = (train_pred_proba >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(f\"\\n‚úÖ BiLSTM Training Accuracy: {accuracy_score(train_labels, train_pred):.4f}\")\n",
        "\n",
        "# Simpan prediksi probabilitas training untuk Ensemble\n",
        "np.save('./results/bilstm_train_preds.npy', train_pred_proba)\n",
        "print(\"üíæ Saved training predictions to './results/bilstm_train_preds.npy'\")\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n6Ô∏è‚É£ Plotting Training History...\")\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('BiLSTM Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.title('BiLSTM Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/bilstm_training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"üìä Training history plot saved to './results/bilstm_training_history.png'\")\n",
        "\n",
        "# 7. VERIFIKASI FILE YANG TERSIMPAN\n",
        "print(\"\\n7Ô∏è‚É£ Verifying saved files...\")\n",
        "files_to_check = [\n",
        "    './results/bilstm_test_preds.npy',\n",
        "    './results/bilstm_train_preds.npy',\n",
        "    './models/bilstm_best_opt.weights.h5'\n",
        "]\n",
        "\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"‚úÖ {file_path} exists ({size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\" {file_path} NOT FOUND!\")\n",
        "\n",
        "print(\"\\n BiLSTM Training Complete! Ready for ensemble.\")"
      ],
      "metadata": {
        "id": "rPIF1jb4nPrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== 6. VISUALISASI LENGKAP ==========\n",
        "print(\"\\n6Ô∏è‚É£ Creating Comprehensive Visualizations...\")\n",
        "\n",
        "# Set style untuk visualisasi\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (16, 12)\n",
        "\n",
        "# ========== VISUALISASI 1: Training & Validation Curves + CM + ROC ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Training & Validation Loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='#e74c3c')\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='#3498db')\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title('BiLSTM: Training & Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Tambahkan marker untuk best epoch\n",
        "best_epoch = np.argmin(history.history['val_loss'])\n",
        "axes[0, 0].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7,\n",
        "                   label=f'Best Epoch ({best_epoch+1})')\n",
        "axes[0, 0].legend(loc='upper right', fontsize=11)\n",
        "\n",
        "# Plot 2: Training & Validation Accuracy\n",
        "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='#e74c3c')\n",
        "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='#3498db')\n",
        "axes[0, 1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('BiLSTM: Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].axvline(x=best_epoch, color='green', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Plot 3: Confusion Matrix\n",
        "cm = confusion_matrix(test_labels, test_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
        "            ax=axes[1, 0], annot_kws={'size': 16, 'weight': 'bold'})\n",
        "axes[1, 0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title('BiLSTM: Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Tambahkan persentase di dalam cell\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        percentage = cm[i, j] / cm[i].sum() * 100\n",
        "        axes[1, 0].text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                       ha='center', va='center', fontsize=10, color='gray')\n",
        "\n",
        "# Plot 4: ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(test_labels, test_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "axes[1, 1].plot(fpr, tpr, color='#e74c3c', linewidth=2.5, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "axes[1, 1].plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
        "axes[1, 1].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_title('BiLSTM: ROC Curve (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend(loc='lower right', fontsize=11)\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].fill_between(fpr, tpr, alpha=0.2, color='#e74c3c')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/bilstm_training_visualization.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi disimpan: ./results/bilstm_training_visualization.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== VISUALISASI 2: Metrics Summary Table ==========\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RINGKASAN METRIK EVALUASI BiLSTM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics_data = {\n",
        "    'Metric': ['Accuracy', 'Precision (Real)', 'Precision (Fake)',\n",
        "               'Recall (Real)', 'Recall (Fake)', 'F1-Score (Real)', 'F1-Score (Fake)', 'AUC-ROC'],\n",
        "    'Value': [\n",
        "        accuracy_score(test_labels, test_pred),\n",
        "        precision_score(test_labels, test_pred, pos_label=0),\n",
        "        precision_score(test_labels, test_pred, pos_label=1),\n",
        "        recall_score(test_labels, test_pred, pos_label=0),\n",
        "        recall_score(test_labels, test_pred, pos_label=1),\n",
        "        f1_score(test_labels, test_pred, pos_label=0),\n",
        "        f1_score(test_labels, test_pred, pos_label=1),\n",
        "        roc_auc\n",
        "    ]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Simpan metrics ke CSV\n",
        "metrics_df.to_csv('./results/bilstm_metrics_summary.csv', index=False)\n",
        "print(\"‚úÖ Metrics disimpan: ./results/bilstm_metrics_summary.csv\")\n",
        "\n",
        "# ========== VISUALISASI 3: Prediction Distribution ==========\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot distribusi probabilitas prediksi untuk Real vs Fake\n",
        "real_probs = test_pred_proba[test_labels == 0].flatten()\n",
        "fake_probs = test_pred_proba[test_labels == 1].flatten()\n",
        "\n",
        "axes[0].hist(real_probs, bins=30, alpha=0.7, color='#3498db', edgecolor='black', label='Real News')\n",
        "axes[0].hist(fake_probs, bins=30, alpha=0.7, color='#e74c3c', edgecolor='black', label='Fake News')\n",
        "axes[0].axvline(x=0.5, color='green', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "axes[0].set_xlabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('BiLSTM: Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot persentase prediksi benar vs salah\n",
        "correct = (test_pred == test_labels).sum()\n",
        "incorrect = len(test_labels) - correct\n",
        "labels_pie = ['Correct', 'Incorrect']\n",
        "sizes = [correct, incorrect]\n",
        "colors_pie = ['#2ecc71', '#e74c3c']\n",
        "explode = (0.05, 0)\n",
        "\n",
        "axes[1].pie(sizes, explode=explode, labels=labels_pie, colors=colors_pie, autopct='%1.1f%%',\n",
        "            shadow=True, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
        "axes[1].set_title(f'BiLSTM Prediction Accuracy: {correct}/{len(test_labels)} samples',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/bilstm_prediction_distribution.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Visualisasi distribusi disimpan: ./results/bilstm_prediction_distribution.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== 7. VERIFIKASI FILE YANG TERSIMPAN ==========\n",
        "print(\"\\n7Ô∏è‚É£ Verifying saved files...\")\n",
        "files_to_check = [\n",
        "    './results/bilstm_test_preds.npy',\n",
        "    './results/bilstm_train_preds.npy',\n",
        "    './models/bilstm_best_opt.weights.h5',\n",
        "    './results/bilstm_training_visualization.png',\n",
        "    './results/bilstm_metrics_summary.csv',\n",
        "    './results/bilstm_prediction_distribution.png'\n",
        "]\n",
        "\n",
        "for file_path in files_to_check:\n",
        "    if os.path.exists(file_path):\n",
        "        size = os.path.getsize(file_path) / 1024  # KB\n",
        "        print(f\"‚úÖ {file_path} exists ({size:.2f} KB)\")\n",
        "    else:\n",
        "        print(f\"‚ùå {file_path} NOT FOUND!\")\n",
        "\n",
        "print(\"\\nüéâ SEMUA VISUALISASI BiLSTM SELESAI!\")\n",
        "print(\"=\"*60)\n",
        "print(\"‚ú® BiLSTM Training Complete! Ready for ensemble.\")"
      ],
      "metadata": {
        "id": "3kc36mVCnGpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENSEMBLE WEIGHTED AVG"
      ],
      "metadata": {
        "id": "uyj1ZIDulCA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîó STEP 7: HYBRID ENSEMBLE (INDOBERT + BILSTM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. LOAD PREDIKSI PROBABILITAS (Hasil dari Step 6A dan 6B)\n",
        "try:\n",
        "    # Pastikan file ini sudah disimpan di step sebelumnya\n",
        "    bert_probs = np.load('./results/indobert_robust_test_preds.npy')\n",
        "    lstm_probs = np.load('./results/bilstm_test_preds.npy')\n",
        "\n",
        "    # Load Label asli untuk evaluasi\n",
        "    test_labels = np.load('/content/tokenized_data/test_labels.npy')\n",
        "\n",
        "    print(\"‚úÖ Berhasil memuat prediksi dari kedua model.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: File prediksi .npy tidak ditemukan. Pastikan Step 6A dan 6B sudah dijalankan sampai selesai.\")\n",
        "    raise\n",
        "\n",
        "# 2. MENCARI BOBOT OPTIMAL (Simple Grid Search)\n",
        "best_weight = 0\n",
        "max_acc = 0\n",
        "\n",
        "print(\"\\nüîç Mencari kombinasi bobot terbaik...\")\n",
        "for i in np.arange(0, 1.1, 0.1):\n",
        "    w1 = i\n",
        "    w2 = 1 - i\n",
        "    # Gabungkan probabilitas\n",
        "    ensemble_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "    acc = accuracy_score(test_labels, ensemble_preds)\n",
        "    if acc > max_acc:\n",
        "        max_acc = acc\n",
        "        best_weight = w1\n",
        "    print(f\"   Bobot IndoBERT: {w1:.1f} | Bobot BiLSTM: {w2:.1f} -> Accuracy: {acc:.4f}\")\n",
        "\n",
        "# 3. EVALUASI AKHIR ENSEMBLE DENGAN BOBOT TERBAIK\n",
        "w1 = best_weight\n",
        "w2 = 1 - best_weight\n",
        "print(f\"\\nüèÜ Bobot Terbaik: IndoBERT ({w1:.1f}), BiLSTM ({w2:.1f})\")\n",
        "\n",
        "final_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "final_preds = (final_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"üìä HASIL AKHIR ENSEMBLE\")\n",
        "print(\"=\"*30)\n",
        "print(f\"üéØ Final Accuracy: {accuracy_score(test_labels, final_preds):.4f}\")\n",
        "print(\"\\nüìã Classification Report:\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# 4. VISUALISASI CONFUSION MATRIX\n",
        "cm = confusion_matrix(test_labels, final_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Asli')\n",
        "plt.title('Confusion Matrix: Hybrid Ensemble')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yK-AYaqniwoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (classification_report, accuracy_score, confusion_matrix,\n",
        "                            roc_curve, auc, precision_score, recall_score, f1_score)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîó STEP 7: HYBRID ENSEMBLE (INDOBERT + BILSTM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# 1. LOAD PREDIKSI PROBABILITAS (Hasil dari Step 6A dan 6B)\n",
        "try:\n",
        "    # Pastikan file ini sudah disimpan di step sebelumnya\n",
        "    bert_probs = np.load('./results/indobert_robust_test_preds.npy')\n",
        "    lstm_probs = np.load('./results/bilstm_test_preds.npy')\n",
        "\n",
        "    # Load Label asli untuk evaluasi\n",
        "    test_labels = np.load('/content/tokenized_data/test_labels.npy')\n",
        "\n",
        "    print(\"‚úÖ Berhasil memuat prediksi dari kedua model.\")\n",
        "    print(f\"   - IndoBERT predictions shape: {bert_probs.shape}\")\n",
        "    print(f\"   - BiLSTM predictions shape: {lstm_probs.shape}\")\n",
        "    print(f\"   - Test labels shape: {test_labels.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: File prediksi .npy tidak ditemukan. Pastikan Step 6A dan 6B sudah dijalankan sampai selesai.\")\n",
        "    raise\n",
        "\n",
        "# 2. MENCARI BOBOT OPTIMAL (Simple Grid Search)\n",
        "best_weight = 0\n",
        "max_acc = 0\n",
        "weight_results = []\n",
        "\n",
        "print(\"\\nüîç Mencari kombinasi bobot terbaik...\")\n",
        "print(\"-\" * 60)\n",
        "for i in np.arange(0, 1.1, 0.1):\n",
        "    w1 = i\n",
        "    w2 = 1 - i\n",
        "    # Gabungkan probabilitas\n",
        "    ensemble_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "    ensemble_preds = (ensemble_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "    acc = accuracy_score(test_labels, ensemble_preds)\n",
        "    weight_results.append({'IndoBERT_Weight': w1, 'BiLSTM_Weight': w2, 'Accuracy': acc})\n",
        "\n",
        "    if acc > max_acc:\n",
        "        max_acc = acc\n",
        "        best_weight = w1\n",
        "    print(f\"   Bobot IndoBERT: {w1:.1f} | Bobot BiLSTM: {w2:.1f} -> Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Convert ke DataFrame untuk visualisasi\n",
        "weights_df = pd.DataFrame(weight_results)\n",
        "\n",
        "# 3. EVALUASI AKHIR ENSEMBLE DENGAN BOBOT TERBAIK\n",
        "w1 = best_weight\n",
        "w2 = 1 - best_weight\n",
        "print(f\"\\nüèÜ Bobot Optimal Ditemukan!\")\n",
        "print(f\"   - IndoBERT: {w1:.2f} ({w1*100:.1f}%)\")\n",
        "print(f\"   - BiLSTM: {w2:.2f} ({w2*100:.1f}%)\")\n",
        "\n",
        "final_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "final_preds = (final_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "# Prediksi individual untuk perbandingan\n",
        "bert_preds = (bert_probs >= 0.5).astype(int).flatten()\n",
        "lstm_preds = (lstm_probs >= 0.5).astype(int).flatten()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üìä HASIL AKHIR ENSEMBLE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hitung metrik untuk semua model\n",
        "models_comparison = {\n",
        "    'Model': ['IndoBERT (Solo)', 'BiLSTM (Solo)', 'Hybrid Ensemble'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(test_labels, bert_preds),\n",
        "        accuracy_score(test_labels, lstm_preds),\n",
        "        accuracy_score(test_labels, final_preds)\n",
        "    ],\n",
        "    'Precision': [\n",
        "        precision_score(test_labels, bert_preds, average='weighted'),\n",
        "        precision_score(test_labels, lstm_preds, average='weighted'),\n",
        "        precision_score(test_labels, final_preds, average='weighted')\n",
        "    ],\n",
        "    'Recall': [\n",
        "        recall_score(test_labels, bert_preds, average='weighted'),\n",
        "        recall_score(test_labels, lstm_preds, average='weighted'),\n",
        "        recall_score(test_labels, final_preds, average='weighted')\n",
        "    ],\n",
        "    'F1-Score': [\n",
        "        f1_score(test_labels, bert_preds, average='weighted'),\n",
        "        f1_score(test_labels, lstm_preds, average='weighted'),\n",
        "        f1_score(test_labels, final_preds, average='weighted')\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(models_comparison)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nüéØ Final Ensemble Accuracy: {accuracy_score(test_labels, final_preds):.4f}\")\n",
        "print(f\"üìà Improvement over IndoBERT: {(accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, bert_preds)):.4f}\")\n",
        "print(f\"üìà Improvement over BiLSTM: {(accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, lstm_preds)):.4f}\")\n",
        "\n",
        "print(\"\\nüìã Detailed Classification Report (Ensemble):\")\n",
        "print(classification_report(test_labels, final_preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Simpan comparison ke CSV\n",
        "comparison_df.to_csv('./results/ensemble_models_comparison.csv', index=False)\n",
        "print(\"\\n‚úÖ Model comparison saved to: ./results/ensemble_models_comparison.csv\")\n",
        "\n",
        "# ========== VISUALISASI LENGKAP ==========\n",
        "print(\"\\nüìä Generating Comprehensive Visualizations...\")\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (18, 12)\n",
        "\n",
        "# ========== VISUALISASI 1: Main Dashboard (3x3) ==========\n",
        "fig = plt.figure(figsize=(18, 14))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# Plot 1: Weight Search Results\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.plot(weights_df['IndoBERT_Weight'], weights_df['Accuracy'],\n",
        "         marker='o', linewidth=2.5, markersize=8, color='#3498db')\n",
        "ax1.axvline(x=best_weight, color='#e74c3c', linestyle='--', linewidth=2,\n",
        "           label=f'Optimal Weight ({best_weight:.2f})')\n",
        "ax1.axhline(y=max_acc, color='#2ecc71', linestyle=':', linewidth=1.5, alpha=0.7)\n",
        "ax1.set_xlabel('IndoBERT Weight', fontsize=11, fontweight='bold')\n",
        "ax1.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "ax1.set_title('Weight Optimization Curve', fontsize=13, fontweight='bold')\n",
        "ax1.legend(fontsize=9)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Model Comparison Bar Chart\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "x_pos = np.arange(len(comparison_df['Model']))\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "bars = ax2.bar(x_pos, comparison_df['Accuracy'], color=colors, alpha=0.8, edgecolor='black')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels(comparison_df['Model'], rotation=15, ha='right', fontsize=10)\n",
        "ax2.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
        "ax2.set_title('Model Performance Comparison', fontsize=13, fontweight='bold')\n",
        "ax2.set_ylim([0, 1.05])\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Tambahkan nilai di atas bar\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "            f'{height:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "# Plot 3: Ensemble Weight Distribution (Pie Chart)\n",
        "ax3 = fig.add_subplot(gs[0, 2])\n",
        "sizes = [w1, w2]\n",
        "labels = ['IndoBERT', 'BiLSTM']\n",
        "colors_pie = ['#3498db', '#e74c3c']\n",
        "explode = (0.05, 0.05)\n",
        "wedges, texts, autotexts = ax3.pie(sizes, explode=explode, labels=labels, colors=colors_pie,\n",
        "                                     autopct='%1.1f%%', shadow=True, startangle=90,\n",
        "                                     textprops={'fontsize': 11, 'weight': 'bold'})\n",
        "ax3.set_title('Optimal Ensemble Weights', fontsize=13, fontweight='bold')\n",
        "\n",
        "# Plot 4-6: Confusion Matrices (3 models)\n",
        "cms = [\n",
        "    (confusion_matrix(test_labels, bert_preds), 'IndoBERT', gs[1, 0]),\n",
        "    (confusion_matrix(test_labels, lstm_preds), 'BiLSTM', gs[1, 1]),\n",
        "    (confusion_matrix(test_labels, final_preds), 'Hybrid Ensemble', gs[1, 2])\n",
        "]\n",
        "\n",
        "for cm, title, grid_pos in cms:\n",
        "    ax = fig.add_subplot(grid_pos)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "               xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'],\n",
        "               ax=ax, annot_kws={'size': 14, 'weight': 'bold'})\n",
        "    ax.set_xlabel('Predicted Label', fontsize=10, fontweight='bold')\n",
        "    ax.set_ylabel('True Label', fontsize=10, fontweight='bold')\n",
        "    ax.set_title(f'{title} - Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Tambahkan persentase\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            percentage = cm[i, j] / cm[i].sum() * 100\n",
        "            ax.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
        "                   ha='center', va='center', fontsize=9, color='gray')\n",
        "\n",
        "# Plot 7: ROC Curves Comparison\n",
        "ax7 = fig.add_subplot(gs[2, :2])\n",
        "\n",
        "# Hitung ROC untuk semua model\n",
        "fpr_bert, tpr_bert, _ = roc_curve(test_labels, bert_probs)\n",
        "roc_auc_bert = auc(fpr_bert, tpr_bert)\n",
        "\n",
        "fpr_lstm, tpr_lstm, _ = roc_curve(test_labels, lstm_probs)\n",
        "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
        "\n",
        "fpr_ensemble, tpr_ensemble, _ = roc_curve(test_labels, final_probs)\n",
        "roc_auc_ensemble = auc(fpr_ensemble, tpr_ensemble)\n",
        "\n",
        "# Plot ROC curves\n",
        "ax7.plot(fpr_bert, tpr_bert, linewidth=2.5, label=f'IndoBERT (AUC = {roc_auc_bert:.4f})',\n",
        "         color='#3498db')\n",
        "ax7.plot(fpr_lstm, tpr_lstm, linewidth=2.5, label=f'BiLSTM (AUC = {roc_auc_lstm:.4f})',\n",
        "         color='#e74c3c')\n",
        "ax7.plot(fpr_ensemble, tpr_ensemble, linewidth=3, label=f'Ensemble (AUC = {roc_auc_ensemble:.4f})',\n",
        "         color='#2ecc71', linestyle='-')\n",
        "ax7.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1.5, label='Random Classifier')\n",
        "\n",
        "ax7.set_xlabel('False Positive Rate', fontsize=11, fontweight='bold')\n",
        "ax7.set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\n",
        "ax7.set_title('ROC Curves Comparison: All Models', fontsize=13, fontweight='bold')\n",
        "ax7.legend(loc='lower right', fontsize=10)\n",
        "ax7.grid(True, alpha=0.3)\n",
        "ax7.fill_between(fpr_ensemble, tpr_ensemble, alpha=0.2, color='#2ecc71')\n",
        "\n",
        "# Plot 8: Prediction Probability Distribution\n",
        "ax8 = fig.add_subplot(gs[2, 2])\n",
        "real_probs = final_probs[test_labels == 0].flatten()\n",
        "fake_probs = final_probs[test_labels == 1].flatten()\n",
        "\n",
        "ax8.hist(real_probs, bins=25, alpha=0.7, color='#3498db', edgecolor='black', label='Real News')\n",
        "ax8.hist(fake_probs, bins=25, alpha=0.7, color='#e74c3c', edgecolor='black', label='Fake News')\n",
        "ax8.axvline(x=0.5, color='green', linestyle='--', linewidth=2.5, label='Threshold (0.5)')\n",
        "ax8.set_xlabel('Predicted Probability', fontsize=11, fontweight='bold')\n",
        "ax8.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
        "ax8.set_title('Ensemble Probability Distribution', fontsize=12, fontweight='bold')\n",
        "ax8.legend(fontsize=9)\n",
        "ax8.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('üîó HYBRID ENSEMBLE: Comprehensive Performance Analysis',\n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.savefig('./results/ensemble_comprehensive_visualization.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Main visualization saved: ./results/ensemble_comprehensive_visualization.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== VISUALISASI 2: Detailed Metrics Comparison ==========\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Plot 1: Accuracy Comparison with Error Margins\n",
        "ax = axes[0, 0]\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x_pos = np.arange(len(comparison_df['Model']))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    offset = (i - 1.5) * width\n",
        "    bars = ax.bar(x_pos + offset, comparison_df[metric], width,\n",
        "                  label=metric, alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(comparison_df['Model'], fontsize=10, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('All Metrics Comparison Across Models', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 2: Per-Class Performance (Real vs Fake)\n",
        "ax = axes[0, 1]\n",
        "real_f1 = [\n",
        "    f1_score(test_labels, bert_preds, pos_label=0),\n",
        "    f1_score(test_labels, lstm_preds, pos_label=0),\n",
        "    f1_score(test_labels, final_preds, pos_label=0)\n",
        "]\n",
        "fake_f1 = [\n",
        "    f1_score(test_labels, bert_preds, pos_label=1),\n",
        "    f1_score(test_labels, lstm_preds, pos_label=1),\n",
        "    f1_score(test_labels, final_preds, pos_label=1)\n",
        "]\n",
        "\n",
        "x = np.arange(len(comparison_df['Model']))\n",
        "width = 0.35\n",
        "bars1 = ax.bar(x - width/2, real_f1, width, label='Real News', color='#3498db', alpha=0.8, edgecolor='black')\n",
        "bars2 = ax.bar(x + width/2, fake_f1, width, label='Fake News', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(comparison_df['Model'], fontsize=10, fontweight='bold')\n",
        "ax.set_ylabel('F1-Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Per-Class F1-Score Comparison', fontsize=13, fontweight='bold')\n",
        "ax.legend(fontsize=10)\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Tambahkan nilai di atas bar\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "               f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "# Plot 3: AUC-ROC Comparison\n",
        "ax = axes[1, 0]\n",
        "auc_scores = [roc_auc_bert, roc_auc_lstm, roc_auc_ensemble]\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "bars = ax.barh(comparison_df['Model'], auc_scores, color=colors, alpha=0.8, edgecolor='black')\n",
        "ax.set_xlabel('AUC-ROC Score', fontsize=11, fontweight='bold')\n",
        "ax.set_title('AUC-ROC Comparison', fontsize=13, fontweight='bold')\n",
        "ax.set_xlim([0, 1.1])\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Tambahkan nilai di samping bar\n",
        "for i, bar in enumerate(bars):\n",
        "    width = bar.get_width()\n",
        "    ax.text(width + 0.01, bar.get_y() + bar.get_height()/2.,\n",
        "           f'{auc_scores[i]:.4f}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot 4: Improvement Summary\n",
        "ax = axes[1, 1]\n",
        "improvement_data = {\n",
        "    'Metric': ['vs IndoBERT', 'vs BiLSTM'],\n",
        "    'Accuracy Gain': [\n",
        "        accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, bert_preds),\n",
        "        accuracy_score(test_labels, final_preds) - accuracy_score(test_labels, lstm_preds)\n",
        "    ]\n",
        "}\n",
        "improvement_colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in improvement_data['Accuracy Gain']]\n",
        "bars = ax.bar(improvement_data['Metric'], improvement_data['Accuracy Gain'],\n",
        "             color=improvement_colors, alpha=0.8, edgecolor='black')\n",
        "ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
        "ax.set_ylabel('Accuracy Improvement', fontsize=11, fontweight='bold')\n",
        "ax.set_title('Ensemble Improvement Over Individual Models', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Tambahkan nilai\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "           f'{height:+.4f}', ha='center', va='bottom' if height > 0 else 'top',\n",
        "           fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.suptitle('üìä Detailed Performance Metrics Analysis', fontsize=15, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/ensemble_detailed_metrics.png', dpi=300, bbox_inches='tight')\n",
        "print(\"‚úÖ Detailed metrics visualization saved: ./results/ensemble_detailed_metrics.png\")\n",
        "plt.show()\n",
        "\n",
        "# ========== SIMPAN SEMUA HASIL ==========\n",
        "print(\"\\nüíæ Saving all results...\")\n",
        "\n",
        "# Simpan prediksi final\n",
        "np.save('./results/ensemble_final_predictions.npy', final_preds)\n",
        "np.save('./results/ensemble_final_probabilities.npy', final_probs)\n",
        "\n",
        "# Simpan weight results\n",
        "weights_df.to_csv('./results/ensemble_weight_search_results.csv', index=False)\n",
        "\n",
        "# Simpan detailed metrics\n",
        "detailed_metrics = {\n",
        "    'Model': ['IndoBERT', 'BiLSTM', 'Ensemble'],\n",
        "    'Accuracy': [accuracy_score(test_labels, bert_preds),\n",
        "                accuracy_score(test_labels, lstm_preds),\n",
        "                accuracy_score(test_labels, final_preds)],\n",
        "    'Precision_Real': [precision_score(test_labels, bert_preds, pos_label=0),\n",
        "                      precision_score(test_labels, lstm_preds, pos_label=0),\n",
        "                      precision_score(test_labels, final_preds, pos_label=0)],\n",
        "    'Precision_Fake': [precision_score(test_labels, bert_preds, pos_label=1),\n",
        "                      precision_score(test_labels, lstm_preds, pos_label=1),\n",
        "                      precision_score(test_labels, final_preds, pos_label=1)],\n",
        "    'Recall_Real': [recall_score(test_labels, bert_preds, pos_label=0),\n",
        "                   recall_score(test_labels, lstm_preds, pos_label=0),\n",
        "                   recall_score(test_labels, final_preds, pos_label=0)],\n",
        "    'Recall_Fake': [recall_score(test_labels, bert_preds, pos_label=1),\n",
        "                   recall_score(test_labels, lstm_preds, pos_label=1),\n",
        "                   recall_score(test_labels, final_preds, pos_label=1)],\n",
        "    'F1_Real': [f1_score(test_labels, bert_preds, pos_label=0),\n",
        "               f1_score(test_labels, lstm_preds, pos_label=0),\n",
        "               f1_score(test_labels, final_preds, pos_label=0)],\n",
        "    'F1_Fake': [f1_score(test_labels, bert_preds, pos_label=1),\n",
        "               f1_score(test_labels, lstm_preds, pos_label=1),\n",
        "               f1_score(test_labels, final_preds, pos_label=1)],\n",
        "    'AUC_ROC': [roc_auc_bert, roc_auc_lstm, roc_auc_ensemble]\n",
        "}\n",
        "\n",
        "detailed_df = pd.DataFrame(detailed_metrics)\n",
        "detailed_df.to_csv('./results/ensemble_detailed_metrics.csv', index=False)\n",
        "\n",
        "print(\"\\n‚úÖ All files saved successfully!\")\n",
        "print(\"   üìÅ ./results/ensemble_final_predictions.npy\")\n",
        "print(\"   üìÅ ./results/ensemble_final_probabilities.npy\")\n",
        "print(\"   üìÅ ./results/ensemble_weight_search_results.csv\")\n",
        "print(\"   üìÅ ./results/ensemble_models_comparison.csv\")\n",
        "print(\"   üìÅ ./results/ensemble_detailed_metrics.csv\")\n",
        "print(\"   üìÅ ./results/ensemble_comprehensive_visualization.png\")\n",
        "print(\"   üìÅ ./results/ensemble_detailed_metrics.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ HYBRID ENSEMBLE ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üèÜ Best Model: Hybrid Ensemble\")\n",
        "print(f\"üéØ Final Accuracy: {accuracy_score(test_labels, final_preds):.4f}\")\n",
        "print(f\"üìà AUC-ROC: {roc_auc_ensemble:.4f}\")\n",
        "print(f\"‚öñÔ∏è  Optimal Weights: IndoBERT ({w1:.2f}) + BiLSTM ({w2:.2f})\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "wnbQRPHplGu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STACKING"
      ],
      "metadata": {
        "id": "lMCKVtfxlbIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üîó ENSEMBLE METHODS: STACKING & MAJORITY VOTING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "try:\n",
        "    # Load prediksi probabilitas dari kedua model\n",
        "    bert_probs = np.load('./results/indobert_robust_test_preds.npy')\n",
        "    lstm_probs = np.load('./results/bilstm_test_preds.npy')\n",
        "\n",
        "    # Load label asli\n",
        "    test_labels = np.load('/content/tokenized_data/test_labels.npy')\n",
        "\n",
        "    # Load juga data training untuk stacking (PENTING!)\n",
        "    bert_train_probs = np.load('./results/indobert_robust_train_preds.npy')  # Harus dibuat dulu\n",
        "    lstm_train_probs = np.load('./results/bilstm_train_preds.npy')  # Harus dibuat dulu\n",
        "    train_labels = np.load('/content/tokenized_data/train_labels.npy')\n",
        "\n",
        "    print(\"‚úÖ Berhasil memuat prediksi dari kedua model.\")\n",
        "    print(f\"   Test samples: {len(test_labels)}\")\n",
        "    print(f\"   Train samples: {len(train_labels)}\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "    print(\"‚ö†Ô∏è  Untuk Stacking, Anda perlu menyimpan prediksi training set juga!\")\n",
        "    print(\"   Tambahkan di script training: np.save('results/indobert_robust_train_preds.npy', train_preds)\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "i-ff8Au-lbwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "# ============================================================================\n",
        "# PREPARASI DATA STACKING\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìö MULTI-MODEL STACKING ENSEMBLE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Gabungkan probabilitas sebagai fitur untuk meta-learner\n",
        "X_train_stack = np.column_stack([bert_train_probs.flatten(), lstm_train_probs.flatten()])\n",
        "X_test_stack = np.column_stack([bert_probs.flatten(), lstm_probs.flatten()])\n",
        "\n",
        "# Definisi Meta-Learners\n",
        "meta_models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"SVM (RBF Kernel)\": SVC(probability=True, kernel='rbf', random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Dictionary untuk menyimpan hasil\n",
        "results = {}\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING & EVALUATION LOOP\n",
        "# ============================================================================\n",
        "for name, model in meta_models.items():\n",
        "    print(f\"\\nüéØ Training Meta-Learner: {name}...\")\n",
        "    model.fit(X_train_stack, train_labels)\n",
        "\n",
        "    # Prediksi\n",
        "    preds = model.predict(X_test_stack)\n",
        "\n",
        "    # Hitung Metrik\n",
        "    acc = accuracy_score(test_labels, preds)\n",
        "    f1 = f1_score(test_labels, preds)\n",
        "    results[name] = {'Accuracy': acc, 'F1-Score': f1, 'Predictions': preds}\n",
        "\n",
        "    print(f\"‚úÖ {name} Done. Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(test_labels, preds, target_names=['Real', 'Fake']))\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI 1: PERBANDINGAN AKURASI\n",
        "# ============================================================================\n",
        "df_results = pd.DataFrame(results).T[['Accuracy', 'F1-Score']]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = df_results.plot(kind='bar', figsize=(10, 6), color=['#3498db', '#e74c3c'])\n",
        "plt.title('Perbandingan Performa Meta-Learner pada Stacking', fontsize=14)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Meta-Learner Model')\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Tambahkan label angka di atas bar\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f\"{p.get_height():.4f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALISASI 2: CONFUSION MATRICES (Side-by-Side)\n",
        "# ============================================================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
        "fig.suptitle('Confusion Matrices: Stacking Ensemble Models', fontsize=16)\n",
        "\n",
        "for i, (name, res) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(test_labels, res['Predictions'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
        "    axes[i].set_title(f\"{name}\\nAcc: {res['Accuracy']:.2%}\")\n",
        "    axes[i].set_xlabel('Prediksi')\n",
        "    axes[i].set_ylabel('Aktual')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EMUr36X2bKnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TRWCE5snbNyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNAPSHOT EMBENDING"
      ],
      "metadata": {
        "id": "iSspSHm0utwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "\n",
        "# 1. Menghitung Rata-rata Probabilitas (Ensemble Averaging)\n",
        "# Kita asumsikan output adalah probabilitas untuk kelas positif (Fake)\n",
        "ensemble_probs = (bert_probs + lstm_probs) / 2\n",
        "\n",
        "# 2. Konversi Probabilitas ke Label (Threshold 0.5)\n",
        "ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
        "\n",
        "# 3. Hitung Metrik untuk Perbandingan\n",
        "models = ['IndoBERT', 'BiLSTM', 'Ensemble']\n",
        "accuracies = [\n",
        "    accuracy_score(test_labels, (bert_probs > 0.5).astype(int)),\n",
        "    accuracy_score(test_labels, (lstm_probs > 0.5).astype(int)),\n",
        "    accuracy_score(test_labels, ensemble_preds)\n",
        "]\n",
        "f1_scores = [\n",
        "    f1_score(test_labels, (bert_probs > 0.5).astype(int)),\n",
        "    f1_score(test_labels, (lstm_probs > 0.5).astype(int)),\n",
        "    f1_score(test_labels, ensemble_preds)\n",
        "]\n",
        "\n",
        "print(\"üìä HASIL EVALUASI ENSEMBLE\")\n",
        "print(\"-\" * 30)\n",
        "print(classification_report(test_labels, ensemble_preds, target_names=['Real', 'Fake']))"
      ],
      "metadata": {
        "id": "5V2Cz7BypMhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualisasi 1: Perbandingan Akurasi & F1-Score ---\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db')\n",
        "rects2 = ax.bar(x + width/2, f1_scores, width, label='F1-Score', color='#e74c3c')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Perbandingan Performa: Individu vs Ensemble')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1.1) # Memberi ruang untuk label\n",
        "\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Visualisasi 2: Confusion Matrix Ensemble ---\n",
        "cm = confusion_matrix(test_labels, ensemble_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Real', 'Fake'],\n",
        "            yticklabels=['Real', 'Fake'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Aktual')\n",
        "plt.title('Confusion Matrix - Ensemble Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HW5q0pLCtgVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PERBANDINGAN SEMUA METODE (Load hasil Weighted Average dari step sebelumnya)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä PERBANDINGAN SEMUA ENSEMBLE METHODS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Anda perlu menjalankan weighted average dulu dan simpan hasilnya\n",
        "# Atau load dari hasil sebelumnya\n",
        "try:\n",
        "    # Contoh: load hasil weighted average yang sudah disimpan\n",
        "    weighted_preds = np.load('./results/weighted_avg_preds.npy')\n",
        "    weighted_acc = accuracy_score(test_labels, weighted_preds)\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Weighted Average belum disimpan. Hitung ulang...\")\n",
        "    # Gunakan bobot terbaik dari weighted average Anda (misal: 0.6, 0.4)\n",
        "    w1, w2 = 0.6, 0.4\n",
        "    weighted_probs = (w1 * bert_probs) + (w2 * lstm_probs)\n",
        "    weighted_preds = (weighted_probs >= 0.5).astype(int).flatten()\n",
        "    weighted_acc = accuracy_score(test_labels, weighted_preds)\n",
        "    np.save('./results/weighted_avg_preds.npy', weighted_preds)\n",
        "\n",
        "# Buat tabel perbandingan\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Method': ['Stacking', 'Majority Voting', 'Weighted Average'],\n",
        "    'Accuracy': [stacking_acc, majority_acc, weighted_acc]\n",
        "})\n",
        "\n",
        "# Tambahkan metrik lainnya\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "for idx, preds in enumerate([stacking_preds, majority_preds, weighted_preds]):\n",
        "    comparison_df.loc[idx, 'Precision'] = precision_score(test_labels, preds, average='weighted')\n",
        "    comparison_df.loc[idx, 'Recall'] = recall_score(test_labels, preds, average='weighted')\n",
        "    comparison_df.loc[idx, 'F1-Score'] = f1_score(test_labels, preds, average='weighted')\n",
        "\n",
        "# Sort by accuracy\n",
        "comparison_df = comparison_df.sort_values('Accuracy', ascending=False).reset_index(drop=True)\n",
        "comparison_df.index = comparison_df.index + 1  # Ranking\n",
        "\n",
        "print(\"\\nüèÜ RANKING ENSEMBLE METHODS:\")\n",
        "print(comparison_df.to_string(index=True))\n",
        "\n",
        "# Visualisasi perbandingan\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Bar chart accuracy\n",
        "axes[0].bar(comparison_df['Method'], comparison_df['Accuracy'],\n",
        "            color=['#2ecc71', '#3498db', '#e74c3c'])\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Perbandingan Accuracy Ensemble Methods')\n",
        "axes[0].set_ylim([0.5, 1.0])\n",
        "for i, v in enumerate(comparison_df['Accuracy']):\n",
        "    axes[0].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Grouped bar chart semua metrik\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x = np.arange(len(comparison_df))\n",
        "width = 0.2\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    axes[1].bar(x + i*width, comparison_df[metric], width,\n",
        "                label=metric, alpha=0.8)\n",
        "\n",
        "axes[1].set_xlabel('Ensemble Method')\n",
        "axes[1].set_ylabel('Score')\n",
        "axes[1].set_title('Perbandingan Semua Metrik')\n",
        "axes[1].set_xticks(x + width * 1.5)\n",
        "axes[1].set_xticklabels(comparison_df['Method'], rotation=15, ha='right')\n",
        "axes[1].legend()\n",
        "axes[1].set_ylim([0.5, 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('./results/ensemble_comparison.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Simpan hasil ke CSV\n",
        "comparison_df.to_csv('./results/ensemble_comparison.csv', index=False)\n",
        "print(\"\\n‚úÖ Hasil perbandingan disimpan ke: ./results/ensemble_comparison.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ SELESAI! Semua ensemble method telah dievaluasi.\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "WJVbCzVEpUYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gN38cG-JFI9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SAVE DAN JALANKAN DI DRIVE"
      ],
      "metadata": {
        "id": "dL8z_2ITuw9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Buat folder backup di Google Drive\n",
        "backup_path = '/content/drive/MyDrive/fake_news_project'\n",
        "os.makedirs(backup_path, exist_ok=True)\n",
        "\n",
        "# 3. Daftar folder yang akan disimpan\n",
        "folders_to_backup = ['models', 'results', 'tokenized_data']\n",
        "\n",
        "print(\"üöÄ Memulai backup ke Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for folder in folders_to_backup:\n",
        "    source = f'/content/{folder}'\n",
        "    destination = f'{backup_path}/{folder}'\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        # Hapus folder lama di Drive jika ada agar tidak error saat copytree\n",
        "        if os.path.exists(destination):\n",
        "            shutil.rmtree(destination)\n",
        "\n",
        "        # Copy folder baru\n",
        "        shutil.copytree(source, destination)\n",
        "\n",
        "        # Hitung ukuran folder\n",
        "        size_bytes = 0\n",
        "        for dirpath, _, filenames in os.walk(destination):\n",
        "            for filename in filenames:\n",
        "                fp = os.path.join(dirpath, filename)\n",
        "                size_bytes += os.path.getsize(fp)\n",
        "\n",
        "        size_mb = size_bytes / (1024 * 1024)\n",
        "\n",
        "        # Emoji di sini aman karena di dalam tanda kutip (string)\n",
        "        print(f\"‚úÖ {folder:20s} ‚Üí {size_mb:8.2f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  {folder} tidak ditemukan, skip...\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ Backup selesai!\")\n",
        "print(f\"üìÅ Lokasi: {backup_path}\")"
      ],
      "metadata": {
        "id": "LCZo34BZuyJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Path sumber dari Google Drive\n",
        "backup_path = '/content/drive/MyDrive/fake_news_project'\n",
        "\n",
        "# 3. Daftar folder yang akan dipulihkan\n",
        "folders_to_restore = ['models', 'results', 'tokenized_data']\n",
        "\n",
        "print(\"üîÑ Memulai restore dari Google Drive...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for folder in folders_to_restore:\n",
        "    source = f'{backup_path}/{folder}'\n",
        "    destination = f'/content/{folder}'\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        # Hapus folder lama di Colab jika ada\n",
        "        if os.path.exists(destination):\n",
        "            shutil.rmtree(destination)\n",
        "\n",
        "        # Copy folder dari Drive ke Colab\n",
        "        shutil.copytree(source, destination)\n",
        "\n",
        "        # Hitung ukuran\n",
        "        size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
        "                     for dirpath, _, filenames in os.walk(destination)\n",
        "                     for filename in filenames) / (1024 * 1024)\n",
        "\n",
        "        print(f\"‚úÖ {folder:20s} ‚Üí {size_mb:8.2f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ùå {folder} tidak ditemukan di Drive!\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ Restore selesai!\")\n",
        "print(\"\\nFolder yang dimuat:\")\n",
        "print(\"  - /content/models/\")\n",
        "print(\"  - /content/results/\")\n",
        "print(\"  - /content/tokenized_data/\")\n",
        "print(\"\\n‚ö° Siap digunakan untuk inference atau training lanjutan!\")"
      ],
      "metadata": {
        "id": "S87n_mY4v1f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"üìã Verifikasi file yang dimuat:\\n\")\n",
        "\n",
        "# Cek models\n",
        "print(\"üîπ Models:\")\n",
        "if os.path.exists('/content/models'):\n",
        "    for file in os.listdir('/content/models'):\n",
        "        print(f\"   ‚úì {file}\")\n",
        "\n",
        "# Cek results\n",
        "print(\"\\nüîπ Results:\")\n",
        "if os.path.exists('/content/results'):\n",
        "    for file in os.listdir('/content/results'):\n",
        "        print(f\"   ‚úì {file}\")\n",
        "\n",
        "# Cek tokenized_data\n",
        "print(\"\\nüîπ Tokenized Data:\")\n",
        "if os.path.exists('/content/tokenized_data'):\n",
        "    for file in os.listdir('/content/tokenized_data'):\n",
        "        print(f\"   ‚úì {file}\")"
      ],
      "metadata": {
        "id": "UfJw17OTv81Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}